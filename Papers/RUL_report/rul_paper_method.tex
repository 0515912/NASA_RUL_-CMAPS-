\section{Estimating Remaining Useful Life using Multi-Layer Perceptron as Regressor}
\label{sec:method}

In this section the proposed \gls{ann}-based method for prognostics is presented. Our method uses a Multi-Layer Perceptron (\gls{mlp}) as the main regressor for estimating the \gls{rul} of the engines at each subset of the \gls{cmaps} dataset. For the training sets, the feature vectors are generated by using a strided time window while the labels vector is generated using a constant \gls{rul} for the early cycles of the simulation and then linearly decreasing the number of remaining cycles \cite{Li2018, Lim2016}. For the test set, a time window is taken from the last sensor readings of the engine and used to predict the \gls{rul} of the engine. 

The window size $n_w$, window stride $n_s$ and constant \gls{rul} $C_r$ hyperparameters have a considerable impact in the quality of the predictions made by the regressor \cite{Li2018, Lim2016}. Hand picking the best parameters for our application is time consuming, furthermore, a grid search approach as the ones used for hyperparameter tuning in Neural Networks is computationally expensive given the search space inherent to the aforementioned parameters. In this paper we propose the use of an evolutionary algorithm, i.e. Differential Evolution (\gls{de}) \cite{Storn1997}, to fine tune the parameters. The optimization framework here proposed allows for the use of simpler architectures of Neural Networks while attaining better results in terms of the quality of the predictions made.



\subsection{The Neural Network Architecture}

For this study we propose to use a rather simple \gls{mlp} architecture. All the implementations were used in python using the Keras/Tensorflow environment. The structure of the Network remained consisted for all the four datasets.

The choice of the network architecture was made using an iterative process; comparing 4 different architectures, running each $10$ times for $100$ epochs. Two objectives were pursued; that the size of the architecture was small enough, e.g. in terms of layers and neurons within each layer and that the performance indicators were better than the ones presented in the literature so far. The process for choosing the network architecture can be described as follows: First, fix the window size $n_w$, the window stride $n_s$ and the constant RUL $C$. An initial setup of two layers with $250$ and $50$ neurons each was chosen, the number of neurons at each layer was then reduced by roughly a factor of 2 and tested using a cross-validation set from subset 1 of \gls{cmaps}. When the performance of the network started to degrade the process was stopped. Table  \ref{table:tested_architectures_100} summarizes the results for each tested architecture, Table \ref{table:proposed_nn} presents the architecture chosen for the remainder of this work  yielded the best results and hence was the chosen for the rest of the experiments. Details for the other 3 tested architectures are presented in Section ...

\begin{table}[!htb]
\centering
\begin{tabular}{l r r | r r | r r | r r}
	\hline	
	& \multicolumn{2}{c}{Min.} & \multicolumn{2}{c}{Max.}  & \multicolumn{2}{c}{Avg.}  & \multicolumn{2}{c}{STD} \\
	Tested Architecture & RMSE & RHS & RMSE & RHS & RMSE & RHS & RMSE & RHS\\
  	\hline
  	Architecture 1 & 10.85 & 151.65 & 12.23 & 277.67 & 11.66 & 226.94 & 0.45 & 41.58\\
  	Architecture 2 & 11.12 & 186.22 & 13.98 & 365.92 & 12.68 & 280.41 & 1.03 & 64.07\\
  	Architecture 3 & 11.58 & 179.15 & 12.72 & 266.55 & 12.04 & 215.09 & 0.35 & 28.39\\
  	Architecture 4 & 12.52 & 262.77 & 14.25 & 368.35 & 13.58 & 325.41 & 0.53 & 34.13\\
  	\hline
\end{tabular}
\caption{Results for different architectures for subset 1, 100 epochs}
\label{table:tested_architectures_100}
\end{table}

\begin{table}[!htb]
\centering
\begin{tabular}{l l l l}
	\hline
	Layer & Shape & Activation & Additional Information\\
  	\hline
  	Fully connected & 30 & ReLU & Dropout(0.6)\\
  	Fully connected & 10 & ReLU & Dropout(0.2)\\
  	Fully connected & 1 & Linear & \\
  	\hline
\end{tabular}
\caption{Proposed Neural Network architecture}
\label{table:proposed_nn}
\end{table}


