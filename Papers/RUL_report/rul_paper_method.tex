\section{Estimating Remaining Useful Life using Multi-Layer Perceptron as Regressor}
\label{sec:method}

In this section the proposed \gls{ann}-based method for prognostics is presented. Our method uses a Multi-Layer Perceptron (\gls{mlp}) as the main regressor for estimating the \gls{rul} of the engines at each subset of the \gls{cmaps} dataset. For the training sets, the feature vectors are generated by using a strided time window while the labels vector is generated using a constant \gls{rul} for the early cycles of the simulation and then linearly decreasing the number of remaining cycles \cite{Li2018, Lim2016}. For the test set, a time window is taken from the last sensor readings of the engine and used to predict the \gls{rul} of the engine. 

The window size $n_w$, window stride $n_s$ and constant \gls{rul} $C_r$ hyperparameters have a considerable impact in the quality of the predictions made by the regressor \cite{Li2018, Lim2016}. Hand picking the best parameters for our application is time consuming, furthermore, a grid search approach as the ones used for hyperparameter tuning in Neural Networks is computationally expensive given the search space inherent to the aforementioned parameters. In this paper we propose the use of an evolutionary algorithm, i.e. Differential Evolution (\gls{de}) \cite{Storn1997}, to fine tune the parameters. The optimization framework here proposed allows for the use of simpler architectures of Neural Networks while attaining better results in terms of the quality of the predictions made.

\subsection{The Neural Network Architecture}

For this study we propose to use a rather simple \gls{mlp} architecture. All the implementations were used in python using the Keras/Tensorflow environment. The structure of the Network remained consisted for all the four datasets, our structure consists of two hidden layers in total; in the first hidden layer we have placed 250 neurons while in the second we have placed 50 neurons. Table \ref{table:proposed_nn} yields a description of the used network.

\begin{table}[!htb]
\centering
\begin{tabular}{l l l l}
	\hline
	Layer & Shape & Activation & Additional Information\\
  	\hline
  	Fully connected & 30 & ReLU & Dropout(0.6)\\
  	Fully connected & 10 & ReLU & Dropout(0.2)\\
  	Fully connected & 1 & Linear & \\
  	\hline
\end{tabular}
\caption{Proposed Neural Network architecture}
\label{table:proposed_nn}
\end{table}

The choice of the network was based upon 