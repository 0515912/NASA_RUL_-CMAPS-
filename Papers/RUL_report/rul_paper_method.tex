\section{Estimating Remaining Useful Life using Multi-Layer Perceptron as Regressor}
\label{sec:method}

In this section the proposed \gls{ann}-based method for prognostics is presented. Our method uses a Multi-Layer Perceptron (\gls{mlp}) as the main regressor for estimating the \gls{rul} of the engines at each subset of the \gls{cmaps} dataset. For the training sets, the feature vectors are generated by using a strided time window while the labels vector is generated using a constant \gls{rul} for the early cycles of the simulation and then linearly decreasing the number of remaining cycles, this is the so called piecewise linear degradation model \cite{Ramasso2014}. For the test set, a time window is taken from the last sensor readings of the engine and used to predict the \gls{rul} of the engine.

The window size $n_w$, window stride $n_s$ and early \gls{rul} $R_e$ data related parameters have a considerable impact in the quality of the predictions made by the regressor. Hand picking the best parameters for our application is time consuming, furthermore, a grid search approach as the ones used for hyperparameter tuning in Neural Networks is computationally expensive given the search space inherent to the aforementioned parameters. In this paper we propose the use of an evolutionary algorithm, i.e. Differential Evolution (\gls{de}) \cite{Storn1997}, to fine tune the parameters. The optimization framework here proposed allows for the use of a simple Neural Network architecture while attaining better results in terms of the quality of the predictions made than the ones in the current literature.

\subsection{The Neural Network Architecture}

For this study we propose to use a rather simple \gls{mlp} architecture. All the implementations were used in python using the Keras/Tensorflow environment. The structure of the Network remained consisted for all the four datasets.

The choice of the network architecture was made using an iterative process; comparing 4 different architectures, running each $10$ times for $100$ epochs and using a mini-batch size of $512$. Two objectives were pursued; that the size of the architecture was small enough, e.g. in terms of layers and neurons within each layer and that the performance indicators were better than the ones presented in the literature so far. The process for choosing the network architecture can be described as follows: First, fix the window size $n_w$, the window stride $n_s$ and the early RUL $R_e$. An initial setup of two layers with $250$ and $50$ neurons each was chosen, the number of neurons at each layer was then reduced by roughly a factor of 2 and tested using a cross-validation set from subset 1 of \gls{cmaps}. When the performance of the network started to degrade the process was stopped. Table  \ref{table:tested_architectures_100} summarizes the results for each tested architecture, Table \ref{table:proposed_nn} presents the architecture chosen for the remainder of this work yielded the best results and hence was the chosen for the rest of the experiments. Details for the other 3 tested architectures are presented in Section ...

\begin{table}[!htb]
\centering

\begin{tabular}{l | r r r r | r r r r}
	\hline	
	& \multicolumn{4}{| c}{RMSE} & \multicolumn{4}{| c}{RHS} \\
	Tested Architecture & Min. & Max. & Avg. & STD & Min. & Max. & Avg. & STD\\
  	\hline
  	Architecture 1 & 10.85 & 12.23 & 11.66 & 0.45 & 151.65 & 277.67 & 226.94 & 41.58\\
  	Architecture 2 & 11.12 & 13.98 & 12.68 & 1.03 & 186.22 & 365.92 & 280.41 & 64.07\\
  	Architecture 3 & 11.58 & 12.72 & 12.04 & 0.35 & 179.15 & 266.55 & 215.09 & 28.39\\
  	Architecture 4 & 12.52 & 14.25 & 13.58 & 0.53 & 262.77 & 368.35 & 325.41 & 34.13\\
  	\hline
\end{tabular}

\caption{Results for different architectures for subset 1, 100 epochs}
\label{table:tested_architectures_100}
\end{table}

\begin{table}[!htb]
\centering
\begin{tabular}{l l l l}
	\hline
	Layer & Shape & Activation & Additional Information\\
  	\hline
  	Fully connected & 30 & ReLU & Dropout(0.6)\\
  	Fully connected & 10 & ReLU & Dropout(0.2)\\
  	Fully connected & 1 & Linear & \\
  	\hline
\end{tabular}
\caption{Proposed Neural Network architecture}
\label{table:proposed_nn}
\end{table}

\subsection{Shaping the data}

This section covers the data preprocessing applied to the raw sensor readings in each of the datasets. Even-though the original datasets contains $21$ different sensor readings some of the sensor do not present much variance and are therefore discarded. Therefore only $14$ sensor readings out of the $21$ are considered for this study, their indices are $\left\lbrace 2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21 \right\rbrace$. The raw measurements are then used to create the strided time windows with window size $n_w$ and window stride $n_s$, for the labels $R_e$ is used at the early stages and then the \gls{rul} is linearly decreased. Finally, the data is normalized to be within the range $\left[ -1,1 \right]$ using the min-max normalization.

\begin{equation}
x^{i,j}_{norm} = 2 \frac{x^{i,j} - x^{j}_{min}}{x^{j}_{max} - x^{j}_{min}} - 1
\label{eq:min_max_norm}
\end{equation}

where $x_{i,j}$ denotes the original $i$-th data point of the $j$-th sensor and $x^{i,j_norm}$ is the normalized value of $x^{i,j}$.  $x^{j}_{max}$ and $x^{j}_{min}$ denote the maximum and minimum values of the original measurement data from the $j$-th sensor, respectively. 

\subsubsection{Time Window Processing}

In multivariate time-series based problems such as \gls{rul}, more information can be generally obtained from the temporal sequence of data as compared with the multivariate data point at a single time stamp. Let $n_w$ denote the size of the time window, for a time window with a stride $n_s = 1$, all the past sensor values within the time window are collected and put together to form a feature vector $\mathbf{x}$. This approach has successfully been tested in \cite{Li2018, Lim2016} where they propose the use of a moving window with values raging from 20 to 30. In this paper we propose not only the use of a moving time window, but also a \textit{strided} time window that updates $n_s$ elements at the time instead of $1$. 

The use of a \textit{strided time window} allows for the regressor to take advantage not only of the previous information available, but also to control the ratio at which the algorithm is fed with new information. With the usual time window approach only one point is updated for every new time window, on the contrary, the strided time window allows for updating $n_s$ points at the time, allowing for the algorithm to catch newer information with fewer iterations, furthermore, the information contained in the strided time window is likely more rich than the one contained in a time window with stride of $1$.

\subsubsection{Piecewise linear degradation model}

Different from common regression problems, the desired output value of the input data is difficult to determine for a \gls{rul} problem. It is usually impossible to evaluate the precise health condition and estimate the \gls{rul} of the system at each time step without an accurate physics based model. For this popular dataset, a piece-wise linear degradation model has been proposed in \cite{Ramasso2014}. The piece-wise linear degradation model assumes that the engines have a constant \gls{rul} label in the early cycles and then the \gls{rul} starts degrading linearly until it reaches 0. The piecewise linear degradation approach is used for this work, in here we denote the value for the \gls{rul} at the early stages as $R_e$. 

\subsection{Choosing optimal data-related parameters}

As mentioned in the previous sections the choice of the data-related parameters window size $n_w$, window stride $n_s$ and constant \gls{rul} $R_e$ have a large impact on the performance of the regressor, i.e. the \gls{mlp}. In this section we present a framework for picking the best combination of the data-related parameters $n_w$, $n_s$ and $R_e$ without spending too much computational time.

Let $\mathbf{v} = (n_w, n_s, R_e)$, where $n_w \in \left[1, b\right]$, $n_s \in \left[1, 10\right]$ and $R_e \in \left[90, 140 \right]$ and all the intervals are integer intervals. The value of $b$ is dependent upon the specific subset, Table \ref{table:b_values} presents the different values $b$ can take for each dataset.

\begin{table}[!htb]
\centering
\begin{tabular}{l | l l l l}
	\hline
	 & FD001 & FD002 2 & FD003 & FD004\\
  	\hline
  	$b$ & 30 & 20 & 30 & 18\\
  	\hline
\end{tabular}
\caption{Allowed values for $b$ per subset}
\label{table:b_values}
\end{table}

Given $\mathbf{v}$, we can evaluate the performance of the regressor by reshaping the data using $\mathbf{v}$, training the \gls{mlp} using the obtained data and then computing the scores in equations \ref{eq:rmse} and \ref{eq:rhs}.This setting is just what is required for performing any kind of optimization, i.e. to have a set of tunable parameters and a performance indicator, therefore, here we propose to fine tune $\mathbf{v}$ using a meta-heuristic algorithm.

\subsubsection{Differential Evolution for obtaining the optimal data-related parameters}

Differential Evolution (\gls{de}) \cite{Storn1997} is a method that optmizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. The method does not make any assumptions about the problem, therefore it is know as a metaheuristic, nevertheless, this kind of methods are not guaranteed to converge to the optimal solution. \gls{de} does not require the gradient of the problem being optimized, making it a very suitable metaheuristic for applications such as Neural Networks.

\gls{de} belongs to a class of algorithms known as evolutionary algorithms. The algorithm optimizes a problem by maintaining a \textit{population} of candidate solutions and creating new candidate solutions $\mathbf{v}'$ by combining existing ones according to a simple cross-over formula, keeping whichever candidate solution $\mathbf{v}^*$ has the best score or fitness on the optimization problem at hand. In this way the optimization problem is treated as a black box that merely provides a measure of quality given a candidate solution and the gradient is therefore not needed.

Although \gls{de} does not have special operators for treating integer variables a very simple modification to the algorithm, consisting on rounding every candidate solution $\mathbf{v}'$ to the nearest integer, is used for this work.

There is yet one last detail to be taken care of, evolutionary algorithms such as \gls{de} tend to use several function evaluations for obtaining the optimal solutions,  for our application one function evaluations implies retraining the Neural Network. Certainly this is not a desirable scenario, as obtaining the optimal parameter vector $\mathbf{v}$ would entail an extensive use of computational power. Instead of running for \gls{de} for several iterations and with a large population we propose to run it just for $10$ iterations (generations in the literature of evolutionary computation) and using a population size of $10$, which seems reasonable given the size of the search space of $\mathbf{v}$. Furthermore, during the tuning process the \gls{mlp} is not trained for  $100$ epochs, instead the \gls{mlp} is trained for just $20$ epochs, this is done mainly for two reasons: the use of the mini-batch in the training process allows for a speed up in the convergence, therefore it can be assumed that the algorithm will most likely be very close to its optima after just a couple of iterations, second and most important is the fact that we assume that parameters that lead to lower score values in the early stages of the \gls{mlp} training process are more likely to provide better performance overall. Details for the use of \gls{de} in finding the optimum data-related parameters are described in Table \ref{table:de_hyperparams}. The optimal data-related parameters for each of the subsets found by \gls{de} are shown in Table \ref{table:optimal_data_params}.

\begin{table}[!htb]
\centering
\begin{tabular}{l l l l l}
	\hline
	 Population Size & Generations & Strategy & \gls{mlp} epochs\\
  	\hline
  	10 & 10 & Best1Bin & 20\\
  	\hline
\end{tabular}
\caption{Differential Evolution hyper-parameters.}
\label{table:de_hyperparams}
\end{table}

\begin{comment}
\begin{table}[!htb]
\centering
\begin{tabular}{l l l l l}
	\hline
	 Dataset & Window Size $n_w$ & Window Stride $n_s$ & Early RUL $R_e$\\
  	\hline
  	FD001 & 26 & 2 & 100\\
  	FD002 & 16 & 2 & 91\\
  	FD003 & 30 & 2 & 97\\
  	FD004 & 16 & 2 & 92\\
  	\hline
\end{tabular}
\caption{Optimal data-related parameters for each subset.}
\label{table:optimal_data_params}
\end{table}
\end{comment}

\begin{table}[!htb]
\centering
\begin{tabular}{l l l l l}
	\hline
	 Dataset & Window Size $n_w$ & Window Stride $n_s$ & Early RUL $R_e$\\
  	\hline
  	FD001 & 29 & 2 & 93\\
  	FD002 & 17 & 2 & 94\\
  	FD003 & 28 & 2 & 92\\
  	FD004 & 17 & 2 & 94\\
  	\hline
\end{tabular}
\caption{Data-related parameters for each subset obtained with Differential Evolution.}
\label{table:optimal_data_params}
\end{table}