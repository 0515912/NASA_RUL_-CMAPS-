\section{The Enhanced Directed Search Method}
\label{sec:eds_method}

\subsection{Main idea}
\label{sec:direction_objective_space}

A major task in the development of the \gls{eds} continuation method is to ``follow'' a certain direction in objective space. For this, we will use the work developed in \cite{ds2}, which allows to map a direction from objective space $\nu \in \mathbb{R}^n$ to  direction  in parameter space $d \in \mathbb{R}^k$. This is achieved by solving the following single-objective optimization problem \cite{ds2}:

\begin{equation}
	\begin{aligned}
	& \underset{\nu, \delta}{\text{min}}
	& & \frac{1}{2}\left\| \nu \right\|_2^2 - \delta\\
	& \text{s. t.}
	& & J(x) \nu = \delta d,\\
	\end{aligned}
	\label{eq:alternative1}
\end{equation}

\noindent where $J(x) \in \mathbb{R}^{k \times n}$ is the Jacobian of $F$ as defined in Eq. \ref{eq:mop} in page \pageref{eq:mop} at the point $x$ and $\delta \in \mathbb{R}$ is a scalar value. Let $(\nu^*, \delta^*)$ be a solution of Problem \eqref{eq:alternative1} where $d \neq 0$. Then for $v^*$ and $\delta^*$ the following propositions \cite{ds2} hold:

\begin{proposition}\ \\
\label{proposition1}

\begin{enumerate}[a)]
\item $v^* \neq 0 \iff \delta^* > 0 \iff \exists v \in \mathbb{R}^n : Jv = d$. In that case  $v^* = \delta^*J^+d$.
\item $v^* = 0 \iff \delta^* = 0$. In that case $x$ is a critical point (see Definition \ref{def:critical_point}).
\item $v(x)$ and $\delta(x)$ are continuous mappings.
\end{enumerate}

\end{proposition}

By Proposition \ref{proposition1} it follows that:

\begin{itemize}
\item $\delta^*$ can be used as a criterion for determining critical points.
\item Problem \eqref{eq:alternative1} can be used for mapping a direction in objective space to a direction in parameter space.
\end{itemize}

Problem \ref{eq:alternative1} along with the above propositions open the possibility for steering the search in a desired direction in objective space and mapping it to parameter space. As it can be seen, this is exactly the main idea underlying \gls{ds}, nevertheless Proposition \ref{proposition1} provides a more stable criterion for determining critical points that the one provided in \cite{directed_search}. Hence, by solving Problem \eqref{eq:alternative1} for a point $x_0 \in \mathbb{R}^n$ and a direction in objective space $d \in \mathbb{R}^k$, one obtains a direction in parameter space $\nu \in \mathbb{R}^n$ such that $F(x_i)$, where $x_i = x_0 + t \nu$, moves along $d$ for a suitable step size $t$, and a scalar $\delta \in \mathbb{R}$, where $\delta = 0$ when $x_0$ is a critical point. The aforementioned ideas serve as the basis for the predictor and corrector steps in the \gls{eds} method.

In the following we propose a new PC method for the continuation along (local) Pareto sets of a given \gls{mmop}. The Enhanced Directed Search (\gls{eds}) method is in principle capable of dealing with problems with $k \geq 2$, furthermore, neighborhood information is exploited in order to save function evaluations and hence improving the overall performance of the method.

\subsection{Predictor}
\label{sec:predictor}

For the predictor we make use of the ideas developed in \cite{directed_search}. Assume we are given a (local) Pareto point and its associated convex weight $\alpha \in \mathbb{R}^k$, further, we assume that rank$(J(x)) = k - 1$. Then it is implied by Theorem \ref{theo:kkt_cond} that $\alpha$ is orthogonal to the linearized Pareto front at $F(x)$. Thus, a search orthogonal to $\alpha$ could be promising to obtain new predictor points. To obtain such directions a $QR$-factorization of $\alpha$ can be computed, i.e.,

\begin{equation}
\alpha = QR,
\label{eq:qr_alpha}
\end{equation} 

where $Q = (q_1, \ldots, q_k) \in \mathbb{R}^{k \times k}$ is an orthogonal matrix and $R = (r_{11}, 0, \ldots, 0)^T \in R^{k \times 1}$ with $r_{11} \in \mathbb{R} \texttt{\char`\\} \{0\}$. Since by Eq. \eqref{eq:qr_alpha} $\alpha = r_{11}	q_1$, it follows that a well spread set of directions can be taken from any of the normalized search directions $\nu_i$ such that:

\begin{equation}
J(x)\nu_i = q_{i+1}, \quad i = 1, \ldots, k-1. 
\label{eq:ds_predictor}
\end{equation}

It follows by the rank of $J(x)$ that the vectors $q_2, \ldots, q_k$, are in the image of $J(x)$ and hence Eq. \eqref{eq:alternative1} can be solved for each $q_i$ where $i \in \{2, \ldots, k\}$. Please note that such predictor directions $\nu_i = \delta_i J^+ q_i$ do not have to be tangent to the \emph{Pareto set}. Instead, $\nu = \delta J(x)^+ q$ points along the linearized Pareto front. Since by Proposition \ref{proposition1} $\nu$ is the most greedy solution of Problem \eqref{eq:alternative1} with respect to $q$, we can expect that the image $F(p)$, where $p \in \mathbb{R}^n$ is the chosen predictor, is also close to the Pareto front, and thus, that only few iteration steps are required to correct back to the front. Hence, a set of predictors can be computed in the following way:

\begin{eqnarray}
p_{i+} = x + \hat{t} \frac{\nu_i}{||\nu_i||} \nonumber \\
p_{i-} = x - \hat{t} \frac{\nu_i}{||\nu_i||},
\label{eq:movement_predictor}
\end{eqnarray}

where $\hat{t}$ is the chosen step size. Note that both, the positive and the negative predictors have to be considered. This is done to ensure that the search is done in every possible direction and hence that a full covering of the Pareto front is obtained at the end of \gls{eds} execution.

Finally, to obtain the predictor $p$ at the current point $x$, we need to select a suitable step size. In this study we are particularly interested in an evenly distributed set of solutions along the Pareto front. That is, at least for two consecutive solutions $x_{i+1}$ and $x_i$ we want that

\begin{equation}
|| F(x_{i+1}) - F(x_i) || \approx \tau,
\label{eq:spreadness_solutions}
\end{equation}

where $\tau > 0$ is a user specified value. For this, we follow the suggestion made in \cite{hillermeier01} and take the step size

\begin{equation}
\hat{t} = \frac{\tau}{|| J \nu ||}.
\label{eq:predictor_stepsize}
\end{equation}

The entire predictor phase is shown in Algorithm \ref{alg:predictor}

\setcounter{algorithm}{0}
\begin{algorithm}[H]
\caption{Obtain predictors}\label{alg:predictor}
\textbf{Input:} Initial point $x \in \mathbb{R}^n$, predictor direction in objective space $d \in \mathbb{R}^k$\\
\textbf{Output:} A set of predictors $p_+$ and $p_-$
	\begin{algorithmic}[1]
		\State Compute $J(x)$
		\State Compute direction $\nu_p$	by solving Problem \eqref{eq:alternative1}
		\State Compute predictor step size $t_p$ by Eq. \eqref{eq:predictor_stepsize}
		\State Set $p_+ = x + \hat{t} \nu$ and $p_- = x - \hat{t} \nu$ 
		\State return $[p_+, p_-]$   
	\end{algorithmic}
\end{algorithm}

Figure \ref{Fig:predictor_example} exemplifies the predictor phase. First, for the local Pareto point $x_i$ the $\alpha$ vector is computed, then a direction $d_+$ orthogonal to $\alpha$ is computed. Finally, after executing Algorithm \ref{alg:predictor}, two predictors $p_+$ and $p_-$ are obtained.

\begin{comment}

 \begin{figure}[H] 
	\centering \def\svgwidth{190pt} 
	\input{img/eds_predictor_example1.pdf_tex} 
	\caption{Computation of the predictors} 
	\label{Fig:predictor_example}
\end{figure}

\end{comment}

\subsection{Corrector}
\label{sec:corrector}

For the computation of the corrector, a new directed search descent method was developed. Assume we are given a predictor $p$ along with the weight vector $\alpha$ associated to the initial point $x$ from which $p$ was computed. Furthermore, since by assumption $F(p)$ is \emph{close} to $F(x)$ it makes sense to use the opposite direction of $\alpha$, i.e., $-\alpha$, in order to move back to the Pareto front (see Figure \ref{Fig:corrector_direction}).

\begin{comment}

\begin{figure}[H] 
	\centering \def\svgwidth{190pt} 
	\input{img/eds_pc_example1.pdf_tex} 
	\caption{$-\alpha$ taken as steering direction for the corrector} 
	\label{Fig:corrector_direction}
\end{figure}

\end{comment}

Thus, given $p \in \mathbb{R}^n$ and $-\alpha \in \mathbb{R}^k$, a direction $\nu_c \in \mathbb{R}^n$ that moves along $-\alpha$ can be computed by solving Eq. \eqref{eq:alternative1} for $p$ and $-\alpha$. A movement along $\nu_c$ (and hence in $-\alpha$ direction in objective space) is then performed in the following way:

\begin{equation}
c_i = p_i + t \frac{\nu_c}{||\nu_c||},
\label{eq:movement_corrector}
\end{equation}

where $t$ is the corrector step size. Contrary to the predictor phase, several corrector iterations may be computed until a certain stopping criterion (see Section \ref{sec:determining_pareto_critical_points}) is met.

Two important issues arise now: first, how to compute a suitable step size for the corrector? And second, how to determine whenever the corrector is indeed a critical point? Both issues will be addressed in the following sections.

\subsubsection{Corrector Step Size}

As stated in Section \ref{sec:directed_search} a movement along $d \in \mathbb{R}^k$ (in objective space) using the map \eqref{eq:direction3}, which maps to a movement $\nu$ in parameter space

\begin{equation}
	x_{i+1} = x_i + t \nu,
	\label{eq:movement_along_nu}
\end{equation}

is only possible for \emph{sufficiently small} values of $t$. Let $\tilde{d} = F(x_{i+1}) - F(x_i) \in \mathbb{R}^k$, be the vector in objective space obtained after moving along $\nu$ direction for a given step size $t$, also let 

\begin{equation}
\beta = cos^{-1}(\frac{\langle \tilde{d}, d \rangle}{|| \tilde{d} ||  || d || }),
\label{eq:dot_product_cosine2}
\end{equation}

be the angle between $\tilde{d}$ and $d$. It was observed during our experiments that as $t$ increases the deviation between $\tilde{d}$ and $d$, measured by the angle $\beta$, also increases. Therefore, the step size for the corrector plays an important role in the overall performance of the method.

Assume we are given a predictor $p$ along with a direction in parameter space $\nu$ and a direction $d$ in objective space. Let $\tilde{d} = F(c) - F(p)$ where $c = p + t \nu$. A suitable step size $t$ provides the largest progress in direction $\tilde{d}$ while keeping angle $\beta$ bellow a user defined threshold. For our convenience we will define $\epsilon \in [0,1]$ as the aforementioned threshold where $\epsilon = cos(\beta)$, therefore keeping the angle itself bellow a threshold $cos^{-1}(\epsilon)$. Furthermore we would like that the corrector $c$ at least weekly dominates the previous point $p$. Hence, the optimal step size $t$ can be computed by solving the following constrained single objective optmization problem (\gls{sop}):

\begin{equation}
	\begin{aligned}
	\underset{t \in \mathbb{R}}{\text{minimize}}
	& \quad -|| \tilde{d} ||\\
	\text{s. t.}\\
	& \quad min(\tilde{d}) & \leq 0\\
	& \quad \epsilon - \frac{\langle \tilde{d}, d \rangle}{|| \tilde{d} || || d ||} & \leq 0,\\
	\end{aligned}
	\label{eq:corrector_step_size_sop}
\end{equation}

where $\tilde{d} = F(p + t \nu) - F(p)$. The objective function of Problem \eqref{eq:corrector_step_size_sop} will try to maximize the movement along $\tilde{d}$. The first constraint ensures that the newly computed point $F(c)$ at least weakly dominates the previous one $F(p)$. We will now bring our attention to the second constraint.

Assume that a certain deviation from direction $d$, measured by $\epsilon \in  [0, 1]$, is permitted. Then any direction $\tilde{d} = F(c) - F(p)$ that fulfills the constraint

\begin{equation}
\epsilon - \frac{\langle \tilde{d}, d \rangle}{|| \tilde{d} || || d ||} \leq 0
\label{eq:direction_cons}
\end{equation}

points in direction $d$ with a maximum deviation of $\beta = cos^{-1}(\epsilon)$ degrees. Thus, the smaller the value of $\epsilon$, the greater the permitted angle between $\tilde{d}$ and $d$ (and thus the deviation between them) is. Also note that by construction of constraint \eqref{eq:direction_cons} angles $\beta \in (\frac{\pi}{2}, \frac{3\pi}{2})$, and thus solutions that are dominated by $F(p)$, are automatically discarded. Figure \ref{Fig:stepsize_deviation} depicts constraint \eqref{eq:direction_cons}, any point in between the dashed arrows is an acceptable corrector point $c$, recall that $\beta = cos^{-1}(\epsilon)$.

\begin{comment}

\begin{figure}[H] 
	\centering \def\svgwidth{150pt} 
	\input{img/eds_stepsize_deviation.pdf_tex} 
	\caption{Acceptable deviations for the corrector $c$} 
	\label{Fig:stepsize_deviation}
\end{figure}

\end{comment}

Therefore, by solving Problem \eqref{eq:corrector_step_size_sop} a suitable step size for the corrector can be obtained. Nevertheless, solving Problem \eqref{eq:corrector_step_size_sop} would directly impact on the efficiency of \gls{eds} method. Instead, a more efficient backtracking-like strategy has been adopted.

Assume we are given $\epsilon$ and an initial step size $t_0$ (usually the last suitable step size computed). We test whether $t_0$ fulfills the constraints of Problem \eqref{eq:corrector_step_size_sop}, if it does we take $t_0$ as step size for the corrector, otherwise we shrink $t_0$ and try again until a suitable step size $t_0$ is found or until $t_0$ is bellow a user defined threshold. In case $t_0$ is suitable at the first try we set $t_1 = 2t_0$ and use it as initial guess for the next corrector step. Algorithm \ref{alg:corrector_stepsize} shows how to compute a suitable step size for the corrector given a direction $\nu$ in parameter space, a direction $d$ in objective space, and an initial point $x_0$.

\begin{algorithm}[H]
\caption{Corrector step size}\label{alg:corrector_stepsize}
\textbf{Input:} Current point $x \in \mathbb{R}^n$ along with its function value $F(x)$, step size guess $t \in \mathbb{R}^+$, $\nu \in \mathbb{R}^n$ direction and minimum step size threshold $\text{min\_}t$.\\
\textbf{Output:} A step size $t$, a new point $x = x + t \nu$ along with its function value $F(x)$, and a flag $b\_first$ indicating whether step size $h$ was suitable at first try.
	\begin{algorithmic}[1]
			
	    \State Compute a random shrinking factor $\rho \in [0.1,0.6]$
		\State $b\_cont = 1$
		\State $b\_first = 1$
		\State $x\_init = x$
		
		\While{$b\_cont \neq 0$}
  
  			\State $x = x\_init + t \nu$
  			\State Evaluate $F(x)$
  			\If{$F(x)$ satisfies the constraints of Problem \eqref{eq:corrector_step_size_sop}}
  				\State $b\_cont = 0$
   			\Else
   				\State Shrink $t$ by $\rho$ factor, i.e., $t = \rho t$
   				\State $b\_first = 0$
			\EndIf
			
			\If{$t < \text{min\_}t$}	
				\State $t = 0$
				\State $b\_cont = 0$
			\EndIf		
		
		\EndWhile	    
		
		\State return $[x, F(x), t, b\_first]$   
	\end{algorithmic}
\end{algorithm}

Finally, it is worth to mention that although the value of $\epsilon$ is a user defined and problem dependent parameter, experimental results have shown that $\epsilon \in [0.6, 0.9]$ improve the overall performance of the \gls{eds} method.

\subsubsection{Determining Critical Points}
\label{sec:determining_pareto_critical_points}

Now that we have a promising search direction $\nu \in \mathbb{R}^n$ and a suitable step size $t \in \mathbb{R}$ the remaining task of the corrector step is to determine whether a corrector point $c$ is a critical point or not. Recall by Proposition \ref{proposition1} that, $\delta = 0$ when $c$ is a critical point and that $\delta(x)$ is a continuous map. Therefore, the value of $\delta$ can be used for determining if a corrector $c$ is indeed a critical point. Two thresholds for $\delta$ can be defined: $\text{max\_}\delta > 0$ and $0 < \text{min\_}\delta < \text{max\_}\delta$. The use of both thresholds and the entire corrector phase is described in Algorithm \ref{alg:corrector}.

\begin{algorithm}[H]
\caption{Compute corrector}\label{alg:corrector}
\textbf{Input:} Direction $-\alpha \in \mathbb{R}^k$, predictor point $p \in \mathbb{R}^n$, thresholds $\text{min\_}\delta \in \mathbb{R}^+$ and $\text{max\_}\delta \in \mathbb{R}^+$\\
\textbf{Output:} A point $x \in \mathbb{R}^n$ along with its function value $F(x)$ and a flag $b \in \{0,1\}$ indicating whether $x$ is a critical point or not.
	\begin{algorithmic}[1]
		\State $x = p$
		
		\While{true}		

			\State Compute $J(x) \in \mathbb{R}^{k \times n}$
			\State Compute $\nu$, s.t. $J \nu = d$, by solving Problem \eqref{eq:alternative1}		
		
			\If{$\delta < \text{min\_}\delta$}
				\State return $[x, F(x),  b = 1]$
			\EndIf
		
			\State Obtain $[x, F(x), h, b\_first]$ by Algorithm \ref{alg:corrector_stepsize} 
		
			\If{$h = 0$, i.e. no suitable step size could be computed}
				\If{$\delta < \text{max\_}\delta$}
					\State return $[x, F(x), b = 1]$
				\Else	
					\State return $[x, F(x), b = 0]$
				\EndIf
			\Else
				\If{$b\_first = 1$}
					$t = 2t$					
				\EndIf	
		
			\EndIf
		
		\EndWhile
		
		\State return $[x, F(x), b = 0]$
	\end{algorithmic}
\end{algorithm}

As it can be seen the thresholds $\text{min\_}\delta$ and $\text{max\_}\delta$ are critical for the robustness and performance of \gls{eds} method. Further investigation on this topic is left for future research.

Finally after a new critical point is computed, its associated weight vector $\alpha$ can be updated as follows (\cite{alpha_update}):

\begin{equation}
\begin{aligned}
	\alpha = &\underset{\lambda \in \mathbb{R}^k}{\text{ arg min}} \left\| \sum_{i = 1}^k \lambda_i \nabla f_i(x) \right\|^2 \\ 
	& \text{ s. t. }\\ 
	& \qquad \qquad \lambda_i \geq 0, \quad i = 1, \ldots k \\
	& \qquad \qquad \sum_{i = 1}^k \lambda_i = 1, \quad i = 1, \ldots k.
\end{aligned}
\end{equation}

We are now in the position to put predictor and corrector methods together into a continuation algorithm that we call Enhanced Directed Search (\gls{eds}). The \gls{eds} method is able to ``follow'' the curve (or manifold) of Pareto points of a \gls{mop}. This procedure perfectly fits into the category of \gls{pc} methods. There are, nevertheless, still two issues that need to be solved before defining the algorithm of \gls{eds}: first, how to identify the parts of the manifold that have already been covered by \gls{eds}? And second, when and how should the gls{eds} method be stopped? Answers to these questions are given in the following sections.

\subsection{Determining Points Already Covered by EDS}
\label{sec:handling_kg2}

The issue of determining points already computed by the \gls{eds} method arises already for \glspl{bop}. Recall that the predictor phase computes two predictors: one that maps to direction $d \in \mathbb{R}^k$ and another that maps to direction $-d \in \mathbb{R}^k$. This is done to ensure that by the end of \gls{eds} execution a well spread discretization of the Pareto front is obtained.

Let $x \in \mathbb{R}^n$ be a local Pareto point, also let $F(x)$. Now, assume that a set of predictors $p_+ = x + \hat{t} \nu$ and $p_- = x - \hat{t} \nu$ are computed for a fixed value of $\hat{t}$ and that from each of the predictors a new critical point can be reached by means of a corrector step. Let $x_1$ be one of such new critical points, shall we compute again a set of predictors and correctors with the same $\hat{t}$ from $x_1$ it is most likely that one of the new critical points, say $x_2$, will be within a ``small'' neighborhood of $x$ and hence if the process is repeated with $x_2$ it is clear to see that a set of ``repeated'' points will be computed. Therefore a mechanism to avoid computing such ``repeated'' points should be developed.

To achieve this, it is crucial to provide an efficient method to keep track of the computed solutions. To this end, the method proposed in \cite{pareto_tracer} is used. According to \cite{pareto_tracer} the objective space can be divided into a set of \emph{small} boxes where each box $\mathcal{B} \subset \mathbb{R}^k$ is represented by a center $\hat{c} \in \mathbb{R}^k$ and a radius $\hat{r} \in \mathbb{R}^k$. Therefore, the covering box $\mathcal{B}(F(x))$ of a given solution $F(x)$ can be unequivocally determined. For a complete description of the strategy used please refer to \cite{pareto_tracer}.

Figure \ref{Fig:example_boxes} depicts the box covering algorithm for three phases of the eds method. It can be seen how as the \gls{eds} method advances more boxes $\mathcal{B}$ are added to the objective space.

\begin{comment}

\begin{figure}[H]
\hspace*{-1cm}
    \subfloat[Box set for 300th iteration \label{Fig:boxes1}]{%
    	\centering \includegraphics[width = 55mm, height = 55mm]{img/boxes1.png}
    }
    \subfloat[Box set for 600th iteration \label{Fig:boxes2}]{%
    	\centering \includegraphics[width = 55mm, height = 55mm]{img/boxes2.png}
    }
    \subfloat[Box set for end of the method\label{Fig:boxes3}]{%
    	\centering \includegraphics[width = 55mm, height = 55mm]{img/boxes3.png}
    }
    \caption{Box set for different stages of the method}
    \label{Fig:example_boxes}
\end{figure}

\end{comment}

\subsection{Handling Box Constrained Problems}
\label{sec:box_constraints}

The strategy for handling box constrained problems in the \gls{eds} method is straightforward. This strategy is widely used and consists on the following: let $x \in \mathbb{R}^n$ be a box constrained vector, i.e., $lb \leq x \leq ub$, where $ub \in\mathbb{R}^n$ and $lb \in \mathbb{R}^n$ are the upper and lower boundaries vectors for $x$. If any of the components of $x$ is bigger than its corresponding component in $ub$, i.e., $x_i > ub_i$, we make $x_i = ub_i$. The same applies if $x_i < lb_i$. In short, this strategy projects the value of the components that violate the constrains $x_i$ to the n-dimensional box formed by $ub$ and $lb$.

\subsection{Stopping Criteria for the EDS Method}
\label{sec:stopping_criteria}

Our last task is to define the stopping criteria for the \gls{eds} method, which is in fact straightforward. Assume that each newly computed critical point ($x$) is stored in a queue $\mathcal{M}$. The \gls{eds} method takes a point $x_i$ from $\mathcal{M}$ at each iteration and tries to compute new predictors (and hence a correctors) from this point and add the newly computed critical points to $\mathcal{M}$. It is then obvious to stop \gls{eds} whenever the queue $\mathcal{M}$ is empty. Nevertheless, if we keep adding points to $\mathcal{M}$ the queue will never be empty. Thus, criteria for adding new points to $\mathcal{M}$ should be defined.\\

It is indeed simple to define which points should be added to $\mathcal{M}$. Here is a list of all the conditions that will avoid a point $x$ from being added to $\mathcal{M}$:

\begin{enumerate}
\item Any point whose $\delta$ value is above the user defined thresholds $min\_\delta$ and $max\_\delta$ 
\item Any point $x$, whose function value $F(x)$ lies within a box that already contains a point (see Section \ref{sec:handling_kg2}).
\item Any point $x$, whose associated weight vector $\alpha \in \mathbb{R}^k$ does not comply with the condition: $max(\alpha) \leq 1 - tol$, where $0 < tol < 0.1$ is a user defined, problem dependent tolerance. This condition excludes points that lie on the boundary of the Pareto front and thus are likely to be dominated ones.
\end{enumerate}

Now we have all the necessary elements to introduce \gls{eds} \gls{pc} method. Its pseudo-code is presented in Algorithm \ref{alg:eds_pc}. 

\begin{algorithm}[H]
\caption{EDS Predictor-Corrector method}\label{alg:eds_pc}
\textbf{Input:} Starting local Pareto point $x_0 \in \mathbb{R}^n$ along with is associated weight vector $\alpha \in \mathbb{R}^k$, $\tau \in \mathbb{R}$ value defining the spreadness of the solutions, $\text{min\_}\delta \in \mathbb{R}^+$ and $\text{max\_}\delta \in \mathbb{R}^+$ thresholds and minimum step size threshold $\text{min\_}h \in \mathbb{R}^+$\\
\textbf{Output:} Finite size approximation of $\mathcal{P}$ and $\mathcal{F}^*$
	\begin{algorithmic}[1]
	
	\State Set $\mathcal{P} = \mathcal{P} \cup x_0$ and $\mathcal{F} = \mathcal{F} \cup F(x_0)$
	\State Compute $J(x)$
	\State Enqueue($\mathcal{M}$, $x_0$)
	
	\While{$\mathcal{M}$ not empty}
			
		\State $x = \text{Dequeue}(\mathcal{M})$	
		\State Compute a set of promising directions in objective space by Eq. \eqref{eq:qr_alpha}
		
		\For{each $q_i \in i = {2,k}$}
			\State Set $d = q_i$
			\State Compute a set of predictors $p_+$ and $p_-$ using Algorithm \ref{alg:predictor}
			
			\For{each predictor}
				\State Compute a corrector $x_1$ using Algorithm \ref{alg:corrector}			
				
				\If{$x_1$ does not meet any of the criteria in Section \ref{sec:stopping_criteria}}
					\State Enqueue($\mathcal{M}$, $x_1$)
					\State $\mathcal{P} = \mathcal{P} \cup x_1$
					\State $\mathcal{F} = \mathcal{F} \cup F(x_1)$
					
				\EndIf
			\EndFor
		
		\EndFor		
		
	\EndWhile
	
	\State return $[\mathcal{P}, \mathcal{F}]$   
	\end{algorithmic}
\end{algorithm}

Finally, one important aspect related to the performance of \gls{eds} method has to do with the information we keep or recompute regarding the current set of solutions. For each point in the queue $\mathcal{M}$, the corresponding function and $\alpha$ values are stored respectively. Furthermore, the Jacobian matrix is also stored. Storing all this information has a direct impact on the memory requirements of the method. Nevertheless, since one of our goals is to make the method efficient in terms of functions evaluations we have taken this approach to avoid recomputing information.

\subsection{Using Neighborhood Information}
\label{sec:neigh_info}

So far, the \gls{eds} method requires Jacobian information for the computation of $\nu$ via Eq. \eqref{eq:alternative1}. In this section, we present an alternative way to obtain such search directions $\nu$ without explicitly computing or approximating the Jacobian. Instead, the neighborhood information is exploited and used for the approximation of $\nu$. This method can be viewed as a particular finite difference method \cite{nocedal}, however, it has the advantage that the information of points already computed can be exploited in order to approximate the Jacobian matrix and hence some function evaluations can be saved. This is particularly interesting within the context of set-based optimization strategies such as \glspl{moea}. The approach taken here was inspired by the usage of neighborhood information in the \gls{ds} method \cite{directed_search} and the Gradient Subspace Approximation (\gls{gsa}) method \cite{gsa}.

The general idea behind the method is as follows: given a point $x_0$ that is designated for local search as well as another point $x_i$ whose function value is known that is in the vicinity of $x_0$, then the given information can be used to approximate the directional derivative in direction

\begin{equation}
\nu_i := \frac{(x_i - x_0)}{|| x_i - x_0 ||}, 
\label{eq:directional_der_info}
\end{equation}

without any additional cost (in terms of function evaluations). That is, it holds

\begin{equation}
f_{\nu_i}(x_0) = \langle \nabla f(x_0), v_i \rangle = \frac{f(x_i) - f(x_0)}{|| x_i - x_0 ||} + \mathcal{O}(|| x_i - x_0 ||).
\label{eq:directional_der}
\end{equation}

This can be seen by considering the forward difference quotient on the line search function $f_{\nu_i}(t) = f(x_0 + t \nu_i)$.

Now assume a candidate solution $x_0 \in \mathbb{R}^n$ is designated for local search and further $r$ search directions $\nu_i \in \mathbb{R}^n$, $i = 1,\ldots,n$, are given. Then, the matrix $\mathcal{A} := JV \in \mathbb{R}^{k \times r}$, where $V = (\nu_1, \ldots, \nu_r) \in \mathbb{R}^{n \times r}$, is as follows:

\begin{equation}
\mathcal{A} = JV = (\langle \nabla f_i(x), \nu_j \rangle)_{i = 1, \ldots, k. \quad j = 1,\ldots,n}.
\label{eq:neigh_matrix}
\end{equation}

Hence, every element $m_{ij}$ of $JV$ is defined by the value of the directional derivative of objective $f_i$ in direction $\nu_j$, an can be approximated as in Eq. \eqref{eq:directional_der}. An approximation to the Jacobian matrix can thus be obtained by computing

\begin{equation}
J \approx \mathcal{A} (V^T V)^{-1} V.
\label{eq:jacobian_approx}
\end{equation}

Crucial for the approximation of $\mathcal{A}$ is the choice of tests point $x_i$. If the function values of points in a neighborhood $N(x_0)$ are already known, it seems to be wise to include them to build the matrix $V$ (see Figure \ref{Fig:using_neighbors}). 

\begin{comment}

\begin{figure}[H] 
	\centering \def\svgwidth{150pt} 
	\input{img/eds_neighborhood.pdf_tex} 
	\caption{Neighbors used for the approximation of the Jacobian are spotted as triangles, squares are points outside the neighborhood $N(x_0)$} 
	\label{Fig:using_neighbors}
\end{figure}

\end{comment}

Nevertheless, it might be that further test points have to be sampled to obtain the $n$ neighbors. More precisely, assume that we are given $x_0 \in \mathbb{R}^n$ as well as $l$ neighboring solutions $x_1, \ldots, x_l \in N(x_0)$. One desirable property of all the remaining search directions is that they are both orthogonal to each other and orthogonal to the previous ones. In order to compute the new search directions $v_{l+1}, \ldots, v_r$, $r > 1$, one can proceed as follows: compute $V = QR = (q_1, \ldots, q_l, q_{l+1}, \ldots, q_n)R$. Then it is by construction $v_i \in span\{ q_1, \ldots, q_i \}$ for $i = 1, \ldots, l$, and hence

\begin{equation}
\langle v_i, q_j \rangle = 0, \qquad \forall i \in \{ i, \ldots, l \}, j \in \{ l+1, \ldots, r \}.
\label{eq:neigh_samples}
\end{equation} 

One can thus e.g. set

\begin{eqnarray}
v_{l+i} = q_{l+1}, \qquad i = 1, \ldots, r-1\\
x_{l+i} = x_0 + v_{l+i}, \qquad i = 1, \ldots, r-l
\label{eq:neigh_samples2}
\end{eqnarray}

This way, neighborhood information can be used to approximate the value of the Jacobian. By doing this, some function evaluations can be saved by using the information computed in previously iterations. It is also important to note that the choice of $r$ and the size of the neighborhood $N(x_0)$ has a direct impact on the quality of the method, it can be seen that for $r = k$ search directions $\nu_i$, $i = 1, \ldots, r$, one can find a descent direction $\nu$ by solving Eq. \eqref{eq:alternative1} and using $J$ as in Eq. \eqref{eq:jacobian_approx} regardless of $n$. However, by construction it is $\nu \in span\{\nu_1,\ldots,\nu_r\}$, which means that only a $r$-dimensional subspace of the $\mathbb{R}^n$ space is explored in each step. One would expect that the more search directions $\nu_i$ are taken into account, the better the choice of $\nu$ is. In fact, for the development of this work $r = n$ is used. As for the size of neighborhood, we recommend $0.01 \leq N(x_0) \leq 1$, nevertheless, it must be taken into account that this is a problem dependent parameter.

\subsection{Handling Mixed-Integer Problems}
\label{sec:handling_mips}

The \gls{eds} method is now capable of computing the connected components of \glspl{mop} with $k \geq 2$ in an efficient way. Nevertheless, our main objective in this work is the development of a continuation method for \glspl{mmop}. A simple modification can be done to the \gls{eds} method in order to make it capable, under a certain condition on the structure of the problem, of solving \glspl{mmop}.

The aforementioned condition has to do with the search space of the problem. Recall that \glspl{mmop} are defined almost exactly as \glspl{mop} except that the parameter space is defined by a mixture of real and discrete variables (see Section \ref{sec:mixed_integer_mops}). The key for the mechanism that allows the \gls{eds} method to solve \glspl{mmop} is the following.

\begin{mydef}[Domain restriction]
Let $f: \mathbf{E} \mapsto \mathbf{F}$ be a function from a set $\mathbf{E}$ to a set $\mathbf{F}$, so that the domain of $f$ is in $\mathbf{E}$ $(\mathrm{dom} \, f \subseteq \mathbf{E})$. If a set $\mathbf{A}$ is a subset of $\mathbf{E}$, then the restriction of $f$ to $\mathbf{A}$ is the function

\[f|_\mathbf{A}: \mathbf{A} \mapsto \mathbf{E}\]

Given two functions $f: \mathbf{A} \mapsto \mathbf{B}$ and $g: \mathbf{D} \mapsto \mathbf{B}$ such that $f$ is a restriction of $g$, that is, $\mathbf{A} \subseteq \mathbf{D}$ and $f = g |_\mathbf{A}$, then $g$ is an extension of $f$.
\label{def:domain_restriction}
\end{mydef} 

In this work we will only consider \glspl{mmop} whose parameter space can be \emph{extended} to the real domain. In other words, the parameter space $\mathbf{E} = \mathbb{R}^{d_1} \times \mathbb{Z}^{d_2} $ must be a \emph{restriction} of $\mathbb{R}^n$ for $n = d_1+d_2$. This condition is necessary for the computation of a direction $\nu$ such that $J(x) \nu = \delta d$, since the approximation of $J(x)$ requires that samples in a small neighborhood of $x$ can be taken (see Section \ref{sec:neigh_info}). In case this sampling process is not possible, $\nu$ direction can not be computed.

Thus, the \gls{eds} method can only be used to solve \glspl{mmop} that comply with the aforementioned condition. The treatment of mixed-integer variables is achieved by the use of the following rounding operator: 

\begin{mydef}[Rounding operator]

Let $d \in \mathbb{R}^k$ be a direction in objective space and let $x \in \mathbb{R}^{d_1} \times \mathbb{Z}^{d_2}$ be a point from which we would like to move in a direction $\nu \in \mathbb{R}^n$ such that $J(x) \nu = \delta d$ for a given step size $t \in \mathbb{R}$. Let also be $\mathcal{R} \subseteq \mathbb{Z}^+$ and $\mathcal{I} \subseteq \mathbb{Z}^+$ two sets of indexes, where $\mathcal{R}$ is the set of indexes that denote the real components of the vector $x$ and $\mathcal{I}$ is the set of indexes denote the integer components of the vector $x$. Finally, let $\lambda \in (0,1]$. Then, a movement from $x$ in direction $\nu$ can be performed by

\[
	round(x_j, \nu_j) = 
	\begin{cases}
	x_j + t \nu_j, & if \quad j \in \mathcal{R}\\ 
	x_j + ceil(\nu_j), & if \quad j \in \mathcal{I} \text{ and } \nu_j \geq 0 \text{ and } abs(\nu_j) \geq \lambda\\
	x_j + floor(\nu_j), & if \quad j \in \mathcal{I} \text{ and } \nu_j < 0 \text{ and } abs(\nu_j) \geq \lambda
	\end{cases}
\]

\end{mydef}

Hence, performing a movement from a point $x \in \mathbb{R}^{d_1} \times \mathbb{Z}^{d_2}$  in direction $\nu \in \mathbb{R}^n$, such that the outcome of such movement is a new point $\hat{x} \in \mathbb{R}^{d_1} \times \mathbb{Z}^{d_2}$ can be achieved by applying the $rounding$ operator on each of the components of $x$.

The main task of the $rounding$ operator, is to ensure that the point $\hat{x}$, which results from performing the line search along $\nu \in \mathbb{R}^n$, belongs to the appropriate space, that is, to the mixed-integer space in case the problem is mixed-integer or to the real space in case of dealing with a problem defined within the real space. 

Recall that $\nu \in \mathbb{R}^n$ is the greediest direction in parameter space such that $J(x) \nu = \delta d$, that is, for a step size $t \in \mathbb{R}$, a movement along $\nu$, i.e. $\hat{x} = x + t \nu$, maps to a movement $d \in \mathbb{R}^k$ in objective space. Let us assume that $x \in \mathbb{R}^{d_1} \times \mathbb{Z}^{d_2}$ (that is $x$ is a mixed-integer variable), since $\nu \in \mathbb{R}^n$ belongs to the real space if we perform the line search as stated before, $\hat{x}$ will belong to the real space instead of belonging to a mixed-integer one. Thus, in order to keep $\hat{x}$ in the appropriate space we have developed the $rounding$ operator.

We will now explain the role of the threshold $\lambda$. By considering that each of the components of $\nu$ determines how much to move in each coordinate, it can be seen that the bigger the magnitude of a certain coordinate direction, the greater the reason to increase/decrease such component in the new point $\hat{x}$. Therefore, the necessity for a threshold on the magnitude of each component of the direction $v$ arises, we have decided to call such threshold $\lambda$ and although it is a user defined parameter, our experiments have shown that setting $\lambda \in [0.2, 0.4]$ usually leads to good results.

Finally, the reason for choosing \emph{floor} and \emph{ceil} functions over \emph{round} is due to the following observation: \emph{round} function is always equivalent to the result of the \emph{floor} function if  $\lambda < 0.5$, hence, if the user defines $\lambda \in (0, 0.5)$ the value of the $i$-th component will always be mapped to the smallest following integer. The situation is analogous for the \emph{ceil} function when $\lambda \in [0.5, 1)$. This is of course an undesirable behavior according to the above discussion, thus, \emph{floor} and \emph{ceil} functions are used instead of \emph{round} since they provide more flexibility for mapping the real components of $\nu$ to an integer space.\\

Hence, to make the \gls{eds} method capable of solving \glspl{mmop} the application of the $rounding$ operator is necessary for every new point that is computed.

\subsection{An Example of the EDS Method}

In the following we would like to exemplify how the various parameters of the \gls{eds} method have an impact on the performance and the reliability of the method. For this example we will use the bi-objective function described in \cite{box_algorithm}, which is defined as follows:

\begin{eqnarray}
& f_1(x) = & (x_2 - a_2)^2 + (x_1 - a_1)^4 \nonumber\\
& f_2(x) =  & (x_1 - b_1)^2 + (x_2 - b_2)^4
\label{eq:biobjective3}
\end{eqnarray}

for $a = (1, 1)$ and $b = -a$. This function has a convex Pareto front with a non-linear Pareto set. Figure \ref{Fig:example_biobjective_eds} shows the resolution of a bi-objective problem using the \gls{eds} method, for this example we set $\tau = 2$ since our goal is to depict predictor and corrector steps. Blue dots represent the real Pareto front and set. The points computed by the \gls{eds} method are depicted in red color, green arrows represent the predictor directions while red arrows represent corrector directions. As can be observed in the picture, the predictor directions are not necessarily tangent to the Pareto set, it can also be observed that the computed points do not necessarily lie on the Pareto set but instead on the Pareto front. Also note that two predictors are computed per point, nevertheless not all of them lead to new correctors (see Section \ref{sec:handling_kg2}).

\begin{comment}

\begin{figure}[H]
    \subfloat[Objective space \label{Fig:example_biobjective3_pf}]{%
    	\centering \includegraphics[width = 60mm, height = 60mm]{img/biobjective3_front.png}
    }
    \hfill
    \subfloat[Decision space \label{Fig:example_biobjective3_ps}]{%
    	\centering \includegraphics[width = 60mm, height = 60mm]{img/biobjective3_set.png}
    }
    \caption{Resolution of a bi-objective problem by using the EDS method}
    \label{Fig:example_biobjective_eds}
\end{figure}

\end{comment}

The computed Pareto front (\gls{pf}) for this example with $\tau = 3$ can be seen in Figure \ref{Fig:example_biobjective3_tau3}, a finer discretization of the objective space can be seen in Figure \ref{Fig:example_biobjective3_tau1} where $\tau = 1$. An even finer discretization, where $\tau = 0.5$, is shown in Figure \ref{Fig:example_biobjective3_tau05}.

\begin{comment}

\begin{figure}[H]
\hspace*{-1cm}
    \subfloat[Computed PF for $\tau = 3$ \label{Fig:example_biobjective3_tau3}]{%
    	\centering \includegraphics[width = 55mm, height = 55mm]{img/biobjective3_front_tau3.png}
    }
    \subfloat[Computed PF for $\tau = 1$ \label{Fig:example_biobjective3_tau1}]{%
    	\centering \includegraphics[width = 55mm, height = 55mm]{img/biobjective3_front_tau1.png}
    }
    \subfloat[Computed PF for $\tau = 0.5$ \label{Fig:example_biobjective3_tau05}]{%
    	\centering \includegraphics[width = 55mm, height = 55mm]{img/biobjective3_front_tau05.png}
    }
    \caption{Pareto fronts computed by the EDS method for different values of $\tau$}
    \label{Fig:example_biobjective3_different_tau}
\end{figure}

\end{comment}

The number of solutions computed for each of the $\tau$ values is displayed in Table \ref{table:sols_per_tau}. As it can be observed the lower the value of $\tau$ the more solutions we get.

\begin{table}[H]
\centering
\begin{tabular}{| r  r |}
	\hline
	$\tau$ value & Solutions\\  
  	\hline
  	3 & 14\\
  	1 & 44\\
  	0.5 & 80\\
  	\hline
\end{tabular}
\caption{Number of solutions computed for the different values of $\tau$}
\label{table:sols_per_tau}
\end{table}

The plots displayed in Picture \ref{Fig:example_biobjective3_different_tau} along with the results in Table \ref{table:sols_per_tau} help to have a better understanding of the $\tau$ role.

\subsubsection{On the Impact of the $\delta$ Thresholds} 

Here we would like to demonstrate how both $\delta$ thresholds impact on the overall performance of the \gls{eds} method. For this we will consider test problem \eqref{eq:biobjective3} again. We set $\tau = 0.5$ and will show some combinations of the max\_$\delta$ and min\_$\delta$ in order to let the reader gain a bigger understanding of the role of such thresholds. Table \ref{table:sols_different_deltas} displays five different settings for the $\delta$ thresholds in the max\_$\delta$ and min\_$\delta$ columns along with the number of solutions, number of function evaluations and the $\bigtriangleup_2$ values for each of the combinations.

\begin{table}[H]
\centering
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{| r  r  r  r  r  r  r |}
	\hline
	Setting & max\_$\delta$ & min\_$\delta$ & Solutions & Feval & $\bigtriangleup_2$ & Avg. Feval/Sols\\  
  	\hline
	1 & $10^{-1}$ & $10^{-3}$ & 81 & 423 & 0.15 & 5.2\\
	2 & $10^{-1}$ & $10^{-2}$ & 77 & 320 & 0.19 & 4.1\\
	3 & $10^{-1}$ & $10^{-1}$ & 77 & 269 & 0.19 & 3.4\\
	4 & $10^{-2}$ & $10^{-3}$ & 26 & 164 & 10.16 & 6.3\\
	5 & $10^{-3}$ & $10^{-3}$ & 7 & 45 & 15.53 & 6.4\\
  	\hline
\end{tabular}
\end{adjustbox}
\caption{Impact of the $\delta$ thresholds on the overall performance of the EDS method}
\label{table:sols_different_deltas}
\end{table}

As can be observed by the results in Table \ref{table:sols_different_deltas} the choice of the values of the $\delta$ thresholds have a large impact on the performance of the \gls{eds} method. In some cases this impact is so large that the \gls{eds} method is unable to compute further correctors and hence stops it execution as was the case for the last two settings in Table \ref{table:sols_different_deltas}. Also note that, as expected, the more tight these thresholds are the more function evaluations are used per computed solution. Therefore a good balance between max\_$\delta$ and min\_$\delta$ thresholds has to be considered in order to obtain reliable results.

\subsubsection{On the Impact of the Size of the Neighborhood $N(x)$}

As mentioned in Section \ref{sec:neigh_info} neighboring information can be used in order to save function evaluations and, therefore, improve the performance of the \gls{eds} method. Here we would like to demonstrate, trough some examples, how the choice of the size of the neighborhood $N(x)$ impacts the performance of the \gls{eds} method.

Once again we will use function \eqref{eq:biobjective3} for our examples. We set $\tau = 0.5$, $\text{max\_}\delta = 10^{-1}$ and $\text{min\_}\delta = 10^{-3}$. For this example we put special emphasis on the number of function evaluations, the number of neighboring solutions used in the computation and the quality of the computed solutions (measured by the $\bigtriangleup_2$ indicator). Table \ref{table:impact_of_nx} summarizes the results obtained for five different neighborhood sizes.

\begin{table}[!htb]
\centering
\begin{tabular}{| r  r  r  r  r  r |}
	\hline
	Size of $N(x)$ & Solutions & Feval & \# Neighbors & $\bigtriangleup_2$ & Avg. Feval/Sols\\  
  	\hline
	0 & 80 & 502 & 0 & 0.153 & 6.2\\
	0.01 & 84 & 458 & 28 & 0.158 & 5.4\\
	0.02 & 80 & 396 & 163 & 0.140 & 4.9\\
	0.03 & 89 & 357 & 177 & 0.164 & 4\\
	0.04 & 74 & 344 & 249 & 0.164 & 4.6\\
	0.05 & 78 & 334 & 268 & 0.171 & 4.2\\
  	\hline
\end{tabular}
\caption{Impact of the size of $N(x)$ on the overall performance of the EDS method}
\label{table:impact_of_nx}
\end{table}

First of all note that the first setting is almost same as the first setting in Table \ref{table:sols_different_deltas} but here we increased the tolerance for smaller step sizes in the corrector, hence, this setting may have slightly more correctors than the setting in Table \ref{table:sols_different_deltas}, this explains with this second setting uses more function evaluations the former. Note, by the data in Table \ref{table:impact_of_nx}, that by increasing the size of the neighborhood more points can be reused, nevertheless, the bigger the size of the neighborhood is the less accurate mapping \eqref{eq:alternative1} and hence, the more function evaluations needed to reach a new critical point by means of the corrector phase. Note the negative impact this effect has on the quality of the solution computed. This effect also explains why although the number of neighboring solutions reused increases in the last three settings, the number of function evaluations required by the method does not improve much. In an extreme case, where the size of the neighborhood is too large, the corrector phase may not be able to compute further critical points, leading to a premature termination of the \gls{eds} method.