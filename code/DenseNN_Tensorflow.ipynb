{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected NN with tensorflow\n",
    "\n",
    "First we import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidlaredorazo/anaconda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from data_handler_CMAPS import CMAPSDataHandler\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_model_weights(sess):\n",
    "    for v in tf.trainable_variables():\n",
    "        \n",
    "        layer = sess.run(v)\n",
    "        \n",
    "        print(v.name)\n",
    "        print(layer.shape)\n",
    "        print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(input_shape, output_shape):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None,input_shape), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=None, name=\"y\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def multilayer_perceptron(X):\n",
    "    \n",
    "    l2_regularizer = 0.0\n",
    "    \n",
    "    X = tf.layers.dense(X, 20, activation=tf.nn.relu, \n",
    "                    kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False, seed=0), \n",
    "                    kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_regularizer))\n",
    "    \n",
    "    X = tf.layers.dense(X, 20, activation=tf.nn.relu, \n",
    "                    kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False, seed=0), \n",
    "                    kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_regularizer))\n",
    "    \n",
    "    X = tf.layers.dense(X, 1, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), \n",
    "                       kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_regularizer))\n",
    "    \n",
    "    return X \n",
    "\n",
    "def multilayer_perceptron2(X):\n",
    "    \n",
    "    l2_regularizer = 0.0\n",
    "    \n",
    "    A1 = tf.layers.dense(X, 20, activation=tf.nn.relu, \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False, seed=0), name=\"fc1\")\n",
    "    \n",
    "    y = tf.layers.dense(A1, 1, activation=None, \n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False, seed=0), name=\"out\")\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def get_minibatches(X_full, y_full, batch_size):\n",
    "    \n",
    "    full_size = X_full.shape[0]\n",
    "    total_batches = math.floor(full_size/batch_size)\n",
    "    remainder = full_size - total_batches*batch_size\n",
    "    \n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    for i in range(total_batches):\n",
    "        X_batches.append(X_full[i*batch_size:(i+1)*batch_size])\n",
    "        y_batches.append(y_full[i*batch_size:(i+1)*batch_size])\n",
    "        \n",
    "    if remainder != 0:\n",
    "        X_batches.append(X_full[total_batches*batch_size:])\n",
    "        y_batches.append(y_full[total_batches*batch_size:])\n",
    "        total_batches = total_batches+1\n",
    "        \n",
    "    return X_batches, y_batches, total_batches\n",
    "        \n",
    "\n",
    "def train_ann(X_train, y_train, epochs, batch_size, display_step = 1, lamba2 = 0.0):\n",
    "    \n",
    "    seed(0)\n",
    "    set_random_seed(0)\n",
    "    \n",
    "    input_shape = X_train.shape[1]\n",
    "    output_shape = 1\n",
    "    \n",
    "    X, y = create_placeholders(input_shape, output_shape)\n",
    "    \n",
    "    predictions = multilayer_perceptron2(X)\n",
    "    cost = tf.losses.mean_squared_error(y, predictions) \n",
    "    reg_cost = tf.losses.get_regularization_loss()\n",
    "    total_cost = cost + reg_cost\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(total_cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #To reset all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        avg_cost = 0.0\n",
    "        \n",
    "        print_model_weights(sess)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            cost_tot = 0.0\n",
    "            cost_reg_tot = 0.0\n",
    "            #total_batch = int(X_train.shape[0]/batch_size)\n",
    "            #X_batches = np.array_split(X_train, total_batch)\n",
    "            #y_batches = np.array_split(y_train, total_batch)\n",
    "            \n",
    "            X_batches, y_batches, total_batch = get_minibatches(X_train, y_train, batch_size)\n",
    "            \n",
    "            #Train with the minibatches\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = X_batches[i], y_batches[i]\n",
    "                \n",
    "                #print(batch_x.shape)\n",
    "                \n",
    "                _, c_reg, c, reg_cost_val = sess.run([optimizer, total_cost, cost, reg_cost], feed_dict={X:batch_x, y:batch_y})\n",
    "                cost_tot += c\n",
    "                cost_reg_tot += c_reg\n",
    "                \n",
    "                #print(reg_cost_val)\n",
    "                \n",
    "            avg_cost = cost_tot/total_batch\n",
    "            avg_cost_reg = cost_reg_tot/total_batch\n",
    "                \n",
    "            if epoch%display_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \"cost_reg=\", \"{:.9f}\".format(avg_cost_reg), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "                    \n",
    "        print(\"Training complete!\")\n",
    "        print(\"Epoch:Final\", \"cost_reg=\", \"{:.9f}\".format(avg_cost_reg), \"cost=\", \"{:.9f}\".format(avg_cost)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Data Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for dataset 1 with window_size of 30, stride of 1 and maxRUL of 128. Cros-Validation ratio 0\n",
      "Loading data from file and computing dataframes\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(17731, 420)\n",
      "(17731, 1)\n",
      "Testing data (X, y)\n",
      "(100, 420)\n",
      "(100, 1)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[-0.58075601 -0.0455243  -0.27982732 ..., -0.81818182  0.43307087\n",
      "   0.4679733 ]\n",
      " [-0.35395189  0.0629156  -0.18014129 ..., -0.45454545  0.25984252\n",
      "   0.25294702]\n",
      " [-0.21649485 -0.13299233 -0.13854003 ..., -0.45454545  0.38582677\n",
      "   0.72049425]\n",
      " [-0.21649485 -0.39897698 -0.2299843  ..., -0.45454545  0.08661417\n",
      "   0.29640676]\n",
      " [-0.20274914 -0.39590793 -0.05926217 ..., -0.45454545  0.05511811\n",
      "   0.17880983]]\n",
      "[[ 128.]\n",
      " [ 128.]\n",
      " [ 128.]\n",
      " [ 128.]\n",
      " [ 128.]]\n",
      "Testing data (X, y)\n",
      "[[-0.65635739 -0.10946292 -0.48312402 ..., -0.27272727  0.05511811\n",
      "   0.30947309]\n",
      " [ 0.03780069 -0.07365729 -0.27629513 ..., -0.63636364  0.05511811\n",
      "   0.04416986]\n",
      " [ 0.13402062 -0.08644501  0.038854   ...,  0.09090909  0.24409449\n",
      "   0.07882403]\n",
      " [-0.14776632  0.16828645  0.00431711 ...,  0.09090909 -0.30708661\n",
      "   0.03365999]\n",
      " [-0.05841924  0.24654731  0.04317111 ..., -0.09090909 -0.03937008\n",
      "   0.46996165]]\n",
      "[[ 112.]\n",
      " [  98.]\n",
      " [  69.]\n",
      " [  82.]\n",
      " [  91.]]\n"
     ]
    }
   ],
   "source": [
    "#Selected as per CNN paper\n",
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../CMAPSSData'\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 128\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "dHandler_cmaps = CMAPSDataHandler(data_folder, 1, selected_features, max_rul=max_rul,\n",
    "                                  sequence_length=window_size, sequence_stride=window_stride)\n",
    "\n",
    "dHandler_cmaps.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "\n",
    "#Rescale the data\n",
    "dHandler_cmaps.X_train = min_max_scaler.fit_transform(dHandler_cmaps.X_train)\n",
    "dHandler_cmaps.X_test = min_max_scaler.transform(dHandler_cmaps.X_test)\n",
    "\n",
    "dHandler_cmaps.print_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1/kernel:0\n",
      "(420, 20)\n",
      "[[ 0.00952573 -0.13807291  0.02388828 ...,  0.0425049   0.11824349\n",
      "   0.00237017]\n",
      " [-0.06897566  0.10282086 -0.01936359 ...,  0.04727668  0.04241179\n",
      "  -0.1119597 ]\n",
      " [ 0.05336403  0.03035624  0.02375947 ...,  0.05647624  0.06885112\n",
      "   0.11151468]\n",
      " ..., \n",
      " [-0.07528567  0.01343323  0.01169548 ...,  0.07860185 -0.05693797\n",
      "   0.0197363 ]\n",
      " [ 0.00382707 -0.00654522  0.04621871 ..., -0.08136505 -0.02641646\n",
      "   0.10063449]\n",
      " [-0.00072989  0.12355479 -0.05378776 ...,  0.00576838 -0.0329321\n",
      "   0.03504931]]\n",
      "fc1/bias:0\n",
      "(20,)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "out/kernel:0\n",
      "(20, 1)\n",
      "[[ 0.04360284]\n",
      " [-0.63201177]\n",
      " [ 0.10934566]\n",
      " [-0.46428487]\n",
      " [ 0.19505545]\n",
      " [-0.33840579]\n",
      " [-0.20801905]\n",
      " [-0.468016  ]\n",
      " [-0.36447951]\n",
      " [ 0.17511755]\n",
      " [-0.31545013]\n",
      " [-0.41481665]\n",
      " [-0.36223534]\n",
      " [ 0.23793462]\n",
      " [ 0.3529304 ]\n",
      " [-0.43133992]\n",
      " [ 0.65901202]\n",
      " [ 0.19456096]\n",
      " [ 0.54124498]\n",
      " [ 0.01084916]]\n",
      "out/bias:0\n",
      "(1,)\n",
      "[ 0.]\n",
      "Epoch: 0001 cost_reg= 7819.079227121 cost= 7819.079227121\n",
      "Epoch: 0002 cost_reg= 6402.038281250 cost= 6402.038281250\n",
      "Epoch: 0003 cost_reg= 4807.000697545 cost= 4807.000697545\n",
      "Epoch: 0004 cost_reg= 3329.012447684 cost= 3329.012447684\n",
      "Epoch: 0005 cost_reg= 2252.441971261 cost= 2252.441971261\n",
      "Epoch: 0006 cost_reg= 1664.438088117 cost= 1664.438088117\n",
      "Epoch: 0007 cost_reg= 1371.737233189 cost= 1371.737233189\n",
      "Epoch: 0008 cost_reg= 1176.796858433 cost= 1176.796858433\n",
      "Epoch: 0009 cost_reg= 998.509440831 cost= 998.509440831\n",
      "Epoch: 0010 cost_reg= 837.316240147 cost= 837.316240147\n",
      "Epoch: 0011 cost_reg= 708.805381121 cost= 708.805381121\n",
      "Epoch: 0012 cost_reg= 613.569413975 cost= 613.569413975\n",
      "Epoch: 0013 cost_reg= 548.499775478 cost= 548.499775478\n",
      "Epoch: 0014 cost_reg= 507.166869681 cost= 507.166869681\n",
      "Epoch: 0015 cost_reg= 481.997526768 cost= 481.997526768\n",
      "Epoch: 0016 cost_reg= 466.376704189 cost= 466.376704189\n",
      "Epoch: 0017 cost_reg= 455.840410505 cost= 455.840410505\n",
      "Epoch: 0018 cost_reg= 447.754705811 cost= 447.754705811\n",
      "Epoch: 0019 cost_reg= 440.797022792 cost= 440.797022792\n",
      "Epoch: 0020 cost_reg= 434.351987130 cost= 434.351987130\n",
      "Training complete!\n",
      "Epoch:Final cost_reg= 434.351987130 cost= 434.351987130\n"
     ]
    }
   ],
   "source": [
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 128\n",
    "num_features = len(selected_features)\n",
    "shape = num_features*window_size\n",
    "\n",
    "#Train the model\n",
    "X = dHandler_cmaps.X_train\n",
    "y = dHandler_cmaps.y_train\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 512\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = train_ann(X, y, epochs=epochs, batch_size=batch_size, display_step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
