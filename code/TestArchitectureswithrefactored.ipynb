{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Test different MLP architectures. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import custom_scores\n",
    "from data_handler_CMAPS import CMAPSDataHandler\n",
    "from tunable_model import SequenceTunableModelRegression\n",
    "import CMAPSAuxFunctions\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define architectures\n",
    "\n",
    "Define each one of the different architectures to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()  #Clear the previous tensorflow graph\n",
    "\n",
    "lambda_regularization = 0.20\n",
    "\n",
    "def RULmodel_SN_5(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(lambda_regularization), name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(model_def, shape, model_type='ann'):\n",
    "\n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0, beta_1=0.5)\n",
    "    lossFunction = \"mean_squared_error\"\n",
    "    metrics = [\"mse\"]\n",
    "    model = None\n",
    "\n",
    "    #Create and compile the models\n",
    "\n",
    "    if model_type=='ann':\n",
    "        model = model_def(shape)\n",
    "        model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    elif model_type=='lstm':\n",
    "        \"\"\"\n",
    "        nFeatures = len(selected_features)\n",
    "        shapeLSTM = (window_size, nFeatures)\n",
    "        model = RULmodel_LSTM(shape)\n",
    "        model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\"\"\"\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the usable models for this notebook\n",
    "\n",
    "models = {'shallow-20':RULmodel_SN_5}\n",
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../CMAPSSData'\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 128\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "dHandler_cmaps = CMAPSDataHandler(data_folder, 1, selected_features, max_rul, \n",
    "                                  window_size, window_stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0, beta_1=0.5)\n",
    "lossFunction = \"mean_squared_error\"\n",
    "metrics = [\"mse\"]\n",
    "\n",
    "\n",
    "#Create and compile the models\n",
    "nFeatures = len(selected_features)\n",
    "shapeSN = nFeatures*window_size\n",
    "modelRULSN = get_compiled_model(models['shallow-20'], shapeSN, model_type='ann')\n",
    "\n",
    "tModel = SequenceTunableModelRegression('ModelRUL_SN_5', modelRULSN, lib_type='keras', data_handler=dHandler_cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for dataset 1 with window_size of 30, stride of 1 and maxRUL of 128. Cros-Validation ratio 0\n",
      "Loading data from memory without recomputing df\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(17731, 420)\n",
      "(17731,)\n",
      "Testing data (X, y)\n",
      "(100, 420)\n",
      "(100,)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[-0.58075601 -0.0455243  -0.27982732 ... -0.81818182  0.43307087\n",
      "   0.4679733 ]\n",
      " [-0.35395189  0.0629156  -0.18014129 ... -0.45454545  0.25984252\n",
      "   0.25294702]\n",
      " [-0.21649485 -0.13299233 -0.13854003 ... -0.45454545  0.38582677\n",
      "   0.72049425]\n",
      " [-0.21649485 -0.39897698 -0.2299843  ... -0.45454545  0.08661417\n",
      "   0.29640676]\n",
      " [-0.20274914 -0.39590793 -0.05926217 ... -0.45454545  0.05511811\n",
      "   0.17880983]]\n",
      "[128. 128. 128. 128. 128.]\n",
      "Testing data (X, y)\n",
      "[[-0.65635739 -0.10946292 -0.48312402 ... -0.27272727  0.05511811\n",
      "   0.30947309]\n",
      " [ 0.03780069 -0.07365729 -0.27629513 ... -0.63636364  0.05511811\n",
      "   0.04416986]\n",
      " [ 0.13402062 -0.08644501  0.038854   ...  0.09090909  0.24409449\n",
      "   0.07882403]\n",
      " [-0.14776632  0.16828645  0.00431711 ...  0.09090909 -0.30708661\n",
      "   0.03365999]\n",
      " [-0.05841924  0.24654731  0.04317111 ... -0.09090909 -0.03937008\n",
      "   0.46996165]]\n",
      "[112.  98.  69.  82.  91.]\n"
     ]
    }
   ],
   "source": [
    "tModel.data_scaler = min_max_scaler\n",
    "#tModel.data_handler.data_scaler = min_max_scaler\n",
    "\n",
    "tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "tModel.print_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Out Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating statistics for model shallow-20\n",
      "Working on dataset 1\n",
      "Loading data from file and computing dataframes\n",
      "100/100 [==============================] - 0s 229us/step\n",
      "100/100 [==============================] - 0s 269us/step\n",
      "100/100 [==============================] - 0s 339us/step\n",
      "100/100 [==============================] - 0s 379us/step\n",
      "100/100 [==============================] - 0s 419us/step\n",
      "100/100 [==============================] - 0s 499us/step\n",
      "100/100 [==============================] - 0s 539us/step\n",
      "100/100 [==============================] - 0s 608us/step\n",
      "100/100 [==============================] - 0s 668us/step\n",
      "100/100 [==============================] - 0s 708us/step\n",
      "Results for model shallow-20\n",
      "DescribeResult(nobs=10, minmax=(array([39.43805269]), array([85.32350204])), mean=array([46.0339571]), variance=array([192.68643696]), skewness=array([2.6121759]), kurtosis=array([4.94179917]))\n",
      "DescribeResult(nobs=10, minmax=(array([124.55529856]), array([5176.212206])), mean=array([727.53684183]), variance=array([2448429.83658641]), skewness=array([2.65628766]), kurtosis=array([5.07843187]))\n",
      "DescribeResult(nobs=10, minmax=(array([22.40898531]), array([23.07903548])), mean=array([22.72294969]), variance=array([0.0492071]), skewness=array([0.01072428]), kurtosis=array([-1.09118972]))\n",
      "Working on dataset 2\n",
      "Loading data from file and computing dataframes\n",
      "259/259 [==============================] - 0s 316us/step\n",
      "259/259 [==============================] - 0s 350us/step\n",
      "259/259 [==============================] - 0s 354us/step\n",
      "259/259 [==============================] - 0s 416us/step\n",
      "259/259 [==============================] - 0s 404us/step\n",
      "259/259 [==============================] - 0s 416us/step\n",
      "259/259 [==============================] - 0s 443us/step\n",
      "259/259 [==============================] - 0s 462us/step\n",
      "259/259 [==============================] - 0s 501us/step\n",
      "259/259 [==============================] - 0s 543us/step\n",
      "Results for model shallow-20\n",
      "DescribeResult(nobs=10, minmax=(array([52.51192179]), array([76.02064849])), mean=array([59.93104859]), variance=array([42.70604184]), skewness=array([1.58476228]), kurtosis=array([1.82851228]))\n",
      "DescribeResult(nobs=10, minmax=(array([530.03741266]), array([211507.24540555])), mean=array([25896.28546141]), variance=array([4.2810597e+09]), skewness=array([2.63509663]), kurtosis=array([5.00951895]))\n",
      "DescribeResult(nobs=10, minmax=(array([28.09443473]), array([29.11749618])), mean=array([28.64163181]), variance=array([0.12919143]), skewness=array([-0.2552991]), kurtosis=array([-1.39913527]))\n",
      "Working on dataset 3\n",
      "Loading data from file and computing dataframes\n",
      "100/100 [==============================] - 0s 1ms/step\n",
      "100/100 [==============================] - 0s 1ms/step\n",
      "100/100 [==============================] - 0s 1ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "Results for model shallow-20\n",
      "DescribeResult(nobs=10, minmax=(array([39.69748103]), array([49.68440399])), mean=array([44.95246313]), variance=array([11.16006523]), skewness=array([-0.48109876]), kurtosis=array([-1.07179385]))\n",
      "DescribeResult(nobs=10, minmax=(array([152.60084624]), array([2011.96872919])), mean=array([742.37949514]), variance=array([304627.48016431]), skewness=array([1.0789433]), kurtosis=array([0.87413116]))\n",
      "DescribeResult(nobs=10, minmax=(array([29.65425132]), array([30.58176724])), mean=array([30.1414641]), variance=array([0.1069819]), skewness=array([-0.05431951]), kurtosis=array([-1.37139616]))\n",
      "Working on dataset 4\n",
      "Loading data from file and computing dataframes\n",
      "248/248 [==============================] - 0s 808us/step\n",
      "248/248 [==============================] - 0s 833us/step\n",
      "248/248 [==============================] - 0s 845us/step\n",
      "248/248 [==============================] - 0s 885us/step\n",
      "248/248 [==============================] - 0s 897us/step\n",
      "248/248 [==============================] - 0s 949us/step\n",
      "248/248 [==============================] - 0s 957us/step\n",
      "248/248 [==============================] - 0s 997us/step\n",
      "248/248 [==============================] - 0s 1ms/step\n",
      "248/248 [==============================] - 0s 1ms/step\n",
      "Results for model shallow-20\n",
      "DescribeResult(nobs=10, minmax=(array([54.25369766]), array([59.99082591])), mean=array([56.88958142]), variance=array([4.06420469]), skewness=array([0.31129031]), kurtosis=array([-1.29121607]))\n",
      "DescribeResult(nobs=10, minmax=(array([638.90913782]), array([52417.0113484])), mean=array([7057.39869832]), variance=array([2.55968632e+08]), skewness=array([2.62899487]), kurtosis=array([4.99212584]))\n",
      "DescribeResult(nobs=10, minmax=(array([30.76305911]), array([31.82171211])), mean=array([31.25919153]), variance=array([0.18193246]), skewness=array([0.07491765]), kurtosis=array([-1.62088642]))\n"
     ]
    }
   ],
   "source": [
    "datasets = [1,2,3,4]\n",
    "iterations = 10\n",
    "tModel.epochs = 200\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "scores ={1:[], 2:[], 3:[], 4:[]}\n",
    "window_sizes = {1:30,2:20,3:30,4:18}\n",
    "strides = {1:1,2:2,3:1,4:2}\n",
    "max_ruls = {1:128, 2:134, 3:128, 4:134}\n",
    "num_features = len(selected_features)\n",
    "\n",
    "input_shape = None\n",
    "\n",
    "#For each model\n",
    "for key, model_def in models.items():\n",
    "    file = open(\"results/MLP/ResultsDatasets_ScalerAtEnd\"+key+\".csv\", \"w\")\n",
    "    csvfile = csv.writer(file, lineterminator='\\n')\n",
    "    \n",
    "    print(\"Generating statistics for model \" + key)\n",
    "\n",
    "    #For each dataset\n",
    "    for i in range(1,5):\n",
    "        \n",
    "        print(\"Working on dataset \" + str(i))\n",
    "        \n",
    "        tempScoresRMSE = np.zeros((iterations,1))\n",
    "        tempScoresRHS = np.zeros((iterations,1))\n",
    "        tempTime = np.zeros((iterations,1))\n",
    "        \n",
    "        input_shape = window_sizes[i]*num_features #For simple ANN\n",
    "        #input_shape = (window_sizes,num_features) #For RNN\n",
    "        \n",
    "        tModel.data_handler.change_dataset(i)\n",
    "        tModel.data_handler.sequence_length = window_sizes[i]\n",
    "        tModel.data_handler.sequence_stride = strides[i]\n",
    "        tModel.data_handler.max_rul = max_ruls[i]\n",
    "        tModel.load_data(unroll=True, verbose=0, cross_validation_ratio=0)\n",
    "        \n",
    "        #tModel.print_data()\n",
    "        \n",
    "        for j in range(iterations):\n",
    "\n",
    "            #Model needs to be recompiled everytime since they are different runs so weights should be reinit\n",
    "            model = get_compiled_model(model_def, input_shape, model_type='ann')\n",
    "\n",
    "            tModel.change_model(key, model, 'keras')\n",
    "            tModel.train_model(learningRate_scheduler=lrate, verbose=0)\n",
    "            tModel.evaluate_model(['rhs', 'rmse'], round=2)\n",
    "            #print(\"scores\")\n",
    "            \n",
    "            #print(j)\n",
    "\n",
    "            cScores = tModel.scores\n",
    "            rmse = math.sqrt(cScores['score_1'])\n",
    "            rmse2 = cScores['rmse']\n",
    "            rhs = cScores['rhs']\n",
    "            time = tModel.train_time\n",
    "            \n",
    "            tempScoresRMSE[j] = rmse2\n",
    "            tempScoresRHS[j] = rhs\n",
    "            tempTime[j] = time\n",
    "            \n",
    "        print(\"Results for model \" + key)\n",
    "    \n",
    "        print(stats.describe(tempScoresRMSE))\n",
    "        print(stats.describe(tempScoresRHS))\n",
    "        print(stats.describe(tempTime))\n",
    "            \n",
    "        tempScoresRMSE = np.reshape(tempScoresRMSE, (iterations,))\n",
    "        tempScoresRHS = np.reshape(tempScoresRHS, (iterations,))\n",
    "        tempTime = np.reshape(tempTime, (iterations,))\n",
    "        csvfile.writerow(tempScoresRMSE)\n",
    "        csvfile.writerow(tempScoresRHS)\n",
    "        csvfile.writerow(tempTime)\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
