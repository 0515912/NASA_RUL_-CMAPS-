{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the DataHandlers. Test the CMAPSS DataHandler\n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from data_handler_MNIST import MNISTDataHandler\n",
    "from tunable_model import SequenceTunableModelRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D, LSTM\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "import CMAPSAuxFunctions\n",
    "from CMAPSAuxFunctions import TrainValTensorBoard\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define different types of Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_lambda_regularization = 0\n",
    "l1_lambda_regularization = 0\n",
    "def RULmodel_SN_5(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_shape=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(20, input_shape=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "#     model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "#                     kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "#                     name='fc3'))\n",
    "    model.add(Dense(10, activation='softmax',kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tunable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(shape, model_type='ann'):\n",
    "    \n",
    "    K.clear_session()  #Clear the previous tensorflow graph\n",
    "    \n",
    "    #To test the model without randomness\n",
    "\n",
    "    \n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0,beta_1=0.5)\n",
    "    \n",
    "    lossFunction = \"categorical_crossentropy\"\n",
    "    metrics = [\"accuracy\"]\n",
    "    model = None\n",
    "\n",
    "    #Create and compile the models\n",
    "    model = RULmodel_SN_5(shape)\n",
    "    model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_features = len(selected_features)\n",
    "\n",
    "shape = (784,)\n",
    "dHandler_mnist = MNISTDataHandler()\n",
    "model = get_compiled_model(shape, model_type='ann')\n",
    "tModel = SequenceTunableModelRegression('ModelMNIST_SN_1', model, lib_type='keras', data_handler=dHandler_mnist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data. Cros-Validation ratio 0.8\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 45s 0us/step\n",
      "Printing shapes\n",
      "\n",
      "Training data (X, y)\n",
      "(9999, 3072)\n",
      "(9999, 10)\n",
      "Cross-Validation data (X, y)\n",
      "(40001, 3072)\n",
      "(40001, 10)\n",
      "Testing data (X, y)\n",
      "(10000, 3072)\n",
      "(10000, 10)\n",
      "Printing first 5 elements\n",
      "\n",
      "Training data (X, y)\n",
      "[[0.01568628 0.02745098 0.05490196 ... 0.18431373 0.2        0.16470589]\n",
      " [0.67058825 0.5882353  0.654902   ... 0.75686276 0.6745098  0.7529412 ]\n",
      " [0.45882353 0.5058824  0.5137255  ... 0.4        0.37254903 0.3882353 ]\n",
      " [0.01568628 0.01960784 0.01960784 ... 0.08235294 0.17254902 0.34509805]\n",
      " [0.47843137 0.5568628  0.78431374 ... 0.78431374 0.7294118  0.62352943]]\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "Cross-Validation data (X, y)\n",
      "[[0.10980392 0.10980392 0.10980392 ... 0.2        0.2        0.2       ]\n",
      " [0.18431373 0.19215687 0.1882353  ... 0.49019608 0.41960785 0.31764707]\n",
      " [0.6745098  0.7372549  0.827451   ... 0.6        0.49019608 0.22352941]\n",
      " [0.85882354 0.93333334 0.9254902  ... 1.         0.99607843 0.8784314 ]\n",
      " [0.4509804  0.28627452 0.27058825 ... 0.6392157  0.5568628  0.40392157]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "Testing data (X, y)\n",
      "[[0.61960787 0.4392157  0.19215687 ... 0.08235294 0.2627451  0.43137255]\n",
      " [0.92156863 0.92156863 0.92156863 ... 0.7294118  0.78431374 0.78039217]\n",
      " [0.61960787 0.74509805 0.87058824 ... 0.02745098 0.03137255 0.02745098]\n",
      " [0.60784316 0.6117647  0.58431375 ... 0.28627452 0.26666668 0.19607843]\n",
      " [0.25490198 0.26666668 0.19607843 ... 0.5019608  0.6117647  0.45882353]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/controlslab/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2069: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Load Non sequenced data (unroll sequence into a single feature vector)\n",
    "\n",
    "tModel.load_data(verbose=1, cross_validation_ratio=0.8)\n",
    "#tModel.data_handler.print_data()\n",
    "tModel.print_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model and test some Tunable Model functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with cv\n",
      "Train on 9999 samples, validate on 40001 samples\n",
      "Epoch 1/20\n",
      "9999/9999 [==============================] - 1s 108us/step - loss: 2.2487 - acc: 0.1559 - val_loss: 2.1617 - val_acc: 0.2111\n",
      "Epoch 2/20\n",
      "9999/9999 [==============================] - 0s 45us/step - loss: 2.1295 - acc: 0.2117 - val_loss: 2.0923 - val_acc: 0.2321\n",
      "Epoch 3/20\n",
      "9999/9999 [==============================] - 0s 47us/step - loss: 2.0752 - acc: 0.2329 - val_loss: 2.0360 - val_acc: 0.2529\n",
      "Epoch 4/20\n",
      "9999/9999 [==============================] - 0s 44us/step - loss: 2.0345 - acc: 0.2465 - val_loss: 2.0043 - val_acc: 0.2641\n",
      "Epoch 5/20\n",
      "9999/9999 [==============================] - 0s 49us/step - loss: 1.9856 - acc: 0.2728 - val_loss: 1.9917 - val_acc: 0.2676\n",
      "Epoch 6/20\n",
      "9999/9999 [==============================] - 0s 48us/step - loss: 1.9622 - acc: 0.2800 - val_loss: 1.9442 - val_acc: 0.2922\n",
      "Epoch 7/20\n",
      "9999/9999 [==============================] - 0s 45us/step - loss: 1.9438 - acc: 0.2936 - val_loss: 1.9364 - val_acc: 0.2893\n",
      "Epoch 8/20\n",
      "9999/9999 [==============================] - 0s 46us/step - loss: 1.9309 - acc: 0.2935 - val_loss: 1.9574 - val_acc: 0.2831\n",
      "Epoch 9/20\n",
      "9999/9999 [==============================] - 0s 46us/step - loss: 1.9198 - acc: 0.2950 - val_loss: 1.9257 - val_acc: 0.2994\n",
      "Epoch 10/20\n",
      "9999/9999 [==============================] - 0s 47us/step - loss: 1.9071 - acc: 0.3003 - val_loss: 1.9461 - val_acc: 0.2838\n",
      "Epoch 11/20\n",
      "9999/9999 [==============================] - 0s 48us/step - loss: 1.9172 - acc: 0.2995 - val_loss: 1.9353 - val_acc: 0.2877\n",
      "Epoch 12/20\n",
      "9999/9999 [==============================] - 0s 48us/step - loss: 1.8934 - acc: 0.3095 - val_loss: 1.9251 - val_acc: 0.3005\n",
      "Epoch 13/20\n",
      "9999/9999 [==============================] - 0s 48us/step - loss: 1.8858 - acc: 0.3142 - val_loss: 1.9708 - val_acc: 0.2680\n",
      "Epoch 14/20\n",
      "9999/9999 [==============================] - 0s 44us/step - loss: 1.8785 - acc: 0.3155 - val_loss: 1.9147 - val_acc: 0.2995\n",
      "Epoch 15/20\n",
      "9999/9999 [==============================] - 0s 43us/step - loss: 1.8689 - acc: 0.3163 - val_loss: 1.8873 - val_acc: 0.3088\n",
      "Epoch 16/20\n",
      "9999/9999 [==============================] - 0s 43us/step - loss: 1.8694 - acc: 0.3197 - val_loss: 1.8854 - val_acc: 0.3192\n",
      "Epoch 17/20\n",
      "9999/9999 [==============================] - 0s 48us/step - loss: 1.8593 - acc: 0.3243 - val_loss: 1.9006 - val_acc: 0.2973\n",
      "Epoch 18/20\n",
      "9999/9999 [==============================] - 0s 47us/step - loss: 1.8608 - acc: 0.3176 - val_loss: 1.8894 - val_acc: 0.3085\n",
      "Epoch 19/20\n",
      "9999/9999 [==============================] - 0s 42us/step - loss: 1.8549 - acc: 0.3254 - val_loss: 1.9436 - val_acc: 0.2881\n",
      "Epoch 20/20\n",
      "9999/9999 [==============================] - 0s 42us/step - loss: 1.8496 - acc: 0.3257 - val_loss: 1.8860 - val_acc: 0.3142\n"
     ]
    }
   ],
   "source": [
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "\n",
    "shape = (3072,)\n",
    "\n",
    "\n",
    "model = get_compiled_model(shape, model_type='ann')\n",
    "tModel.change_model('ModelRUL_SN_1', model, 'keras')\n",
    "\n",
    "# now = datetime.now()\n",
    "# tensorboard = TrainValTensorBoard(log_dir=\"./logs/\"+now.strftime(\"%Y%m%d-%H%M%S\")+ \"/\",write_graph=False)\n",
    "\n",
    "tModel.epochs = 20\n",
    "tModel.train_model(learningRate_scheduler=lrate, verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40001/40001 [==============================] - 1s 17us/step\n",
      "{'loss': 1.88600313084438, 'score_1': 0.31419214519637007}\n"
     ]
    }
   ],
   "source": [
    "tModel.evaluate_model(cross_validation=True)\n",
    "\n",
    "cScores = tModel.scores\n",
    "print(cScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 18us/step\n",
      "{'loss': 1.8851857538223267, 'score_1': 0.3154}\n"
     ]
    }
   ],
   "source": [
    "tModel.evaluate_model(cross_validation=False)\n",
    "\n",
    "cScores = tModel.scores\n",
    "print(cScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
