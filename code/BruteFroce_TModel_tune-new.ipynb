{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Get best parameters for each dataset using brute force search. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import custom_scores\n",
    "from data_handler_CMAPS import CMAPSDataHandler\n",
    "from tunable_model import SequenceTunableModelRegression\n",
    "import CMAPSAuxFunctions\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "from scipy.optimize import differential_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.clear_session()  #Clear the previous tensorflow graph\n",
    "\n",
    "l2_lambda_regularization = 0.20\n",
    "l1_lambda_regularization = 0.20\n",
    "\n",
    "def RULmodel_SN_5(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a new model\n",
    "\n",
    "Function to get new model with new optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(model_def, shape, model_type='ann'):\n",
    "\n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0, beta_1=0.5)\n",
    "    lossFunction = \"mean_squared_error\"\n",
    "    metrics = [\"mse\"]\n",
    "    model = None\n",
    "\n",
    "    #Create and compile the models\n",
    "\n",
    "    if model_type=='ann':\n",
    "        model = model_def(shape)\n",
    "        model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return model\n",
    "\n",
    "models = {'shallow-20':RULmodel_SN_5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform exhaustive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for dataset 2\n",
      "Testing for w:15, s:1, r:90\n",
      "Loading data for dataset 2 with window_size of 15, stride of 1 and maxRUL of 90. Cros-Validation ratio 0\n",
      "Loading data from file and computing dataframes\n",
      "259/259 [==============================] - 0s 112us/step\n",
      "Testing for w:15, s:2, r:90\n",
      "Loading data for dataset 2 with window_size of 15, stride of 2 and maxRUL of 90. Cros-Validation ratio 0\n",
      "Loading data from memory without recomputing df\n"
     ]
    }
   ],
   "source": [
    "#Perform exhaustive search to find the optimal parameters\n",
    "\n",
    "#Selected as per CNN paper\n",
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../CMAPSSData'\n",
    "num_features = len(selected_features)\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 125\n",
    "shape = num_features*window_size\n",
    "\n",
    "#maxWindowSize = {'1':30, '2':20, '3':30, '4':18}\n",
    "max_window_size = {'1':30, '2':20} #Do it only for datasets 1 and 2\n",
    "total_time = {'1':0, '2':0, '3':0, '4':0}\n",
    "results = {'1':0, '2':0, '3':0, '4':0}\n",
    "\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "\n",
    "#Create necessary objects\n",
    "dHandler_cmaps = CMAPSDataHandler(data_folder, 1, selected_features, max_rul, window_size, window_stride)\n",
    "\n",
    "model = get_compiled_model(models['shallow-20'], shape, model_type='ann')\n",
    "tunable_model = SequenceTunableModelRegression('ModelRUL_SN_5', model, lib_type='keras', data_handler=dHandler_cmaps,\n",
    "                                              epochs=20)\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "tunable_model.data_scaler = min_max_scaler\n",
    "\n",
    "count = 0\n",
    "\n",
    "for dataset_number in max_window_size:\n",
    "    \n",
    "    print(\"Results for dataset \"+dataset_number)\n",
    "    file = open(\"results/MLP/exhauxtive_search_\"+dataset_number+\".csv\", \"w\")\n",
    "    start_time = time.clock()\n",
    "    \n",
    "    tunable_model.data_handler.change_dataset(dataset_number)\n",
    "    \n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    for r in range(90, 141):   #Load max_rul first as it forces reloading the dataset from file\n",
    "        \n",
    "        tunable_model.data_handler.max_rul = r\n",
    "        verbose = 1\n",
    "        \n",
    "        for w in range(15, max_window_size[dataset_number]+1):\n",
    "        \n",
    "            for s in range(1,11):\n",
    "                \n",
    "                print(\"Testing for w:{}, s:{}, r:{}\".format(w, s, r))\n",
    "                \n",
    "                #Set data parameters\n",
    "                tunable_model.data_handler.sequence_length = w\n",
    "                tunable_model.data_handler.sequence_stride = s\n",
    "\n",
    "                #Create and compile the models\n",
    "                shape = num_features*w\n",
    "                model = get_compiled_model(models['shallow-20'], shape, model_type='ann')\n",
    "\n",
    "                #Add model to tunable model\n",
    "                tunable_model.change_model('ModelRUL_SN', model, 'keras')\n",
    "                                \n",
    "                #Load the data\n",
    "                tunable_model.load_data(unroll=True, verbose=verbose, cross_validation_ratio=0)\n",
    "                \n",
    "                if s > 1:\n",
    "                    verbose = 0\n",
    "                \n",
    "                #Train and evaluate\n",
    "                tunable_model.train_model(learningRate_scheduler=lrate, verbose=0)\n",
    "                tunable_model.evaluate_model(['rhs', 'rmse'], round=2)\n",
    "\n",
    "\n",
    "                cScores = tunable_model.scores\n",
    "                rmse = math.sqrt(cScores['score_1'])\n",
    "                rmse2 = cScores['rmse']\n",
    "                rhs = cScores['rhs']\n",
    "                time = tunable_model.train_time\n",
    "                \n",
    "                row = [w, s, r, rmse, rhs]\n",
    "                writer.writerow(row)\n",
    "                \n",
    "                \n",
    "                #msgStr = \"The model variables are \" + str(x) + \"\\tThe scores are: [RMSE:{:.4f}, RHS:{:.4f}]\\n\".format(rmse, rhs)\n",
    "                #file.write(msgStr)\n",
    "                \n",
    "                \n",
    "    end_time = time.clock()\n",
    "    file.close()\n",
    "    totalTime[dataset_number] = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total time {}\".format(totalTime))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
