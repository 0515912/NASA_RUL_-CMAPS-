{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Get best parameters for each dataset using brute force search. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import custom_scores\n",
    "import CMAPSAuxFunctions\n",
    "from tunableModel import TunableModel\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "from scipy.optimize import differential_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RULmodel_SN(input_shape):\n",
    "    \n",
    "    lambda_regularization = 0.20\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(lambda_regularization), name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define objective function\n",
    "\n",
    "Define the function that evaluates each set of data-related params and returns the RMSE as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_optmize_fun(x, selected_features=['T24', 'T30', 'T50', 'P30', 'Nf', 'Nc', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'htBleed', 'W31', 'W32'], datasetNumber = '1', scaler = None, verbose=0, epochs=250, \n",
    "                  saveToFile = None, iterations = 0):\n",
    "    \n",
    "    #Clear the previous tensorflow graph\n",
    "    K.clear_session()\n",
    "    \n",
    "    maxWindowSize = {'1':30, '2':20, '3':30, '4':18}\n",
    "    \n",
    "    #Extract the tunning variables from the input vector\n",
    "    #Round the values to the nearest integer since this implementation is for real numbers\n",
    "    x = x.astype(int)\n",
    "    windows_size = x[0]\n",
    "    #windows_size = maxWindowSize[datasetNumber]\n",
    "    window_stride = x[1]\n",
    "    constantRUL = x[2]\n",
    "    \n",
    "    if iterations == 0:\n",
    "        print(\"Creating model\")\n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0, beta_1=0.5)\n",
    "    lossFunction = \"mean_squared_error\"\n",
    "    metrics = [\"mse\"]\n",
    "    \n",
    "    #Define the model\n",
    "    nFeatures = len(selected_features)\n",
    "    shapeSN = nFeatures*windows_size\n",
    "    modelRULSN = RULmodel_SN(shapeSN)\n",
    "    modelRULSN.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "\n",
    "    #load the data using the selected parameters\n",
    "    tModel = TunableModel(\"RUL_SN_optmizable\", modelRULSN, selected_features, '../CMAPSSData', 'keras',\n",
    "                          scaler = min_max_scaler, window_stride = window_stride, \n",
    "                          window_size = windows_size, constRul = constantRUL, datasetNumber = datasetNumber, \n",
    "                         epochs=epochs)\n",
    "    \n",
    "    tModel.loadData(rectify_labels = False)\n",
    "    \n",
    "    if iterations == 0:\n",
    "        print(\"Training model\")\n",
    "    #Train the model\n",
    "    lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "    tModel.trainModel(learningRateScheduler=lrate, verbose=verbose)\n",
    "    time = tModel.trainTime\n",
    "    \n",
    "    if iterations == 0:\n",
    "        print(\"Training time {}\".format(time))\n",
    "    \n",
    "    if iterations == 0:\n",
    "        print(\"Assesing model performance\")\n",
    "    #Assess the model performance\n",
    "    tModel.evaluateModel([\"rhs\"], round=2)\n",
    "    cScores = tModel.scores\n",
    "    rmse = math.sqrt(cScores['score_1'])\n",
    "    rhs = cScores['rhs']\n",
    "    #print(\"The score for this model is: {}\".format(rmse))\n",
    "    \n",
    "    msgStr = \"The model variables are \" + str(x) + \"\\tThe scores are: [RMSE:{:.4f}, RHS:{:.4f}]\\n\".format(rmse, rhs)\n",
    "    \n",
    "    if saveToFile is not None:\n",
    "        #print(msgStr)\n",
    "        saveToFile.write(msgStr)\n",
    "    else:\n",
    "        print(msgStr)\n",
    "    \n",
    "    #Return RMSE as the performance metric to steer the search\n",
    "    return rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the parameters for the NN using DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunning for dataset 1\n",
      "Creating model\n",
      "WARNING:tensorflow:From C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Training model\n",
      "Training time 2.6263802038634907\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.446787667545692\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 169us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.6850021747780346\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.7104741406213435\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 209us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5020477534445433\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5166190100956527\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 3.0950609620515905\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 219us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.7887239709301497\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5980954113158674\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 369us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.434126899761253\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 189us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.480747606768915\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 3.140796397865415\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 169us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.71627606807931\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 160us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.506851526050525\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 169us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.4907742599903315\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.8346186141495195\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 159us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 3.0543228833460887\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 189us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.8869044716557255\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.4355617168848767\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.423824498338206\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 3.0761179626913417\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 3.076940818730776\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 250us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.7646361098974808\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 309us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.4937099665298774\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 181us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.4606733327351833\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 3.3746676880646476\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.6589560382760737\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 240us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5743242584299253\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 269us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.4729925238347903\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5462293932466764\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 159us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.8597794794576146\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 199us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5292449131649164\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5360276850219066\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5041286649398558\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.758674975033955\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 159us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5842541194002138\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 160us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.457172964095662\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 181us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.4987782721144924\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.6121010306302423\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 169us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.804094434128615\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 160us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.7450148333515187\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 199us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5182976364161505\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.485413870987884\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5515337061338528\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 159us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 3.06780601955154\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 160us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.5049303095791515\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 190us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.441204436937312\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 169us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.437535839951977\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 169us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.9354980826844894\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 200us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.6752744316319763\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 279us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 2.478064974175709\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 160us/step\n",
      "Creating model\n",
      "Training model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 1.3379968343813857\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 1.3803557566277505\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 170us/step\n",
      "Creating model\n",
      "Training model\n",
      "Training time 1.4139590029959095\n",
      "Assesing model performance\n",
      "100/100 [==============================] - 0s 169us/step\n",
      "Creating model\n",
      "Training model\n"
     ]
    }
   ],
   "source": [
    "#Optimize the parameters for the NN using DE\n",
    "\n",
    "#maxWindowSize = {'1':30, '2':20, '3':30, '4':18}\n",
    "maxWindowSize = {'1':30, '2':20} #Do it only for datasets 1 and 2\n",
    "totalTime = {'1':0, '2':0, '3':0, '4':0}\n",
    "results = {'1':0, '2':0, '3':0, '4':0}\n",
    "\n",
    "#datasetNumber = '1'\n",
    "\n",
    "selected_features = ['T24', 'T30', 'T50', 'P30', 'Nf', 'Nc', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'htBleed', 'W31', 'W32']\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# for datasetNumber in maxWindowSize:\n",
    "    \n",
    "#    print(\"Tunning for dataset \"+datasetNumber)\n",
    "#     file = open(\"intermediateResults6_notRectified\"+datasetNumber+\".txt\", \"w\")\n",
    "\n",
    "#     windowSizeBounds = [1,maxWindowSize[datasetNumber]]\n",
    "#     windowStrideBounds = [1,10]\n",
    "#     constantRULBounds = [90,140]\n",
    "\n",
    "#     #bounds = [windowSizeBounds, windowStrideBounds, constantRULBounds]\n",
    "#     bounds = [windowStrideBounds, constantRULBounds] #Optimize only 2 variabes\n",
    "\n",
    "\n",
    "#     startTime = time.clock()\n",
    "#     tempResults = differential_evolution(nn_optmize_fun, bounds, \n",
    "#                                      args=(selected_features, datasetNumber, min_max_scaler, 0, 20, file, 1),\n",
    "#                                     strategy='best1bin', maxiter=20, popsize=30, disp=True)\n",
    "#     results[datasetNumber] = tempResults\n",
    "#     endTime = time.clock()\n",
    "\n",
    "#     file.close()\n",
    "#     totalTime[datasetNumber] = endTime - startTime\n",
    "\n",
    "for datasetNumber in maxWindowSize:\n",
    "    \n",
    "    print(\"Tunning for dataset \"+datasetNumber)\n",
    "    file = open(\"intermediateResultsbruteforce_notRectified\"+datasetNumber+\".txt\", \"w\")\n",
    "    startTime = time.clock()\n",
    "    for w in range(15,maxWindowSize[datasetNumber]+1):\n",
    "        \n",
    "        for s in range(1,11):\n",
    "            for r in range(90,141):\n",
    "                \n",
    "                x = np.array([w,s,r])\n",
    "                \n",
    "                results = nn_optmize_fun(x,selected_features=selected_features,datasetNumber=datasetNumber, \n",
    "                                         scaler = min_max_scaler, verbose=0, epochs=20, \n",
    "                                        saveToFile = file, iterations = 0)\n",
    "    endTime = time.clock()\n",
    "    file.close()\n",
    "    totalTime[datasetNumber] = endTime - startTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time {'4': 0, '1': 18118.907471727885, '2': 15901.300738888152, '3': 0}\n",
      "59.27322450422804\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time {}\".format(totalTime))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
