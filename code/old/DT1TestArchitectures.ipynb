{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Test different MLP architectures. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import custom_scores\n",
    "import CMAPSAuxFunctions\n",
    "from tunableModel import TunableModel\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define architectures\n",
    "\n",
    "Define each one of the different architectures to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clear the previous tensorflow graph\n",
    "K.clear_session()\n",
    "lambda_regularization = 0.20\n",
    "\n",
    "def RULmodel_SN_5(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(lambda_regularization), name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Shared parameters for the models\n",
    "optimizer = Adam(lr=0, beta_1=0.5)\n",
    "lossFunction = \"mean_squared_error\"\n",
    "metrics = [\"mse\"]\n",
    "\n",
    "#Selected as per CNN paper\n",
    "selected_features = ['T24', 'T30', 'T50', 'P30', 'Nf', 'Nc', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'htBleed', 'W31', 'W32']\n",
    "\n",
    "#Selected from the results of running DE on the old model (250->50->1)\n",
    "windowSize = 30\n",
    "stride = 128\n",
    "constRUL = 1\n",
    "\n",
    "dataFolder = '../CMAPSSData'\n",
    "\n",
    "#Create and compile the models\n",
    "nFeatures = len(selected_features)\n",
    "shapeSN = nFeatures*windowSize\n",
    "modelRULSN = RULmodel_SN_5(shapeSN)\n",
    "modelRULSN.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "tModel = TunableModel('ModelRUL_SN_1', modelRULSN, selected_features, dataFolder, 'keras', window_size=windowSize,\n",
    "                      scaler = min_max_scaler, window_stride=stride, constRul = constRUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model 5\n",
      "Computing for dataset 1\n",
      "Loading data for dataset 1 with window_size of 30, stride of 1 and constRUL of 128. Cros-Validation ratio 0\n",
      "Data loaded for dataset 1\n",
      "Iteration 1\n",
      "100/100 [==============================] - 0s 691us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Iteration 2\n",
      "100/100 [==============================] - 0s 44us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Iteration 3\n",
      "100/100 [==============================] - 0s 49us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Iteration 4\n",
      "100/100 [==============================] - 0s 106us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Iteration 5\n",
      "100/100 [==============================] - 0s 41us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Iteration 6\n",
      "100/100 [==============================] - 0s 48us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Iteration 7\n",
      "100/100 [==============================] - 0s 44us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Iteration 8\n",
      "100/100 [==============================] - 0s 178us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Iteration 9\n",
      "100/100 [==============================] - 0s 83us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Iteration 10\n",
      "100/100 [==============================] - 0s 41us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "DescribeResult(nobs=10, minmax=(15.191115824718079, 15.658224675869228), mean=15.422657346662783, variance=0.02848929714276179, skewness=-0.18813671402891666, kurtosis=-1.5241585902433699)\n",
      "DescribeResult(nobs=10, minmax=(3.6883274853160355, 4.518883828699219), mean=4.1567702564760225, variance=0.058258477063467616, skewness=-0.3616168731317834, kurtosis=-0.3434665193519857)\n",
      "DescribeResult(nobs=10, minmax=(25.167299999999955, 27.228219999999965), mean=26.248165399999998, variance=0.46839727383071467, skewness=-0.4752875355302355, kurtosis=-0.8918251063689491)\n",
      "RMSE2: 15.658224675869228\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "tModel.epochs = 100\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "\n",
    "windowSize = 30\n",
    "windowStride = 1\n",
    "constRul = 128\n",
    "nFeatures = len(selected_features)\n",
    "shapeSN = nFeatures*windowSize\n",
    "\n",
    "#models = {1:RULmodel_SN_1,2:RULmodel_SN_2,3:RULmodel_SN_3,4:RULmodel_SN_4, 5:RULmodel_SN_5, 6:RULmodel_SN_6}\n",
    "models = {5:RULmodel_SN_5}\n",
    "\n",
    "\n",
    "for key, model in models.items():\n",
    "    \n",
    "    print(\"For model \"+str(key))\n",
    "    #file.write(\"For model \"+str(key)+'\\n\\n')\n",
    "    \n",
    "    for i in range(1,2):\n",
    "\n",
    "        dataset = i\n",
    "        print(\"Computing for dataset \"+str(i))\n",
    "        #file.write(\"Computing for dataset \"+str(i)+'\\n\\n')\n",
    "        \n",
    "        #Create and compile the models\n",
    "        modelRULSN = model(shapeSN)\n",
    "        modelRULSN.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "\n",
    "        tModel.changeModel('ModelRUL_SN_Dat'+str(shapeSN), modelRULSN, 'keras')\n",
    "        tModel.windowSize = windowSize\n",
    "        tModel.windowStride = windowStride\n",
    "        tModel.constRul = constRul\n",
    "        tModel.changeDataset(dataset)\n",
    "        tModel.loadData(verbose=1, rectify_labels = False)\n",
    "        tempScoresRMSE = np.zeros((iterations,1))\n",
    "        tempScoresRHS = np.zeros((iterations,1))\n",
    "        tempTime = np.zeros((iterations,1))\n",
    "\n",
    "        for j in range(iterations):\n",
    "\n",
    "            print(\"Iteration \"+str(j+1))\n",
    "            #file.write(\"Iteration \"+str(j+1)+'\\n\\n')\n",
    "            tModel.trainModel(learningRateScheduler=lrate, verbose=0)\n",
    "            #tModel.evaluateModel([\"rhs\"], round=2, setLimits=[7,135])\n",
    "            tModel.evaluateModel([\"rhs\"], round=2, setLimits=[])\n",
    "\n",
    "            cScores = tModel.scores\n",
    "            rmse = math.sqrt(cScores['score_1'])\n",
    "            rmse2 = cScores['rmse']\n",
    "            rhs = cScores['rhs']\n",
    "            time = tModel.trainTime\n",
    "\n",
    "            tempScoresRMSE[j] = rmse2\n",
    "            tempScoresRHS[j] = rhs\n",
    "            tempTime[j] = time\n",
    "\n",
    "        #print(tempScoresRMSE)\n",
    "        tempScoresRMSE = np.reshape(tempScoresRMSE, (iterations,))\n",
    "        tempScoresRHS = np.reshape(tempScoresRHS, (iterations,))\n",
    "        tempTime = np.reshape(tempTime, (iterations,))\n",
    "     \n",
    "        \n",
    "        print(stats.describe(tempScoresRMSE))\n",
    "        print(stats.describe(tempScoresRHS))\n",
    "        print(stats.describe(tempTime))\n",
    "        \n",
    "print(\"RMSE2: {}\".format(rmse2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on all datasets\n",
    "\n",
    "Test the choosen model on the four datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model 1\n",
      "Computing for dataset 1\n",
      "Loading data for dataset 1 with window_size of 18, stride of 2 and constRUL of 128. Cros-Validation ratio 0\n",
      "Data loaded for dataset 1\n",
      "Iteration 1\n",
      "100/100 [==============================] - 0s 1ms/step\n",
      "Iteration 2\n",
      "100/100 [==============================] - 0s 61us/step\n",
      "Iteration 3\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 4\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 5\n",
      "100/100 [==============================] - 0s 50us/step\n",
      "Iteration 6\n",
      "100/100 [==============================] - 0s 30us/step\n",
      "Iteration 7\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 8\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 9\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 10\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "DescribeResult(nobs=10, minmax=(17.588916964952674, 18.56932955170972), mean=17.820665657547316, variance=0.08430613545967351, skewness=1.8895598132413094, kurtosis=2.574885483344616)\n",
      "DescribeResult(nobs=10, minmax=(8.269199819471044, 9.182618812385165), mean=8.594825733134403, variance=0.08224442291852586, skewness=0.738241147315495, kurtosis=-0.3385233015652629)\n",
      "DescribeResult(nobs=10, minmax=(11.914297897779761, 12.600884100909752), mean=12.143739296776221, variance=0.05197453727747149, skewness=0.7934582140189548, kurtosis=-0.5296369026854668)\n",
      "Computing for dataset 2\n",
      "Loading data for dataset 2 with window_size of 18, stride of 2 and constRUL of 128. Cros-Validation ratio 0\n",
      "Data loaded for dataset 2\n",
      "Iteration 1\n",
      "259/259 [==============================] - 0s 589us/step\n",
      "Iteration 2\n",
      "259/259 [==============================] - 0s 31us/step\n",
      "Iteration 3\n",
      "259/259 [==============================] - 0s 35us/step\n",
      "Iteration 4\n",
      "259/259 [==============================] - 0s 35us/step\n",
      "Iteration 5\n",
      "259/259 [==============================] - 0s 31us/step\n",
      "Iteration 6\n",
      "259/259 [==============================] - 0s 39us/step\n",
      "Iteration 7\n",
      "259/259 [==============================] - 0s 35us/step\n",
      "Iteration 8\n",
      "259/259 [==============================] - 0s 31us/step\n",
      "Iteration 9\n",
      "259/259 [==============================] - 0s 35us/step\n",
      "Iteration 10\n",
      "259/259 [==============================] - 0s 42us/step\n",
      "DescribeResult(nobs=10, minmax=(29.48548494418605, 31.929458928869238), mean=30.154799828653182, variance=0.5667277954062723, skewness=1.4510652102455763, kurtosis=1.0871488183039695)\n",
      "DescribeResult(nobs=10, minmax=(65.83311470970234, 104.37802385485215), mean=75.73817260695344, variance=133.28459737263168, skewness=1.6613545626642605, kurtosis=1.8768461253411024)\n",
      "DescribeResult(nobs=10, minmax=(30.42952865830739, 31.466484037324335), mean=30.927445259959132, variance=0.09816591660820978, skewness=0.03566818944788664, kurtosis=-0.6827894045316802)\n",
      "Computing for dataset 3\n",
      "Loading data for dataset 3 with window_size of 18, stride of 2 and constRUL of 128. Cros-Validation ratio 0\n",
      "Data loaded for dataset 3\n",
      "Iteration 1\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "Iteration 2\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 3\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 4\n",
      "100/100 [==============================] - 0s 50us/step\n",
      "Iteration 5\n",
      "100/100 [==============================] - 0s 50us/step\n",
      "Iteration 6\n",
      "100/100 [==============================] - 0s 50us/step\n",
      "Iteration 7\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 8\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 9\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "Iteration 10\n",
      "100/100 [==============================] - 0s 40us/step\n",
      "DescribeResult(nobs=10, minmax=(17.35857136978732, 19.998749960935058), mean=18.2078921176959, variance=0.5207384781638988, skewness=1.5785248984630063, kurtosis=1.9782159840984788)\n",
      "DescribeResult(nobs=10, minmax=(6.258241232910122, 16.09533966296191), mean=8.271666169852045, variance=8.19392357638636, skewness=2.3095836388602335, kurtosis=3.941299587080964)\n",
      "DescribeResult(nobs=10, minmax=(14.303653627101085, 14.917944551936671), mean=14.595112405792133, variance=0.04395215549351953, skewness=0.38214866206152237, kurtosis=-1.0312759369635365)\n",
      "Computing for dataset 4\n",
      "Loading data for dataset 4 with window_size of 18, stride of 2 and constRUL of 128. Cros-Validation ratio 0\n",
      "Data loaded for dataset 4\n",
      "Iteration 1\n",
      "248/248 [==============================] - 0s 656us/step\n",
      "Iteration 2\n",
      "248/248 [==============================] - 0s 32us/step\n",
      "Iteration 3\n",
      "248/248 [==============================] - 0s 36us/step\n",
      "Iteration 4\n",
      "248/248 [==============================] - 0s 32us/step\n",
      "Iteration 5\n",
      "248/248 [==============================] - 0s 32us/step\n",
      "Iteration 6\n",
      "248/248 [==============================] - 0s 32us/step\n",
      "Iteration 7\n",
      "248/248 [==============================] - 0s 41us/step\n",
      "Iteration 8\n",
      "248/248 [==============================] - 0s 28us/step\n",
      "Iteration 9\n",
      "248/248 [==============================] - 0s 28us/step\n",
      "Iteration 10\n",
      "248/248 [==============================] - 0s 28us/step\n",
      "DescribeResult(nobs=10, minmax=(32.919746010857295, 37.28870580544679), mean=34.4598444171115, variance=2.000638179302849, skewness=0.9027562897743275, kurtosis=-0.3671049560974664)\n",
      "DescribeResult(nobs=10, minmax=(48.903365107237676, 77.93109486825057), mean=59.60667217938635, variance=93.29184599298827, skewness=0.8093266320427187, kurtosis=-0.6507189007289558)\n",
      "DescribeResult(nobs=10, minmax=(34.989408197059674, 36.44191099598902), mean=35.36688802916306, variance=0.20774653419353065, skewness=1.461193822865988, kurtosis=1.057714070862139)\n"
     ]
    }
   ],
   "source": [
    "datasets = [1,2,3,4]\n",
    "iterations = 10\n",
    "tModel.epochs = 200\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "scores ={1:[], 2:[], 3:[], 4:[]}\n",
    "windowSizes = {1:18,2:18,3:18,4:18}\n",
    "strides = {1:2,2:2,3:2,4:2}\n",
    "constRUL = {1:128, 2:128, 3:128, 4:128}\n",
    "#models = {1:RULmodel_SN_1,2:RULmodel_SN_2,3:RULmodel_SN_3,4:RULmodel_SN_4, 5:RULmodel_SN_5, 6:RULmodel_SN_6}\n",
    "models = {1:RULmodel_SN_5}\n",
    "\n",
    "file = open(\"ResultsDatasets_NonRectified_NoLimits_smallerMLP2_2.csv\", \"w\")\n",
    "csvfile = csv.writer(file, lineterminator='\\n')\n",
    "\n",
    "for key, model in models.items():\n",
    "    \n",
    "    print(\"For model \"+str(key))\n",
    "    #file.write(\"For model \"+str(key)+'\\n\\n')\n",
    "    \n",
    "    for i in range(1,5):\n",
    "\n",
    "        dataset = i\n",
    "        print(\"Computing for dataset \"+str(i))\n",
    "        #file.write(\"Computing for dataset \"+str(i)+'\\n\\n')\n",
    "\n",
    "        #Create and compile the models\n",
    "        windowSize = windowSizes[i]\n",
    "        nFeatures = len(selected_features)\n",
    "        shapeSN = nFeatures*windowSize\n",
    "        modelRULSN = model(shapeSN)\n",
    "        modelRULSN.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "\n",
    "        tModel.changeModel('ModelRUL_SN_Dat'+str(shapeSN), modelRULSN, 'keras')\n",
    "        tModel.windowSize = windowSize\n",
    "        tModel.windowStride = strides[i]\n",
    "        tModel.constRul = constRUL[i]\n",
    "        tModel.changeDataset(dataset)\n",
    "        tModel.loadData(verbose=1, rectify_labels = False)\n",
    "        tempScoresRMSE = np.zeros((iterations,1))\n",
    "        tempScoresRHS = np.zeros((iterations,1))\n",
    "        tempTime = np.zeros((iterations,1))\n",
    "\n",
    "        for j in range(iterations):\n",
    "\n",
    "            print(\"Iteration \"+str(j+1))\n",
    "            #file.write(\"Iteration \"+str(j+1)+'\\n\\n')\n",
    "            tModel.trainModel(learningRateScheduler=lrate, verbose=0)\n",
    "            #tModel.evaluateModel([\"rhs\"], round=2, setLimits=[7,135])\n",
    "            tModel.evaluateModel([\"rhs\"], round=2)\n",
    "\n",
    "            cScores = tModel.scores\n",
    "            rmse = math.sqrt(cScores['score_1'])\n",
    "            rmse2 = cScores['rmse']\n",
    "            rhs = cScores['rhs']\n",
    "            time = tModel.trainTime\n",
    "\n",
    "            tempScoresRMSE[j] = rmse2\n",
    "            tempScoresRHS[j] = rhs\n",
    "            tempTime[j] = time\n",
    "\n",
    "        #print(tempScoresRMSE)\n",
    "        tempScoresRMSE = np.reshape(tempScoresRMSE, (iterations,))\n",
    "        tempScoresRHS = np.reshape(tempScoresRHS, (iterations,))\n",
    "        tempTime = np.reshape(tempTime, (iterations,))\n",
    "        csvfile.writerow(tempScoresRMSE)\n",
    "        csvfile.writerow(tempScoresRHS)\n",
    "        csvfile.writerow(tempTime)\n",
    "        #file.write(str(stats.describe(tempScoresRMSE))+'\\n\\n')\n",
    "        #file.write(str(stats.describe(tempScoresRHS))+'\\n\\n')\n",
    "        #file.write(str(stats.describe(tempTime))+'\\n\\n')\n",
    "        print(stats.describe(tempScoresRMSE))\n",
    "        print(stats.describe(tempScoresRHS))\n",
    "        print(stats.describe(tempTime))\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Best Model\n",
    "\n",
    "Save the model and weights of the best model throughout 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "100/100 [==============================] - 0s 291us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tempScoresRMSE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1bce8eea1a0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mbestIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mtempScoresRMSE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrmse2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0mtempScoresRHS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrhs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mtempTime\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tempScoresRMSE' is not defined"
     ]
    }
   ],
   "source": [
    "#Clear the previous tensorflow graph\n",
    "K.clear_session()\n",
    "\n",
    "def RULmodel_SN_1(input_shape):\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(30, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', name='fc1'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(10, activation='relu', kernel_initializer='glorot_normal', name='fc2'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Shared parameters for the models\n",
    "optimizer = Adam(lr=0, beta_1=0.5)\n",
    "lossFunction = \"mean_squared_error\"\n",
    "metrics = [\"mse\"]\n",
    "\n",
    "#Selected as per CNN paper\n",
    "selected_features = ['T24', 'T30', 'T50', 'P30', 'Nf', 'Nc', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'htBleed', 'W31', 'W32']\n",
    "\n",
    "dataFolder = '../CMAPSSData'\n",
    "\n",
    "datasets = [1]\n",
    "iterations = 10\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "scores ={1:[], 2:[], 3:[], 4:[]}\n",
    "#windowSizes = {1:25,2:15,3:25,4:15}\n",
    "windowSizes = 15\n",
    "stride = 2\n",
    "#constRUL = {1:93, 2:94, 3:92, 4:94}\n",
    "constRUL = 95\n",
    "#models = {1:RULmodel_SN_1,2:RULmodel_SN_2,3:RULmodel_SN_3,4:RULmodel_SN_4}\n",
    "models = {1:RULmodel_SN_1}\n",
    "\n",
    "#Create and compile the model\n",
    "windowSize = 15\n",
    "nFeatures = len(selected_features)\n",
    "shapeSN = nFeatures*windowSize\n",
    "modelRULSN = RULmodel_SN_1(shapeSN)\n",
    "modelRULSN.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "tModel = TunableModel('ModelRUL_SN_1', modelRULSN, selected_features, dataFolder, window_size=windowSize,\n",
    "                      scaler = min_max_scaler, window_stride=stride, constRul = constRUL)\n",
    "tModel.loadData()\n",
    "tModel.epochs = 200\n",
    "\n",
    "#file = open(\"ResultsBestModelAllDatasetsSingleSet.csv\", \"w\")\n",
    "#csvfile = csv.writer(file, lineterminator='\\n')\n",
    "\n",
    "min_rmse = 1000\n",
    "bestModel = None\n",
    "bestIndex = 0\n",
    "\n",
    "for i in range(iterations):\n",
    "\n",
    "    print(\"Iteration \"+str(i+1))\n",
    "    #file.write(\"Iteration \"+str(j+1)+'\\n\\n')\n",
    "    tModel.trainModel(learningRateScheduler=lrate, verbose=0)\n",
    "    tModel.evaluateModel([\"rhs\"], round=True)\n",
    "\n",
    "    cScores = tModel.scores\n",
    "    rmse = math.sqrt(cScores['score_1'])\n",
    "    rmse2 = cScores['rmse']\n",
    "    rhs = cScores['rhs']\n",
    "    time = tModel.trainTime\n",
    "\n",
    "    if rmse2 < min_rmse:\n",
    "        bestModel = tModel.model\n",
    "        bestIndex = i\n",
    "    \n",
    "    tempScoresRMSE[i] = rmse2\n",
    "    tempScoresRHS[i] = rhs\n",
    "    tempTime[i] = time\n",
    "\n",
    "print(tempScoresRMSE)    \n",
    "    \n",
    "#save best model to file\n",
    "# serialize model to JSON\n",
    "model_json = bestModel.to_json()\n",
    "with open(\"bestRULModel.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "bestModel.save_weights(\"bestRULModel.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "        \n",
    "#file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
