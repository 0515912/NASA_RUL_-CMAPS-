{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Test different MLP architectures. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\controlslab\\Anaconda3\\envs\\gpu_tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import custom_scores\n",
    "from data_handler_CMAPS import CMAPSDataHandler\n",
    "from tunable_model import SequenceTunableModelRegression\n",
    "import CMAPSAuxFunctions\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define architectures\n",
    "\n",
    "Define each one of the different architectures to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-d4288f0c2d18>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-d4288f0c2d18>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    \"\"\"Define the RNN model\"\"\"\u001b[0m\n\u001b[1;37m                              \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()  #Clear the previous tensorflow graph\n",
    "\n",
    "l2_lambda_regularization = 0.20\n",
    "l1_lambda_regularization = 0.10\n",
    "\n",
    "\"\"\"\n",
    "def RULmodel_LSTM(input_shape):\n",
    "    Define the RNN model\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    #model.add(Masking(mask_value=0, imput))\n",
    "    #model.add(LSTM(input_shape=input_shape, units=100, return_sequences=True, name='lstm1')))\n",
    "    model.add(LSTM(input_shape=input_shape, units=20, return_sequences=False, dropout=0.2, name='lstm2'))\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda_regularization), name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def RULmodel_LSTM2(input_shape):\n",
    "    \"\"\"Define the RNN model\"\"\"\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    #model.add(Masking(mask_value=0, imput))\n",
    "    #model.add(LSTM(input_shape=input_shape, units=20, return_sequences=False, dropout=0.2, name='lstm1'))\n",
    "    model.add(CuDNNLSTM(input_shape=input_shape, units=20, return_sequences=False, name='lstm1'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def RULmodel_SN_4(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "    model.add(Dense(1, activation='linear', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RULmodel_SN_1(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(250, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RULmodel_SN_2(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(100, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RULmodel_SN_3(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "    model.add(Dense(1, activation='linear', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RULmodel_SN_5(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RULmodel_SN_6(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(model_def, shape, model_type='ann'):\n",
    "\n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0, beta_1=0.5)\n",
    "    lossFunction = \"mean_squared_error\"\n",
    "    metrics = [\"mse\"]\n",
    "    model = None\n",
    "\n",
    "    #Create and compile the models\n",
    "\n",
    "    if model_type=='ann':\n",
    "        model = model_def(shape)\n",
    "        model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    elif model_type=='lstm':\n",
    "        model = model_def(shape)\n",
    "        model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RULmodel_LSTM2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9679e7d6d918>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Define the usable models for this notebook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'lstm-20-10'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mRULmodel_LSTM2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#models = {'shallow-250-100':RULmodel_SN_4, 'shallow-100-50':RULmodel_SN_1, 'shallow-50-20':RULmodel_SN_2,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RULmodel_LSTM2' is not defined"
     ]
    }
   ],
   "source": [
    "#Define the usable models for this notebook\n",
    "\n",
    "models = {'lstm-20-10':RULmodel_LSTM2}\n",
    "\n",
    "#models = {'shallow-250-100':RULmodel_SN_4, 'shallow-100-50':RULmodel_SN_1, 'shallow-50-20':RULmodel_SN_2,\n",
    "#          'shallow-20-20':RULmodel_SN_3, 'shallow-20':RULmodel_SN_5, 'shallow-10':RULmodel_SN_6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../CMAPSSData'\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 128\n",
    "\n",
    "#scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dHandler_cmaps = CMAPSDataHandler(data_folder, 1, selected_features, max_rul, \n",
    "                                  window_size, window_stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LSTM\n",
    "#tModel.data_handler.data_scaler = scaler\n",
    "#tModel.data_scaler = None\n",
    "\n",
    "#For ANN\n",
    "#tModel.data_handler.data_scaler = None\n",
    "#tModel.data_scaler = scaler\n",
    "\n",
    "#tModel.data_handler.sequence_length = 30\n",
    "#tModel.data_handler.sequence_length = maxWindowSize[datasetNumber]\n",
    "#tModel.data_handler.sequence_stride = 1\n",
    "#tModel.data_handler.max_rul = 128\n",
    "\n",
    "#tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "#tModel.print_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model lstm-20-10\n",
      "Computing for dataset 1\n",
      "100/100 [==============================] - 0s 908us/step\n",
      "100/100 [==============================] - 0s 1ms/step\n",
      "100/100 [==============================] - 0s 1ms/step\n",
      "100/100 [==============================] - 0s 1ms/step\n",
      "100/100 [==============================] - 0s 1ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "Results for model lstm-20-10\n",
      "DescribeResult(nobs=10, minmax=(array([14.05311353]), array([16.95405556])), mean=array([15.40438337]), variance=array([0.59885903]), skewness=array([0.25049903]), kurtosis=array([0.24450839]))\n",
      "DescribeResult(nobs=10, minmax=(array([3.38318361]), array([6.03681275])), mean=array([4.4024342]), variance=array([0.73696]), skewness=array([0.7398216]), kurtosis=array([-0.44529501]))\n",
      "DescribeResult(nobs=10, minmax=(array([22.28814782]), array([23.63712607])), mean=array([22.79051421]), variance=array([0.1396914]), skewness=array([1.04947319]), kurtosis=array([0.75605086]))\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "num_features = len(selected_features)\n",
    "\n",
    "windowSize = 30\n",
    "windowStride = 1\n",
    "constRul = 125\n",
    "\n",
    "file = open(\"results/MLP/ResultsDataset_lstm_3.csv\", \"w\")\n",
    "csvfile = csv.writer(file, lineterminator='\\n')\n",
    "\n",
    "tModel = SequenceTunableModelRegression('rnn-20-10', None, lib_type='keras', data_handler=dHandler_cmaps)\n",
    "tModel.epochs = 100\n",
    "\n",
    "#For LSTM\n",
    "tModel.data_handler.data_scaler = scaler\n",
    "tModel.data_scaler = None\n",
    "\n",
    "#For ANN\n",
    "#tModel.data_handler.data_scaler = None\n",
    "#tModel.data_scaler = min_max_scaler\n",
    "\n",
    "for key, model_def in models.items():\n",
    "  \n",
    "    print(\"For model \"+str(key))\n",
    "    #file.write(\"For model \"+str(key)+'\\n\\n')\n",
    "  \n",
    "    for i in range(1,2):\n",
    "\n",
    "        dataset = i\n",
    "        print(\"Computing for dataset \"+str(i))\n",
    "        #file.write(\"Computing for dataset \"+str(i)+'\\n\\n')\n",
    "      \n",
    "        tempScoresRMSE = np.zeros((iterations,1))\n",
    "        tempScoresRHS = np.zeros((iterations,1))\n",
    "        tempTime = np.zeros((iterations,1))\n",
    "      \n",
    "        tModel.data_handler.change_dataset(i)\n",
    "        tModel.data_handler.sequence_length = windowSize\n",
    "        tModel.data_handler.sequence_stride = windowStride\n",
    "        tModel.data_handler.max_rul = constRul\n",
    "        #tModel.load_data(unroll=True, verbose=0, cross_validation_ratio=0)\n",
    "        tModel.load_data(unroll=False, verbose=0, cross_validation_ratio=0)\n",
    "        #tModel.print_data()\n",
    "        \n",
    "        #input_shape = windowSize*num_features #For simple ANN\n",
    "        input_shape = (tModel.data_handler.sequence_length,num_features) #For RNN\n",
    "\n",
    "        for j in range(iterations):\n",
    "\n",
    "            #Model needs to be recompiled everytime since they are different runs so weights should be reinit\n",
    "            #model = get_compiled_model(model_def, input_shape, model_type='ann')\n",
    "            model = get_compiled_model(model_def, input_shape, model_type='lstm')\n",
    "\n",
    "            tModel.change_model(key, model, 'keras')\n",
    "            tModel.train_model(verbose=0, learningRate_scheduler=lrate)\n",
    "            #tModel.train_model(learningRate_scheduler=lrate, verbose=0)\n",
    "            tModel.evaluate_model(['rhs', 'rmse'], round=2)\n",
    "            #print(\"scores\")\n",
    "          \n",
    "            #print(j)\n",
    "\n",
    "            cScores = tModel.scores\n",
    "            rmse = math.sqrt(cScores['score_1'])\n",
    "            rmse2 = cScores['rmse']\n",
    "            rhs = cScores['rhs']\n",
    "            time = tModel.train_time\n",
    "          \n",
    "            tempScoresRMSE[j] = rmse2\n",
    "            tempScoresRHS[j] = rhs\n",
    "            tempTime[j] = time\n",
    "\n",
    "        print(\"Results for model \" + key)\n",
    "  \n",
    "        print(stats.describe(tempScoresRMSE))\n",
    "        print(stats.describe(tempScoresRHS))\n",
    "        print(stats.describe(tempTime))\n",
    "          \n",
    "        tempScoresRMSE = np.reshape(tempScoresRMSE, (iterations,))\n",
    "        tempScoresRHS = np.reshape(tempScoresRHS, (iterations,))\n",
    "        tempTime = np.reshape(tempTime, (iterations,))\n",
    "        csvfile.writerow(tempScoresRMSE)\n",
    "        csvfile.writerow(tempScoresRHS)\n",
    "        csvfile.writerow(tempTime)\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [1,2,3,4]\n",
    "iterations = 10\n",
    "tModel.epochs = 150\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "scores ={1:[], 2:[], 3:[], 4:[]}\n",
    "window_sizes = {1:17,2:17,3:17,4:17}\n",
    "strides = {1:1,2:1,3:1,4:1}\n",
    "max_ruls = {1:129, 2:129, 3:129, 4:129}\n",
    "num_features = len(selected_features)\n",
    "\n",
    "input_shape = None\n",
    "models = {'shallow-20-20':RULmodel_SN_4}\n",
    "\n",
    "#For each model\n",
    "for key, model_def in models.items():\n",
    "    file = open(\"results/MLP/ResultsDatasets_singleSet\"+key+\".csv\", \"w\")\n",
    "    csvfile = csv.writer(file, lineterminator='\\n')\n",
    "    \n",
    "    print(\"Generating statistics for model \" + key)\n",
    "\n",
    "    #For each dataset\n",
    "    for i in range(1,5):\n",
    "        \n",
    "        print(\"Working on dataset \" + str(i))\n",
    "        \n",
    "        tempScoresRMSE = np.zeros((iterations,1))\n",
    "        tempScoresRHS = np.zeros((iterations,1))\n",
    "        tempTime = np.zeros((iterations,1))\n",
    "        \n",
    "        input_shape = window_sizes[i]*num_features #For simple ANN\n",
    "        #input_shape = (window_sizes[i],num_features) #For RNN\n",
    "        \n",
    "        print(input_shape)\n",
    "        \n",
    "        tModel.data_handler.change_dataset(i)\n",
    "        tModel.data_handler.sequence_length = window_sizes[i]\n",
    "        tModel.data_handler.sequence_stride = strides[i]\n",
    "        tModel.data_handler.max_rul = max_ruls[i]\n",
    "        tModel.load_data(unroll=True, verbose=0, cross_validation_ratio=0)\n",
    "        #tModel.print_data()\n",
    "        \n",
    "        #tModel.print_data()\n",
    "        \n",
    "        for j in range(iterations):\n",
    "\n",
    "            #Model needs to be recompiled everytime since they are different runs so weights should be reinit\n",
    "            model = get_compiled_model(model_def, input_shape, model_type='ann')\n",
    "\n",
    "            tModel.change_model(key, model, 'keras')\n",
    "            tModel.train_model(learningRate_scheduler=lrate, verbose=0)\n",
    "            tModel.evaluate_model(['rhs', 'rmse'], round=2)\n",
    "            #print(\"scores\")\n",
    "            \n",
    "            #print(j)\n",
    "\n",
    "            cScores = tModel.scores\n",
    "            rmse = math.sqrt(cScores['score_1'])\n",
    "            rmse2 = cScores['rmse']\n",
    "            rhs = cScores['rhs']\n",
    "            time = tModel.train_time\n",
    "            \n",
    "            tempScoresRMSE[j] = rmse2\n",
    "            tempScoresRHS[j] = rhs\n",
    "            tempTime[j] = time\n",
    "            \n",
    "        print(\"Results for model \" + key)\n",
    "    \n",
    "        print(stats.describe(tempScoresRMSE))\n",
    "        print(stats.describe(tempScoresRHS))\n",
    "        print(stats.describe(tempTime))\n",
    "            \n",
    "        tempScoresRMSE = np.reshape(tempScoresRMSE, (iterations,))\n",
    "        tempScoresRHS = np.reshape(tempScoresRHS, (iterations,))\n",
    "        tempTime = np.reshape(tempTime, (iterations,))\n",
    "        csvfile.writerow(tempScoresRMSE)\n",
    "        csvfile.writerow(tempScoresRHS)\n",
    "        csvfile.writerow(tempTime)\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
