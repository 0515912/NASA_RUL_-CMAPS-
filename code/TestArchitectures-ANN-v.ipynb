{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Test different MLP architectures. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import custom_scores\n",
    "from data_handler_CMAPS import CMAPSDataHandler\n",
    "from tunable_model import SequenceTunableModelRegression\n",
    "import CMAPSAuxFunctions\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D,CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers.noise import GaussianNoise\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define architectures\n",
    "\n",
    "Define each one of the different architectures to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()  #Clear the previous tensorflow graph\n",
    "\n",
    "l2_lambda_regularization = 0.20\n",
    "l1_lambda_regularization = 0.10\n",
    "\n",
    "def RULmodel_LSTM(input_shape):\n",
    "    \"\"\"Define the RNN model\"\"\"\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "#     #model.add(Masking(mask_value=0, imput))\n",
    "#     #model.add(LSTM(input_shape=input_shape, units=100, return_sequences=True, name='lstm1')))\n",
    "#     model.add(LSTM(input_shape=input_shape, units=150, dropout=0.2, recurrent_dropout=0.2, return_sequences=False, name='lstm1'))\n",
    "# #     model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "# #                     kernel_regularizer=regularizers.l2(l2_lambda_regularization), name='fc1'))\n",
    "#     model.add(Dense(1, activation='linear', kernel_initializer='glorot_normal', name='out'))\n",
    "    #model.add(LSTM(input_shape=input_shape, units=150, dropout=0.1, recurrent_dropout=0.2, return_sequences=False, name='lstm1'))\n",
    "    model.add(CuDNNLSTM(input_shape=input_shape, units=150, return_sequences=False, name='lstm1'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear', kernel_initializer='glorot_normal', name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# def RULmodel_SN_4(input_shape):\n",
    "#     #Create a sequential model\n",
    "#     model = Sequential()\n",
    "# #     model.add(GaussianNoise( 0.02 ))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     #Add the layers for the model\n",
    "# #     model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "# #                     kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "# #                     name='fc1'))\n",
    "# #     model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "# #                     kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "# #                     name='fc2'))\n",
    "# #     model.add(Dense(1, activation='linear', kernel_initializer='glorot_normal', \n",
    "# #                     kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "\n",
    "#     model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "#                     kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "#                     name='fc1'))\n",
    "#     #model.add(Dropout(0.2))\n",
    "#     model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "#                     kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "#                     name='fc2'))\n",
    "    \n",
    "# #     model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "# #                     kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "# #                     name='fc3'))\n",
    "    \n",
    "#     model.add(Dense(1, activation='linear', kernel_initializer='glorot_normal', \n",
    "#                     kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RULmodel_SN_1(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(250, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RULmodel_SN_2(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(100, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RULmodel_SN_3(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(20, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc2'))\n",
    "    model.add(Dense(1, activation='linear', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RULmodel_SN_5(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def RULmodel_SN_6(input_shape):\n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(50, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), \n",
    "                    name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=regularizers.L1L2(l1_lambda_regularization, l2_lambda_regularization), name='out'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(model_def, shape, model_type='ann'):\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    #Shared parameters for the models\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.5)\n",
    "    lossFunction = \"mean_squared_error\"\n",
    "    metrics = [\"mse\"]\n",
    "    model = None\n",
    "\n",
    "    #Create and compile the models\n",
    "\n",
    "    if model_type=='ann':\n",
    "        model = model_def(shape)\n",
    "        model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    elif model_type=='lstm':\n",
    "        model = RULmodel_LSTM(shape)\n",
    "        model.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the usable models for this notebook\n",
    "\n",
    "models = {'lstm-20-10':RULmodel_LSTM}\n",
    "#models = {'shallow-16-16-12': RULmodel_SN_4}\n",
    "\n",
    "#models = {'shallow-250-100':RULmodel_SN_4, 'shallow-100-50':RULmodel_SN_1, 'shallow-50-20':RULmodel_SN_2,\n",
    "#          'shallow-20-20':RULmodel_SN_3, 'shallow-20':RULmodel_SN_5, 'shallow-10':RULmodel_SN_6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../CMAPSSData'\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 125\n",
    "\n",
    "dHandler_cmaps = CMAPSDataHandler(data_folder, 1, selected_features, max_rul, \n",
    "                                  window_size, window_stride)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler = StandardScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LSTM\n",
    "#tModel.data_handler.data_scaler = scaler\n",
    "#tModel.data_scaler = None\n",
    "\n",
    "#For ANN\n",
    "# tModel.data_handler.data_scaler = None\n",
    "# tModel.data_scaler = scaler\n",
    "\n",
    "# tModel.data_handler.sequence_length = 24\n",
    "# tModel.data_handler.sequence_length = maxWindowSize[datasetNumber]\n",
    "# tModel.data_handler.sequence_stride = 1\n",
    "# tModel.data_handler.max_rul = 129\n",
    "\n",
    "# tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0.3)\n",
    "# tModel.print_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model lstm-20-10\n",
      "Computing for dataset 1\n",
      "Loading data for dataset 1 with window_size of 30, stride of 1 and maxRUL of 125. Cros-Validation ratio 0.2\n",
      "Loading data from file and computing dataframes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/controlslab/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/controlslab/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/media/controlslab/DATA/Projects/NASA_RUL_-CMAPS-/code/data_handler_CMAPS.py:133: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  norm_test_df = pd.DataFrame(self._data_scaler.transform(self._df_test[cols_normalize]), columns=cols_normalize, index=self._df_test.index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv number\n",
      "[91, 40, 33, 2, 73, 24, 78, 76, 6, 21, 65, 48, 11, 79, 1, 58, 51, 60, 88, 57]\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 92us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 97us/step\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 99us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 101us/step\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 94us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 102us/step\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 97us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 103us/step\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 88us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 106us/step\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 103us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 109us/step\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 92us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 92us/step\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 105us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 103us/step\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 103us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 91us/step\n",
      "training with cv\n",
      "CV set\n",
      "20/20 [==============================] - 0s 93us/step\n",
      "Test set\n",
      "10/10 [==============================] - 0s 109us/step\n",
      "Results for cross validtion model lstm-20-10\n",
      "DescribeResult(nobs=10, minmax=(array([23.010867]), array([28.04817285])), mean=array([25.64704152]), variance=array([2.82140141]), skewness=array([-0.02175492]), kurtosis=array([-1.12529722]))\n",
      "DescribeResult(nobs=10, minmax=(array([10.52356604]), array([24.53166829])), mean=array([15.61582652]), variance=array([22.18199658]), skewness=array([0.8216985]), kurtosis=array([-0.67615936]))\n",
      "DescribeResult(nobs=10, minmax=(array([25.125331]), array([26.637697])), mean=array([25.820319]), variance=array([0.24504046]), skewness=array([0.34949773]), kurtosis=array([-1.11492467]))\n",
      "Results for test model lstm-20-10\n",
      "DescribeResult(nobs=10, minmax=(array([14.32829369]), array([20.59126028])), mean=array([17.28912262]), variance=array([5.35137652]), skewness=array([0.08238698]), kurtosis=array([-1.65304507]))\n",
      "DescribeResult(nobs=10, minmax=(array([2.78151767]), array([6.64577532])), mean=array([4.44016429]), variance=array([1.98069468]), skewness=array([0.28553902]), kurtosis=array([-1.4620884]))\n",
      "DescribeResult(nobs=10, minmax=(array([25.125331]), array([26.637697])), mean=array([25.820319]), variance=array([0.24504046]), skewness=array([0.34949773]), kurtosis=array([-1.11492467]))\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "num_features = len(selected_features)\n",
    "\n",
    "windowSize = 30\n",
    "#tModel.data_handler.sequence_length = maxWindowSize[datasetNumber]\n",
    "windowStride = 1\n",
    "constRul = 125\n",
    "\n",
    "file = open(\"results/LSTM/1layerdrop0.2.csv\", \"w\")\n",
    "csvfile = csv.writer(file, lineterminator='\\n')\n",
    "\n",
    "tModel = SequenceTunableModelRegression('lstm-20-10', None, lib_type='keras', data_handler=dHandler_cmaps)\n",
    "tModel.epochs = 100\n",
    "\n",
    "#For LSTM\n",
    "tModel.data_handler.data_scaler = scaler\n",
    "tModel.data_scaler = None\n",
    "\n",
    "#For ANN\n",
    "# tModel.data_handler.data_scaler = None\n",
    "# #tModel.data_scaler = min_max_scaler\n",
    "# tModel.data_scaler = scaler\n",
    "\n",
    "for key, model_def in models.items():\n",
    "  \n",
    "    print(\"For model \"+str(key))\n",
    "    #file.write(\"For model \"+str(key)+'\\n\\n')\n",
    "  \n",
    "    for i in range(1,2):\n",
    "\n",
    "        dataset = i\n",
    "        print(\"Computing for dataset \"+str(i))\n",
    "        #file.write(\"Computing for dataset \"+str(i)+'\\n\\n')\n",
    "      \n",
    "        tempScoresRMSE = np.zeros((iterations,1))\n",
    "        tempScoresRHS = np.zeros((iterations,1))\n",
    "        tempTime = np.zeros((iterations,1))\n",
    "        \n",
    "        cv_tempScoresRMSE = np.zeros((iterations,1))\n",
    "        cv_tempScoresRHS = np.zeros((iterations,1))\n",
    "        cv_tempTime = np.zeros((iterations,1))\n",
    "      \n",
    "        tModel.data_handler.change_dataset(i)\n",
    "        tModel.data_handler.sequence_length = windowSize\n",
    "        tModel.data_handler.sequence_stride = windowStride\n",
    "        tModel.data_handler.max_rul = constRul\n",
    "        #tModel.load_data(unroll=True, verbose=0, cross_validation_ratio=0.2)\n",
    "        #for lstm\n",
    "        tModel.load_data(unroll=False, verbose=1, cross_validation_ratio=0.2)\n",
    "        #tModel.print_data()\n",
    "        \n",
    "        #input_shape = tModel.data_handler.sequence_length*num_features #For simple ANN\n",
    "        input_shape = (tModel.data_handler.sequence_length,num_features) #For RNN\n",
    "\n",
    "        for j in range(iterations):\n",
    "\n",
    "            #Model needs to be recompiled everytime since they are different runs so weights should be reinit\n",
    "            #model = get_compiled_model(model_def, input_shape, model_type='ann')\n",
    "            model = get_compiled_model(model_def, input_shape, model_type='lstm')\n",
    "\n",
    "            tModel.change_model(key, model, 'keras')\n",
    "            #tModel.train_model(verbose=0, learningRate_scheduler=lrate)\n",
    "            tModel.train_model(verbose=0) #lstm\n",
    "            print(\"CV set\")\n",
    "            tModel.evaluate_model(['rhs', 'rmse'], round=2, cross_validation=True)\n",
    "\n",
    "            cScores = tModel.scores\n",
    "            rmse = math.sqrt(cScores['score_1'])\n",
    "            rmse2 = cScores['rmse']\n",
    "            rhs = cScores['rhs']\n",
    "            time = tModel.train_time\n",
    "          \n",
    "            cv_tempScoresRMSE[j] = rmse2\n",
    "            cv_tempScoresRHS[j] = rhs\n",
    "            cv_tempTime[j] = time\n",
    "            \n",
    "            print(\"Test set\")\n",
    "            tModel.evaluate_model(['rhs', 'rmse'], round=2, cross_validation=False)\n",
    "\n",
    "            cScores = tModel.scores\n",
    "            rmse = math.sqrt(cScores['score_1'])\n",
    "            rmse2 = cScores['rmse']\n",
    "            rhs = cScores['rhs']\n",
    "            time = tModel.train_time\n",
    "          \n",
    "            tempScoresRMSE[j] = rmse2\n",
    "            tempScoresRHS[j] = rhs\n",
    "            tempTime[j] = time\n",
    "\n",
    "        print(\"Results for cross validtion model \" + key)\n",
    "  \n",
    "        print(stats.describe(cv_tempScoresRMSE))\n",
    "        print(stats.describe(cv_tempScoresRHS))\n",
    "        print(stats.describe(cv_tempTime))\n",
    "        \n",
    "        cv_tempScoresRMSE = np.reshape(cv_tempScoresRMSE, (iterations,))\n",
    "        cv_tempScoresRHS = np.reshape(cv_tempScoresRHS, (iterations,))\n",
    "        cv_tempTime = np.reshape(cv_tempTime, (iterations,))\n",
    "        \n",
    "        csvfile.writerow(cv_tempScoresRMSE)\n",
    "        csvfile.writerow(cv_tempScoresRHS)\n",
    "        csvfile.writerow(cv_tempTime)\n",
    "\n",
    "        print(\"Results for test model \" + key)\n",
    "  \n",
    "        print(stats.describe(tempScoresRMSE))\n",
    "        print(stats.describe(tempScoresRHS))\n",
    "        print(stats.describe(tempTime))\n",
    "          \n",
    "        tempScoresRMSE = np.reshape(tempScoresRMSE, (iterations,))\n",
    "        tempScoresRHS = np.reshape(tempScoresRHS, (iterations,))\n",
    "        tempTime = np.reshape(tempTime, (iterations,))\n",
    "        \n",
    "        \n",
    "        csvfile.writerow(tempScoresRMSE)\n",
    "        csvfile.writerow(tempScoresRHS)\n",
    "        csvfile.writerow(tempTime)\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = [1,2,3,4]\n",
    "# iterations = 10\n",
    "# tModel.epochs = 150\n",
    "# lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "# scores ={1:[], 2:[], 3:[], 4:[]}\n",
    "# window_sizes = {1:17,2:17,3:17,4:17}\n",
    "# strides = {1:1,2:1,3:1,4:1}\n",
    "# max_ruls = {1:129, 2:129, 3:129, 4:129}\n",
    "# num_features = len(selected_features)\n",
    "\n",
    "# input_shape = None\n",
    "# models = {'shallow-20-20':RULmodel_SN_4}\n",
    "\n",
    "# #For each model\n",
    "# for key, model_def in models.items():\n",
    "#     file = open(\"results/MLP/ResultsDatasets_singleSet\"+key+\".csv\", \"w\")\n",
    "#     csvfile = csv.writer(file, lineterminator='\\n')\n",
    "    \n",
    "#     print(\"Generating statistics for model \" + key)\n",
    "\n",
    "#     #For each dataset\n",
    "#     for i in range(1,5):\n",
    "        \n",
    "#         print(\"Working on dataset \" + str(i))\n",
    "        \n",
    "#         tempScoresRMSE = np.zeros((iterations,1))\n",
    "#         tempScoresRHS = np.zeros((iterations,1))\n",
    "#         tempTime = np.zeros((iterations,1))\n",
    "        \n",
    "#         input_shape = window_sizes[i]*num_features #For simple ANN\n",
    "#         #input_shape = (window_sizes[i],num_features) #For RNN\n",
    "        \n",
    "#         print(input_shape)\n",
    "        \n",
    "#         tModel.data_handler.change_dataset(i)\n",
    "#         tModel.data_handler.sequence_length = window_sizes[i]\n",
    "#         tModel.data_handler.sequence_stride = strides[i]\n",
    "#         tModel.data_handler.max_rul = max_ruls[i]\n",
    "#         tModel.load_data(unroll=True, verbose=0, cross_validation_ratio=0)\n",
    "#         #tModel.print_data()\n",
    "        \n",
    "#         #tModel.print_data()\n",
    "        \n",
    "#         for j in range(iterations):\n",
    "\n",
    "#             #Model needs to be recompiled everytime since they are different runs so weights should be reinit\n",
    "#             model = get_compiled_model(model_def, input_shape, model_type='ann')\n",
    "\n",
    "#             tModel.change_model(key, model, 'keras')\n",
    "#             tModel.train_model(learningRate_scheduler=lrate, verbose=0)\n",
    "#             tModel.evaluate_model(['rhs', 'rmse'], round=2)\n",
    "#             #print(\"scores\")\n",
    "            \n",
    "#             #print(j)\n",
    "\n",
    "#             cScores = tModel.scores\n",
    "#             rmse = math.sqrt(cScores['score_1'])\n",
    "#             rmse2 = cScores['rmse']\n",
    "#             rhs = cScores['rhs']\n",
    "#             time = tModel.train_time\n",
    "            \n",
    "#             tempScoresRMSE[j] = rmse2\n",
    "#             tempScoresRHS[j] = rhs\n",
    "#             tempTime[j] = time\n",
    "            \n",
    "#         print(\"Results for model \" + key)\n",
    "    \n",
    "#         print(stats.describe(tempScoresRMSE))\n",
    "#         print(stats.describe(tempScoresRHS))\n",
    "#         print(stats.describe(tempTime))\n",
    "            \n",
    "#         tempScoresRMSE = np.reshape(tempScoresRMSE, (iterations,))\n",
    "#         tempScoresRHS = np.reshape(tempScoresRHS, (iterations,))\n",
    "#         tempTime = np.reshape(tempTime, (iterations,))\n",
    "#         csvfile.writerow(tempScoresRMSE)\n",
    "#         csvfile.writerow(tempScoresRHS)\n",
    "#         csvfile.writerow(tempTime)\n",
    "    \n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
