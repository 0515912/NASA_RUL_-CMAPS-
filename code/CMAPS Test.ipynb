{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Approach using MLP. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 6,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import CMAPSAuxFunctions\n",
    "#import plottingTools\n",
    "#from datetime import datetime\n",
    "#from sklearn.covariance import EllipticEnvelope\n",
    "#from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.model_selection import train_test_split, cross_validate\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#from dataManagement import DataManagerDamadics\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib notebook\n",
    "\n",
    "global constRUL\n",
    "\n",
    "constRUL = 125\n",
    "time_window = 30\n",
    "rul_vector = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and Reshape data\n",
    "\n",
    "Get the data from the text files, store it in a Pandas Dataframe and reshape it as appropiately."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20631, 26)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-088d105822d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#Get the X and y matrices with the specified time window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m X_train, y_train = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_train, selected_features, \n\u001b[0;32m---> 16\u001b[0;31m                                                         time_window, 'train', scaler=min_max_scaler)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#X_test, _ = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_test, selected_features,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University of California/Research/Projects/NASA_RUL_(CMAPS)/code/CMAPSAuxFunctions.py\u001b[0m in \u001b[0;36mretrieve_and_reshape_data\u001b[0;34m(from_file, selected_features, time_window, dataset_type, constRUL, unit_Number, scaler)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mdf_selected_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_features_rul\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_X_y_from_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_selected_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/University of California/Research/Projects/NASA_RUL_(CMAPS)/code/CMAPSAuxFunctions.py\u001b[0m in \u001b[0;36mget_X_y_from_df\u001b[0;34m(df, time_window, features, num_units, dataset_type)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m#Create the numpy arrays to hold the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdataset_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cross_validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_x\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtime_window\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_x\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtime_window\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "data_file_train = '../CMAPSSData/train_FD001.txt'\n",
    "data_file_test = '../CMAPSSData/test_FD001.txt'\n",
    "\n",
    "#min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "standardScaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "#Selected as per CNN paper\n",
    "selected_features = ['T24', 'T30', 'T50', 'P30', 'Nf', 'Nc', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'htBleed', 'W31', 'W32']\n",
    "\n",
    "nFeatures = len(selected_features)\n",
    "\n",
    "#Get the X and y matrices with the specified time window\n",
    "X_train, y_train = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_train, selected_features, \n",
    "                                                        time_window, 'train', scaler=min_max_scaler)\n",
    "\n",
    "#X_test, _ = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_test, selected_features, \n",
    "#                                                        time_window, 'test', scaler=min_max_scaler)\n",
    "\n",
    "y_test = np.loadtxt(\"../CMAPSSData/RUL_FD001.txt\")\n",
    "y_test = np.array([x if x < constRUL else constRUL for x in y_test])\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "\n",
<<<<<<< Updated upstream
    "X_train, y_train, X_test, y_test = CMAPSAuxFunctions.get_X_y_from_Dataset('1', '../CMAPSSData/', constRUL, time_window)\n",
    "X_train = np.reshape(X_train, newshape=(X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
    "X_test = np.reshape(X_test, newshape=(X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
    "y_train = np.reshape(y_train, newshape=(y_train.shape[0], 1))\n",
    "y_test = np.reshape(y_test, newshape=(y_test.shape[0], 1))\n",
=======
    "'''X_train, y_train, X_test, y_test = CMAPSAuxFunctions.get_X_y_from_Dataset('1', '../CMAPSSData/', constRUL, time_window)\n",
    "X_train = np.reshape(X_train, newshape=(X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
    "X_test = np.reshape(X_test, newshape=(X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
    "y_train = np.reshape(y_train, newshape=(y_train.shape[0], 1))\n",
    "y_test = np.reshape(y_test, newshape=(y_test.shape[0], 1))'''\n",
>>>>>>> Stashed changes
    "\n",
    "\n",
    "#Standardize the data\n",
    "#X_train = min_max_scaler.fit_transform(X_train)\n",
    "#X_test = min_max_scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "(17731, 420)\n",
      "(17731, 1)\n",
      "Testing data\n",
      "(100, 420)\n",
      "(100, 1)\n",
      "Training data\n",
      "[[ 0.42168675  0.12579028  0.17049291 ...  0.5        -0.45736434\n",
      "  -0.78099972]\n",
      " [ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.75193798\n",
      "  -0.26760563]\n",
      " [-0.01204819 -0.02419882  0.47467927 ...  0.66666667 -0.53488372\n",
      "  -0.89201878]\n",
      " [-0.09638554 -0.02419882  0.53376097 ...  0.16666667 -0.76744186\n",
      "  -0.53106877]\n",
      " [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333 -0.64341085\n",
      "  -0.56365645]]\n",
      "[[4]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [0]]\n",
      "Testing data\n",
      "[[-0.55421687 -0.61107478 -0.55232951 ... -0.5         0.27131783\n",
      "   0.56420878]\n",
      " [-0.02409639 -0.30978853 -0.32343011 ...  0.16666667 -0.27131783\n",
      "   0.10770505]\n",
      " [-0.15662651 -0.05777196 -0.29642134 ...  0.         -0.03875969\n",
      "   0.28859431]\n",
      " [-0.40963855 -0.19032047 -0.56617151 ... -0.5         0.25581395\n",
      "   0.28500414]\n",
      " [-0.30722892 -0.11881404 -0.1144497  ...  0.33333333 -0.13178295\n",
      "  -0.1955261 ]]\n",
      "[[125]\n",
      " [ 82]\n",
      " [ 59]\n",
      " [117]\n",
      " [ 20]]\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "print(\"Training data\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"Testing data\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\"Training data\")\n",
    "print(X_train[-5:,:])\n",
    "print(y_train[-5:,:])\n",
    "print(\"Testing data\")\n",
    "print(X_test[-5:,:])\n",
    "print(y_test[-5:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model\n",
    "\n",
    "We will use a very simple ANN for this example. The model is Dense(ReLU, 100)->Dense(ReLu, 100)->Dense(Linear, 1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 28,
=======
   "execution_count": 9,
>>>>>>> Stashed changes
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RULmodel(input_shape):\n",
    "    \n",
    "    print(input_shape)\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(500, input_dim=input_shape, activation='tanh', kernel_initializer='glorot_normal', name='fc1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='tanh', kernel_initializer='glorot_normal', name='fc2'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(100, activation='tanh', name='fc3'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(10, activation='tanh', name='fc4'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    #create a placeholder for the input\n",
    "    #X_input = Input(shape=(input_shape))\n",
    "    \n",
    "    #Create the layers\n",
    "    #X = Dense(100, activation='relu', name='fc1')(X_input)\n",
    "    #X = Dense(100, activation='relu', name='fc2')(X)\n",
    "    #X = Dense(1, activation='linear', name='out')(X)\n",
    "    \n",
    "    # Create model. This creates the Keras model instance, you'll use this instance to train/test the model.\n",
    "    #model = Sequential(inputs = X_input, outputs = X, name='RUL')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the keras model\n",
    "Fit the Keras model to the data and determine its performance."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 29,
=======
   "execution_count": 10,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n",
      "Epoch 1/250\n",
<<<<<<< Updated upstream
      "17731/17731 [==============================] - 0s 13us/step - loss: 6335.1290 - mean_squared_error: 6335.1290\n",
      "Epoch 2/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 5675.1038 - mean_squared_error: 5675.1038\n",
      "Epoch 3/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 5244.6205 - mean_squared_error: 5244.6205\n",
      "Epoch 4/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 4851.1704 - mean_squared_error: 4851.1704\n",
      "Epoch 5/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 4490.4882 - mean_squared_error: 4490.4882\n",
      "Epoch 6/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 4151.0204 - mean_squared_error: 4151.0204\n",
      "Epoch 7/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3839.1931 - mean_squared_error: 3839.1931\n",
      "Epoch 8/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3545.9781 - mean_squared_error: 3545.9781\n",
      "Epoch 9/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3280.1871 - mean_squared_error: 3280.1871\n",
      "Epoch 10/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3036.7135 - mean_squared_error: 3036.7135\n",
      "Epoch 11/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2801.5128 - mean_squared_error: 2801.5128\n",
      "Epoch 12/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2594.8191 - mean_squared_error: 2594.8191\n",
      "Epoch 13/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2395.7986 - mean_squared_error: 2395.7986\n",
      "Epoch 14/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2220.1299 - mean_squared_error: 2220.1299: 0s - loss: 2271.2580 - mean_squared_error: 2271.\n",
      "Epoch 15/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2041.0942 - mean_squared_error: 2041.0942\n",
      "Epoch 16/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1896.2381 - mean_squared_error: 1896.2381\n",
      "Epoch 17/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1756.7298 - mean_squared_error: 1756.7298\n",
      "Epoch 18/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 1624.0311 - mean_squared_error: 1624.0311\n",
      "Epoch 19/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 1506.4642 - mean_squared_error: 1506.4642\n",
      "Epoch 20/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1404.9985 - mean_squared_error: 1404.9985\n",
      "Epoch 21/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1299.8259 - mean_squared_error: 1299.8259\n",
      "Epoch 22/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 1206.4207 - mean_squared_error: 1206.4207\n",
      "Epoch 23/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1126.8711 - mean_squared_error: 1126.8711\n",
      "Epoch 24/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1052.5638 - mean_squared_error: 1052.5638\n",
      "Epoch 25/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 979.6480 - mean_squared_error: 979.6480\n",
      "Epoch 26/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 910.8254 - mean_squared_error: 910.8254\n",
      "Epoch 27/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 855.3054 - mean_squared_error: 855.3054\n",
      "Epoch 28/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 802.6121 - mean_squared_error: 802.6121\n",
      "Epoch 29/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 757.5916 - mean_squared_error: 757.5916\n",
      "Epoch 30/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 714.7993 - mean_squared_error: 714.7993\n",
      "Epoch 31/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 674.1612 - mean_squared_error: 674.1612\n",
      "Epoch 32/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 639.6175 - mean_squared_error: 639.6175\n",
      "Epoch 33/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 601.6746 - mean_squared_error: 601.6746\n",
      "Epoch 34/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 572.4981 - mean_squared_error: 572.4981\n",
      "Epoch 35/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 543.0838 - mean_squared_error: 543.0838\n",
      "Epoch 36/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 524.8199 - mean_squared_error: 524.8199\n",
      "Epoch 37/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 500.9139 - mean_squared_error: 500.9139\n",
      "Epoch 38/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 478.4121 - mean_squared_error: 478.4121\n",
      "Epoch 39/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 461.5738 - mean_squared_error: 461.5738\n",
      "Epoch 40/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 444.9050 - mean_squared_error: 444.9050\n",
      "Epoch 41/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 434.4851 - mean_squared_error: 434.4851\n",
      "Epoch 42/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 416.3182 - mean_squared_error: 416.3182\n",
      "Epoch 43/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 406.2215 - mean_squared_error: 406.2215\n",
      "Epoch 44/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 388.8638 - mean_squared_error: 388.8638\n",
      "Epoch 45/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 387.7303 - mean_squared_error: 387.7303\n",
      "Epoch 46/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 372.7424 - mean_squared_error: 372.7424\n",
      "Epoch 47/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 366.0062 - mean_squared_error: 366.0062\n",
      "Epoch 48/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 352.9673 - mean_squared_error: 352.9673\n",
      "Epoch 49/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 357.5855 - mean_squared_error: 357.5855\n",
      "Epoch 50/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 346.0164 - mean_squared_error: 346.0164\n",
      "Epoch 51/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 345.8930 - mean_squared_error: 345.8930\n",
      "Epoch 52/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 341.9430 - mean_squared_error: 341.9430\n",
      "Epoch 53/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 333.5820 - mean_squared_error: 333.5820\n",
      "Epoch 54/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 327.1545 - mean_squared_error: 327.1545\n",
      "Epoch 55/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 328.3561 - mean_squared_error: 328.3561\n",
      "Epoch 56/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 320.2059 - mean_squared_error: 320.2059\n",
      "Epoch 57/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 314.3924 - mean_squared_error: 314.3924\n",
      "Epoch 58/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 318.3994 - mean_squared_error: 318.3994\n",
      "Epoch 59/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 309.3444 - mean_squared_error: 309.3444\n",
      "Epoch 60/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 307.1630 - mean_squared_error: 307.1630\n",
      "Epoch 61/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 306.4841 - mean_squared_error: 306.4841\n",
      "Epoch 62/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 304.4009 - mean_squared_error: 304.4009\n",
      "Epoch 63/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 303.4078 - mean_squared_error: 303.4078\n",
      "Epoch 64/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 298.8081 - mean_squared_error: 298.8081\n",
      "Epoch 65/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 297.4879 - mean_squared_error: 297.4879\n",
      "Epoch 66/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 295.6664 - mean_squared_error: 295.6664\n",
      "Epoch 67/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 301.2894 - mean_squared_error: 301.2894\n",
      "Epoch 68/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 294.0188 - mean_squared_error: 294.0188\n",
      "Epoch 69/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 292.2741 - mean_squared_error: 292.2741\n",
      "Epoch 70/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 290.1731 - mean_squared_error: 290.1731\n",
      "Epoch 71/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 291.1203 - mean_squared_error: 291.1203\n",
      "Epoch 72/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 294.1866 - mean_squared_error: 294.1866\n",
      "Epoch 73/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 288.9920 - mean_squared_error: 288.9920\n",
      "Epoch 74/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 283.0023 - mean_squared_error: 283.0023\n",
      "Epoch 75/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 294.6650 - mean_squared_error: 294.6650\n",
      "Epoch 76/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 283.6709 - mean_squared_error: 283.6709\n",
      "Epoch 77/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 286.3872 - mean_squared_error: 286.3872\n",
      "Epoch 78/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 286.2361 - mean_squared_error: 286.2361\n",
      "Epoch 79/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 280.6757 - mean_squared_error: 280.6757\n",
      "Epoch 80/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 278.5512 - mean_squared_error: 278.5512\n",
      "Epoch 81/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 287.5994 - mean_squared_error: 287.5994\n",
      "Epoch 82/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 277.4456 - mean_squared_error: 277.4456\n",
      "Epoch 83/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 274.6253 - mean_squared_error: 274.6253\n",
      "Epoch 84/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 275.1243 - mean_squared_error: 275.1243\n",
      "Epoch 85/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 274.6014 - mean_squared_error: 274.6014\n",
      "Epoch 86/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 276.0267 - mean_squared_error: 276.0267\n",
      "Epoch 87/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 276.1763 - mean_squared_error: 276.1763\n",
      "Epoch 88/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 277.5976 - mean_squared_error: 277.5976\n",
      "Epoch 89/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 275.0446 - mean_squared_error: 275.0446\n",
      "Epoch 90/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 273.5616 - mean_squared_error: 273.5616\n",
      "Epoch 91/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 274.9801 - mean_squared_error: 274.9801\n",
      "Epoch 92/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 271.3998 - mean_squared_error: 271.3998\n",
      "Epoch 93/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 273.2769 - mean_squared_error: 273.2769\n",
      "Epoch 94/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 270.1009 - mean_squared_error: 270.1009\n",
      "Epoch 95/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 268.1360 - mean_squared_error: 268.1360\n",
      "Epoch 96/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 273.3139 - mean_squared_error: 273.3139\n",
      "Epoch 97/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 269.4274 - mean_squared_error: 269.4274\n",
      "Epoch 98/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 275.7265 - mean_squared_error: 275.7265\n",
      "Epoch 99/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 263.1063 - mean_squared_error: 263.1063\n",
      "Epoch 100/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 266.3256 - mean_squared_error: 266.3256\n",
      "Epoch 101/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 268.9591 - mean_squared_error: 268.9591\n",
      "Epoch 102/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 273.2610 - mean_squared_error: 273.2610\n",
      "Epoch 103/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 271.0682 - mean_squared_error: 271.0682\n",
      "Epoch 104/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.3452 - mean_squared_error: 262.3452\n",
      "Epoch 105/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 264.1392 - mean_squared_error: 264.1392\n",
      "Epoch 106/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 265.6442 - mean_squared_error: 265.6442\n",
      "Epoch 107/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 268.2224 - mean_squared_error: 268.2224\n",
      "Epoch 108/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 264.8042 - mean_squared_error: 264.8042\n",
      "Epoch 109/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.8926 - mean_squared_error: 257.8926\n",
      "Epoch 110/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 266.6090 - mean_squared_error: 266.6090\n",
      "Epoch 111/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 263.5458 - mean_squared_error: 263.5458\n",
      "Epoch 112/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 260.8859 - mean_squared_error: 260.8859\n",
      "Epoch 113/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.1112 - mean_squared_error: 262.1112\n",
      "Epoch 114/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 265.3777 - mean_squared_error: 265.3777\n",
      "Epoch 115/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 260.1105 - mean_squared_error: 260.1105\n",
      "Epoch 116/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.4421 - mean_squared_error: 261.4421\n",
      "Epoch 117/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 263.1461 - mean_squared_error: 263.1461\n",
      "Epoch 118/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.7865 - mean_squared_error: 257.7865\n",
      "Epoch 119/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 264.0863 - mean_squared_error: 264.0863\n",
      "Epoch 120/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.7560 - mean_squared_error: 262.7560\n",
      "Epoch 121/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.5801 - mean_squared_error: 261.5801\n",
      "Epoch 122/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.1730 - mean_squared_error: 261.1730\n",
      "Epoch 123/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 260.5187 - mean_squared_error: 260.5187\n",
      "Epoch 124/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 263.8414 - mean_squared_error: 263.8414\n",
      "Epoch 125/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.8023 - mean_squared_error: 256.8023\n",
      "Epoch 126/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 258.1848 - mean_squared_error: 258.1848\n",
      "Epoch 127/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 259.6251 - mean_squared_error: 259.6251\n",
      "Epoch 128/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.3110 - mean_squared_error: 255.3110\n",
      "Epoch 129/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 264.2196 - mean_squared_error: 264.2196\n",
      "Epoch 130/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.5362 - mean_squared_error: 256.5362\n",
      "Epoch 131/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 253.8734 - mean_squared_error: 253.8734\n",
      "Epoch 132/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.4916 - mean_squared_error: 252.4916\n",
      "Epoch 133/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 259.5486 - mean_squared_error: 259.5486\n",
      "Epoch 134/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 260.8266 - mean_squared_error: 260.8266\n",
      "Epoch 135/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.6456 - mean_squared_error: 257.6456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 256.2809 - mean_squared_error: 256.2809\n",
      "Epoch 137/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 257.7277 - mean_squared_error: 257.7277\n",
      "Epoch 138/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 251.2366 - mean_squared_error: 251.2366\n",
      "Epoch 139/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 256.5770 - mean_squared_error: 256.5770\n",
      "Epoch 140/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.8790 - mean_squared_error: 256.8790\n",
      "Epoch 141/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.9468 - mean_squared_error: 256.9468\n",
      "Epoch 142/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.4369 - mean_squared_error: 254.4369\n",
      "Epoch 143/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.4954 - mean_squared_error: 256.4954\n",
      "Epoch 144/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.4923 - mean_squared_error: 250.4923\n",
      "Epoch 145/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.2821 - mean_squared_error: 255.2821\n",
      "Epoch 146/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 259.5073 - mean_squared_error: 259.5073\n",
      "Epoch 147/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.1356 - mean_squared_error: 255.1356\n",
      "Epoch 148/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.9505 - mean_squared_error: 256.9505\n",
      "Epoch 149/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.0774 - mean_squared_error: 254.0774\n",
      "Epoch 150/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.6844 - mean_squared_error: 252.6844\n",
      "Epoch 151/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.3602 - mean_squared_error: 256.3602\n",
      "Epoch 152/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 253.5643 - mean_squared_error: 253.5643\n",
      "Epoch 153/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.9158 - mean_squared_error: 250.9158\n",
      "Epoch 154/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.7309 - mean_squared_error: 249.7309\n",
      "Epoch 155/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.2037 - mean_squared_error: 252.2037\n",
      "Epoch 156/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 253.1075 - mean_squared_error: 253.1075\n",
      "Epoch 157/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.3383 - mean_squared_error: 252.3383\n",
      "Epoch 158/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.1468 - mean_squared_error: 252.1468\n",
      "Epoch 159/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 247.5826 - mean_squared_error: 247.5826\n",
      "Epoch 160/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.9678 - mean_squared_error: 252.9678\n",
      "Epoch 161/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 245.6312 - mean_squared_error: 245.6312\n",
      "Epoch 162/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 248.1394 - mean_squared_error: 248.1394\n",
      "Epoch 163/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.0075 - mean_squared_error: 246.0075\n",
      "Epoch 164/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.7370 - mean_squared_error: 248.7370\n",
      "Epoch 165/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.1111 - mean_squared_error: 250.1111\n",
      "Epoch 166/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 253.8635 - mean_squared_error: 253.8635\n",
      "Epoch 167/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 245.5495 - mean_squared_error: 245.5495\n",
      "Epoch 168/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 245.3121 - mean_squared_error: 245.3121\n",
      "Epoch 169/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.4422 - mean_squared_error: 247.4422\n",
      "Epoch 170/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.5593 - mean_squared_error: 248.5593\n",
      "Epoch 171/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.8951 - mean_squared_error: 249.8951\n",
      "Epoch 172/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.3323 - mean_squared_error: 249.3323\n",
      "Epoch 173/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.9251 - mean_squared_error: 248.9251\n",
      "Epoch 174/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.2380 - mean_squared_error: 248.2380\n",
      "Epoch 175/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 242.7631 - mean_squared_error: 242.7631\n",
      "Epoch 176/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.8137 - mean_squared_error: 249.8137\n",
      "Epoch 177/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.1867 - mean_squared_error: 250.1867\n",
      "Epoch 178/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 246.7431 - mean_squared_error: 246.7431\n",
      "Epoch 179/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 251.5101 - mean_squared_error: 251.5101\n",
      "Epoch 180/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.3788 - mean_squared_error: 247.3788\n",
      "Epoch 181/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.1532 - mean_squared_error: 241.1532\n",
      "Epoch 182/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.5299 - mean_squared_error: 241.5299\n",
      "Epoch 183/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.1041 - mean_squared_error: 246.1041\n",
      "Epoch 184/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 244.6009 - mean_squared_error: 244.6009\n",
      "Epoch 185/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 245.6425 - mean_squared_error: 245.6425\n",
      "Epoch 186/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 242.3501 - mean_squared_error: 242.3501\n",
      "Epoch 187/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.0723 - mean_squared_error: 246.0723\n",
      "Epoch 188/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 242.1040 - mean_squared_error: 242.1040\n",
      "Epoch 189/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 239.1061 - mean_squared_error: 239.1061\n",
      "Epoch 190/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 243.6883 - mean_squared_error: 243.6883\n",
      "Epoch 191/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 243.7246 - mean_squared_error: 243.7246\n",
      "Epoch 192/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 239.3924 - mean_squared_error: 239.3924\n",
      "Epoch 193/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 242.2814 - mean_squared_error: 242.2814\n",
      "Epoch 194/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.6635 - mean_squared_error: 241.6635\n",
      "Epoch 195/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 240.9501 - mean_squared_error: 240.9501\n",
      "Epoch 196/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.2376 - mean_squared_error: 241.2376\n",
      "Epoch 197/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.3312 - mean_squared_error: 247.3312\n",
      "Epoch 198/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 243.8621 - mean_squared_error: 243.8621\n",
      "Epoch 199/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 244.0094 - mean_squared_error: 244.0094\n",
      "Epoch 200/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.5723 - mean_squared_error: 241.5723\n",
      "Epoch 201/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.3645 - mean_squared_error: 241.3645\n",
      "Epoch 202/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.5667 - mean_squared_error: 236.5667\n",
      "Epoch 203/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.8416 - mean_squared_error: 232.8416\n",
      "Epoch 204/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.8266 - mean_squared_error: 236.8266\n",
      "Epoch 205/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.6247 - mean_squared_error: 236.6247\n",
      "Epoch 206/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.1539 - mean_squared_error: 233.1539\n",
      "Epoch 207/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 233.7690 - mean_squared_error: 233.7690\n",
      "Epoch 208/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.0701 - mean_squared_error: 234.0701\n",
      "Epoch 209/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.5106 - mean_squared_error: 229.5106\n",
      "Epoch 210/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 236.8967 - mean_squared_error: 236.8967\n",
      "Epoch 211/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 226.8716 - mean_squared_error: 226.8716\n",
      "Epoch 212/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 232.8274 - mean_squared_error: 232.8274\n",
      "Epoch 213/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 235.2260 - mean_squared_error: 235.2260\n",
      "Epoch 214/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.0826 - mean_squared_error: 236.0826\n",
      "Epoch 215/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 235.2770 - mean_squared_error: 235.2770\n",
      "Epoch 216/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.0707 - mean_squared_error: 231.0707\n",
      "Epoch 217/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.0595 - mean_squared_error: 232.0595\n",
      "Epoch 218/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.5635 - mean_squared_error: 228.5635\n",
      "Epoch 219/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 231.4713 - mean_squared_error: 231.4713\n",
      "Epoch 220/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.2077 - mean_squared_error: 228.2077\n",
      "Epoch 221/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.3001 - mean_squared_error: 229.3001\n",
      "Epoch 222/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 231.1774 - mean_squared_error: 231.1774\n",
      "Epoch 223/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.4091 - mean_squared_error: 233.4091\n",
      "Epoch 224/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.7774 - mean_squared_error: 231.7774\n",
      "Epoch 225/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.8717 - mean_squared_error: 230.8717\n",
      "Epoch 226/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.4150 - mean_squared_error: 228.4150\n",
      "Epoch 227/250\n",
      "17731/17731 [==============================] - 0s 11us/step - loss: 232.8015 - mean_squared_error: 232.8015\n",
      "Epoch 228/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.5124 - mean_squared_error: 232.5124\n",
      "Epoch 229/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.9387 - mean_squared_error: 231.9387\n",
      "Epoch 230/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.1123 - mean_squared_error: 233.1123\n",
      "Epoch 231/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.9448 - mean_squared_error: 230.9448\n",
      "Epoch 232/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.5435 - mean_squared_error: 228.5435\n",
      "Epoch 233/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 231.8919 - mean_squared_error: 231.8919\n",
      "Epoch 234/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 230.2916 - mean_squared_error: 230.2916\n",
      "Epoch 235/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.0990 - mean_squared_error: 233.0990\n",
      "Epoch 236/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.8104 - mean_squared_error: 229.8104\n",
      "Epoch 237/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.0172 - mean_squared_error: 233.0172\n",
      "Epoch 238/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.9544 - mean_squared_error: 230.9544\n",
      "Epoch 239/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 231.5669 - mean_squared_error: 231.5669\n",
      "Epoch 240/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.2616 - mean_squared_error: 233.2616\n",
      "Epoch 241/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.0275 - mean_squared_error: 229.0275\n",
      "Epoch 242/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 227.3901 - mean_squared_error: 227.3901\n",
      "Epoch 243/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 228.9475 - mean_squared_error: 228.9475\n",
      "Epoch 244/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.0250 - mean_squared_error: 231.0250\n",
      "Epoch 245/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.0964 - mean_squared_error: 229.0964\n",
      "Epoch 246/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.8410 - mean_squared_error: 229.8410\n",
      "Epoch 247/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 230.4452 - mean_squared_error: 230.4452\n",
      "Epoch 248/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.1163 - mean_squared_error: 234.1163\n",
      "Epoch 249/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.9446 - mean_squared_error: 231.9446\n",
      "Epoch 250/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.4635 - mean_squared_error: 236.4635\n"
=======
      "17731/17731 [==============================] - 2s 113us/step - loss: 6348.6700 - mean_squared_error: 6348.6700\n",
      "Epoch 2/250\n",
      "17731/17731 [==============================] - 1s 65us/step - loss: 5689.9737 - mean_squared_error: 5689.9737\n",
      "Epoch 3/250\n",
      "17731/17731 [==============================] - 1s 60us/step - loss: 5269.0864 - mean_squared_error: 5269.0864\n",
      "Epoch 4/250\n",
      "17731/17731 [==============================] - 1s 59us/step - loss: 4888.3642 - mean_squared_error: 4888.3642\n",
      "Epoch 5/250\n",
      "17731/17731 [==============================] - 1s 60us/step - loss: 4541.6081 - mean_squared_error: 4541.6081\n",
      "Epoch 6/250\n",
      "17731/17731 [==============================] - 1s 61us/step - loss: 4225.4339 - mean_squared_error: 4225.4339\n",
      "Epoch 7/250\n",
      "17731/17731 [==============================] - 1s 65us/step - loss: 3949.3328 - mean_squared_error: 3949.3328\n",
      "Epoch 8/250\n",
      "17731/17731 [==============================] - 1s 81us/step - loss: 3685.8940 - mean_squared_error: 3685.8940\n",
      "Epoch 9/250\n",
      "17731/17731 [==============================] - 1s 74us/step - loss: 3454.0966 - mean_squared_error: 3454.0966\n",
      "Epoch 10/250\n",
      "17731/17731 [==============================] - 2s 120us/step - loss: 3244.0147 - mean_squared_error: 3244.0147\n",
      "Epoch 11/250\n",
      "17731/17731 [==============================] - 2s 87us/step - loss: 3051.3742 - mean_squared_error: 3051.3742\n",
      "Epoch 12/250\n",
      "17731/17731 [==============================] - 1s 75us/step - loss: 2884.1305 - mean_squared_error: 2884.1305\n",
      "Epoch 13/250\n",
      "17731/17731 [==============================] - 2s 88us/step - loss: 2737.8150 - mean_squared_error: 2737.8150\n",
      "Epoch 14/250\n",
      "17731/17731 [==============================] - 1s 69us/step - loss: 2609.5638 - mean_squared_error: 2609.5638\n",
      "Epoch 15/250\n",
      "17731/17731 [==============================] - 1s 72us/step - loss: 2488.2357 - mean_squared_error: 2488.2357\n",
      "Epoch 16/250\n",
      "17731/17731 [==============================] - 2s 135us/step - loss: 2386.0040 - mean_squared_error: 2386.0040\n",
      "Epoch 17/250\n",
      "11264/17731 [==================>...........] - ETA: 0s - loss: 2320.8667 - mean_squared_error: 2320.8667"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-78319902f516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Train the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodelRUL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlrate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mendTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "opt = Adam(lr=0, beta_1=0.5)\n",
    "\n",
    "#Create the model\n",
    "modelRUL = RULmodel(X_train.shape[1])\n",
    "\n",
    "#Compile the model.\n",
    "modelRUL.compile(optimizer = opt, loss = \"mean_squared_error\", metrics = [\"mse\"])\n",
    "\n",
    "startTime = time.clock()\n",
    "#Train the model.\n",
    "modelRUL.fit(x = X_train, y = y_train, epochs = 250, batch_size = 512, callbacks=[lrate])  \n",
    "endTime = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 290us/step\n",
      "Root Square Mean Error score: 14.195613320521813\n",
      "Health score: [507.31751693]\n",
      "Elapsed time: 41.29436907207547\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "score = modelRUL.evaluate(x = X_test, y = y_test)\n",
    "y_pred = modelRUL.predict(X_test)\n",
    "healtScore = CMAPSAuxFunctions.compute_health_score(y_test, y_pred)\n",
    "\n",
    "print(\"Root Square Mean Error score: {}\".format(np.sqrt(score[0])))\n",
    "print(\"Health score: {}\".format(healtScore))\n",
    "print(\"Elapsed time: {}\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model\n",
    "Fit the Keras model to the data using a CNN and determine its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, math, random, pickle, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler,EarlyStopping\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.pooling import AveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Activation, Input, merge, Conv2D, Reshape, Flatten, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import CMAPSAuxFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FeatureN = 14\n",
    "nb_epoch = 250\n",
    "batch_size = 512\n",
    "FilterN = 10\n",
    "FilterL = 10\n",
    "rmse,sco,tm = [], [], []\n",
    "\n",
    "ConstRUL = 125\n",
    "TW = 30\n",
    "Dataset = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reshape data to fit a convNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "(17731, 30, 14)\n",
      "(17731,)\n",
      "Testing data\n",
      "(100, 30, 14)\n",
      "(100,)\n",
      "Training data\n",
      "[[[ 0.42168675  0.12579028  0.17049291 ...  0.33333333 -0.27131783\n",
      "   -0.07953604]\n",
      "  [ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.27131783\n",
      "    0.01712234]\n",
      "  [-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  ...\n",
      "  [ 0.51204819  0.14453891  0.52464551 ...  0.         -0.62790698\n",
      "   -0.34217067]\n",
      "  [ 0.3253012   0.26444299  0.67623228 ...  0.         -1.\n",
      "   -0.17674676]\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]]\n",
      "\n",
      " [[ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.27131783\n",
      "    0.01712234]\n",
      "  [-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  [-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  ...\n",
      "  [ 0.3253012   0.26444299  0.67623228 ...  0.         -1.\n",
      "   -0.17674676]\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]]\n",
      "\n",
      " [[-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  [-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  ...\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]]\n",
      "\n",
      " [[-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  [ 0.06024096  0.36516242  0.4409183  ...  0.         -0.45736434\n",
      "   -0.17702292]\n",
      "  ...\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]\n",
      "  [ 0.21686747  0.49204273  0.49493585 ...  0.16666667 -0.76744186\n",
      "   -0.53106877]]\n",
      "\n",
      " [[ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  [ 0.06024096  0.36516242  0.4409183  ...  0.         -0.45736434\n",
      "   -0.17702292]\n",
      "  [ 0.31927711  0.06780031  0.22822417 ... -0.16666667 -0.56589147\n",
      "   -0.21264844]\n",
      "  ...\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]\n",
      "  [ 0.21686747  0.49204273  0.49493585 ...  0.16666667 -0.76744186\n",
      "   -0.53106877]\n",
      "  [ 0.59036145  0.2792675   0.68433491 ...  0.33333333 -0.64341085\n",
      "   -0.56365645]]]\n",
      "[4 3 2 1 0]\n",
      "Testing data\n",
      "[[[-0.55421687 -0.61107478 -0.55232951 ... -0.33333333  0.39534884\n",
      "    0.55426678]\n",
      "  [-0.65060241 -0.17680401 -0.75151924 ... -0.33333333  0.28682171\n",
      "    0.14526374]\n",
      "  [-0.55421687 -0.58098975 -0.41458474 ... -0.33333333  0.41085271\n",
      "    0.341066  ]\n",
      "  ...\n",
      "  [-0.46987952  0.10660562 -0.28257934 ... -0.16666667  0.34883721\n",
      "    0.38663353]\n",
      "  [-0.24698795 -0.3381295  -0.24004051 ... -0.16666667  0.51937984\n",
      "    0.11875173]\n",
      "  [-0.34337349 -0.13494659 -0.47029034 ... -0.5         0.27131783\n",
      "    0.56420878]]\n",
      "\n",
      " [[-0.02409639 -0.30978853 -0.32343011 ... -0.33333333  0.37984496\n",
      "    0.25158796]\n",
      "  [-0.1626506  -0.29016787 -0.23869007 ... -0.16666667  0.27131783\n",
      "    0.31593482]\n",
      "  [-0.39156627 -0.29627207 -0.36664416 ... -0.16666667  0.25581395\n",
      "    0.3985087 ]\n",
      "  ...\n",
      "  [-0.40361446 -0.21342926 -0.32039163 ... -0.16666667  0.37984496\n",
      "    0.31759183]\n",
      "  [-0.39156627 -0.39481142 -0.26772451 ... -0.33333333  0.06976744\n",
      "    0.50483292]\n",
      "  [-0.1686747  -0.48027033 -0.03207292 ...  0.16666667 -0.27131783\n",
      "    0.10770505]]\n",
      "\n",
      " [[-0.15662651 -0.05777196 -0.29642134 ... -0.16666667  0.17829457\n",
      "    0.11543772]\n",
      "  [-0.24698795 -0.23348594 -0.11006077 ... -0.16666667  0.1627907\n",
      "    0.1248274 ]\n",
      "  [-0.18072289 -0.24177022 -0.04051317 ... -0.33333333  0.2248062\n",
      "    0.34962717]\n",
      "  ...\n",
      "  [ 0.12048193 -0.27883148  0.05773126 ...  0.16666667 -0.10077519\n",
      "    0.02734051]\n",
      "  [ 0.01204819 -0.03727927 -0.28899392 ...  0.         -0.02325581\n",
      "    0.00303783]\n",
      "  [-0.11445783  0.24133421  0.1215395  ...  0.         -0.03875969\n",
      "    0.28859431]]\n",
      "\n",
      " [[-0.40963855 -0.19032047 -0.56617151 ... -0.66666667  0.42635659\n",
      "    0.32836233]\n",
      "  [-0.62048193 -0.57096141 -0.67555706 ... -0.66666667  0.51937984\n",
      "    0.4084507 ]\n",
      "  [-0.42771084 -0.36908655 -0.486158   ... -0.5         0.95348837\n",
      "    0.48715824]\n",
      "  ...\n",
      "  [-0.04216867 -0.5382603  -0.49729912 ... -0.5         0.44186047\n",
      "    0.08091687]\n",
      "  [-0.3253012  -0.46152169 -0.28865631 ... -0.66666667  0.28682171\n",
      "    0.54045844]\n",
      "  [-0.52409639 -0.39001526 -0.46893991 ... -0.5         0.25581395\n",
      "    0.28500414]]\n",
      "\n",
      " [[-0.30722892 -0.11881404 -0.1144497  ... -0.33333333  0.06976744\n",
      "    0.1996686 ]\n",
      "  [-0.10240964 -0.17680401 -0.13301823 ... -0.16666667  0.14728682\n",
      "    0.11516156]\n",
      "  [-0.02409639 -0.31981687 -0.02295746 ... -0.33333333  0.08527132\n",
      "   -0.12703673]\n",
      "  ...\n",
      "  [ 0.34337349 -0.03597122 -0.17049291 ...  0.16666667 -0.25581395\n",
      "   -0.1413974 ]\n",
      "  [ 0.23493976  0.0442555   0.25286968 ...  0.16666667 -0.19379845\n",
      "    0.03755869]\n",
      "  [ 0.04819277  0.33333333  0.44294396 ...  0.33333333 -0.13178295\n",
      "   -0.1955261 ]]]\n",
      "[125  82  59 117  20]\n"
     ]
    }
   ],
   "source": [
    "'''samples = np.reshape(X_train, newshape=(X_train.shape[0], int(X_train.shape[1]/nFeatures), nFeatures))\n",
    "samplet = np.reshape(X_test, newshape=(X_test.shape[0], int(X_test.shape[1]/nFeatures), nFeatures))\n",
    "targets = y_train\n",
    "labelt = y_test'''\n",
    "\n",
    "samples, targets, samplet, labelt = CMAPSAuxFunctions.get_X_y_from_Dataset('1', '../CMAPSSData/', constRUL, time_window)\n",
    "\n",
    "print(\"Training data\")\n",
    "print(samples.shape)\n",
    "print(targets.shape)\n",
    "print(\"Testing data\")\n",
    "print(samplet.shape)\n",
    "print(labelt.shape)\n",
    "print(\"Training data\")\n",
    "print(samples[-5:,:])\n",
    "print(targets[-5:])\n",
    "print(\"Testing data\")\n",
    "print(samplet[-5:,:])\n",
    "print(labelt[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model\n",
    "\n",
    "CNN model. The model is Dense(ReLU, 100)->Dense(ReLu, 100)->Dense(Linear, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RULCNNModel(TW, FeatureN):\n",
    "    \n",
    "    input_layer = Input(shape=(TW, FeatureN))\n",
    "    y = Reshape((TW, FeatureN, 1), input_shape=(TW, FeatureN, ),name = 'Reshape')(input_layer)\n",
    "\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C1')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C2')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C3')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C4')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C5')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C6')(y)\n",
    "    \n",
    "    y = Conv2D(1, 3, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='Clast')(y)  \n",
    "    \n",
    "    y = Reshape((TW,14))(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    \n",
    "    #y = Dense(100, activation='tanh', init='glorot_normal', activity_regularizer=keras.regularizers.l2(0.01),)(y)\n",
    "    y = Dense(100,activation='tanh', kernel_initializer='glorot_normal', name='fc')(y)\n",
    "    y = Dense(1)(y)\n",
    "    \n",
    "    model = Model(inputs = input_layer, outputs = y, name='RUL_CNN_Model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the keras model\n",
    "\n",
    "Fit the Keras model to the data and determine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), padding=\"same\", name=\"C1\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  \n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), padding=\"same\", name=\"C2\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  import sys\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), padding=\"same\", name=\"C3\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  \n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), padding=\"same\", name=\"C4\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (3, 1), padding=\"same\", name=\"Clast\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17731 samples, validate on 100 samples\n",
      "Epoch 1/250\n",
      "17731/17731 [==============================] - 1s 70us/step - loss: 6498.8944 - val_loss: 4993.4146\n",
      "Epoch 2/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 5660.3737 - val_loss: 4571.3345\n",
      "Epoch 3/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 5215.3743 - val_loss: 4199.5576\n",
      "Epoch 4/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 4815.2465 - val_loss: 3860.5178\n",
      "Epoch 5/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4448.4811 - val_loss: 3549.6030\n",
      "Epoch 6/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 4110.1514 - val_loss: 3260.2922\n",
      "Epoch 7/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3797.6195 - val_loss: 2995.1016\n",
      "Epoch 8/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 3507.4581 - val_loss: 2745.6318\n",
      "Epoch 9/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3238.1227 - val_loss: 2515.3210\n",
      "Epoch 10/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2989.0717 - val_loss: 2304.9712\n",
      "Epoch 11/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2759.2463 - val_loss: 2100.0779\n",
      "Epoch 12/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 2548.1877 - val_loss: 1918.5565\n",
      "Epoch 13/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 2353.5785 - val_loss: 1809.1090\n",
      "Epoch 14/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 2176.3849 - val_loss: 1608.5559\n",
      "Epoch 15/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 2006.6118 - val_loss: 1464.3606\n",
      "Epoch 16/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1852.0604 - val_loss: 1347.0189\n",
      "Epoch 17/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1711.9574 - val_loss: 1224.3226\n",
      "Epoch 18/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1586.4496 - val_loss: 1119.9308\n",
      "Epoch 19/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 1465.3301 - val_loss: 1031.3207\n",
      "Epoch 20/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 1351.6995 - val_loss: 934.2416\n",
      "Epoch 21/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1249.3415 - val_loss: 852.5265\n",
      "Epoch 22/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1168.2534 - val_loss: 783.8724\n",
      "Epoch 23/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 1069.7237 - val_loss: 727.8953\n",
      "Epoch 24/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 998.0033 - val_loss: 665.2661\n",
      "Epoch 25/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 931.7317 - val_loss: 604.8176\n",
      "Epoch 26/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 864.6463 - val_loss: 553.7521\n",
      "Epoch 27/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 819.7415 - val_loss: 565.8857\n",
      "Epoch 28/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 752.5281 - val_loss: 485.7041\n",
      "Epoch 29/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 697.0973 - val_loss: 474.7287\n",
      "Epoch 30/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 655.6706 - val_loss: 473.7102\n",
      "Epoch 31/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 617.3182 - val_loss: 377.6540\n",
      "Epoch 32/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 572.2137 - val_loss: 358.3694\n",
      "Epoch 33/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 548.1615 - val_loss: 352.5532\n",
      "Epoch 34/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 511.5056 - val_loss: 309.7088\n",
      "Epoch 35/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 486.6145 - val_loss: 287.5079\n",
      "Epoch 36/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 478.5970 - val_loss: 290.5173\n",
      "Epoch 37/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 441.3492 - val_loss: 274.7759\n",
      "Epoch 38/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 405.7026 - val_loss: 246.4678\n",
      "Epoch 39/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 393.5840 - val_loss: 256.2430\n",
      "Epoch 40/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 376.6621 - val_loss: 234.2642\n",
      "Epoch 41/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 352.4065 - val_loss: 230.1924\n",
      "Epoch 42/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 353.0114 - val_loss: 205.8289\n",
      "Epoch 43/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 333.5447 - val_loss: 215.3065\n",
      "Epoch 44/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 319.8214 - val_loss: 201.9352\n",
      "Epoch 45/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 312.2370 - val_loss: 194.3019\n",
      "Epoch 46/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 296.9722 - val_loss: 209.2463\n",
      "Epoch 47/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 288.1498 - val_loss: 201.7367\n",
      "Epoch 48/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 290.5985 - val_loss: 193.9563\n",
      "Epoch 49/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 265.4651 - val_loss: 196.5190\n",
      "Epoch 50/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 281.9886 - val_loss: 191.4252\n",
      "Epoch 51/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 258.0009 - val_loss: 161.0669\n",
      "Epoch 52/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 259.7428 - val_loss: 248.4145\n",
      "Epoch 53/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 253.1349 - val_loss: 161.5856\n",
      "Epoch 54/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 234.4647 - val_loss: 162.3658\n",
      "Epoch 55/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 249.1887 - val_loss: 160.4021\n",
      "Epoch 56/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 227.6829 - val_loss: 159.6138\n",
      "Epoch 57/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 225.7310 - val_loss: 160.7817\n",
      "Epoch 58/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 221.3810 - val_loss: 171.8839\n",
      "Epoch 59/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 231.1298 - val_loss: 163.8362\n",
      "Epoch 60/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 218.6234 - val_loss: 161.1915\n",
      "Epoch 61/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 204.7979 - val_loss: 155.0924\n",
      "Epoch 62/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 206.9786 - val_loss: 241.4003\n",
      "Epoch 63/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 207.6643 - val_loss: 289.7419\n",
      "Epoch 64/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 198.8481 - val_loss: 163.0110\n",
      "Epoch 65/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 214.9866 - val_loss: 163.7525\n",
      "Epoch 66/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 194.3202 - val_loss: 162.7689\n",
      "Epoch 67/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 214.3599 - val_loss: 173.1570\n",
      "Epoch 68/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 192.3204 - val_loss: 156.9183\n",
      "Epoch 69/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 191.4514 - val_loss: 190.1748\n",
      "Epoch 70/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 180.9660 - val_loss: 155.3956\n",
      "Epoch 71/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 191.5446 - val_loss: 252.8201\n",
      "Epoch 72/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 202.8824 - val_loss: 159.7746\n",
      "Epoch 73/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 178.1730 - val_loss: 164.9516\n",
      "Epoch 74/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 21us/step - loss: 176.4628 - val_loss: 177.9437\n",
      "Epoch 75/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 177.5512 - val_loss: 163.5049\n",
      "Epoch 76/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 175.3709 - val_loss: 155.9617\n",
      "Epoch 77/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 176.9591 - val_loss: 187.3227\n",
      "Epoch 78/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 181.9026 - val_loss: 161.3189\n",
      "Epoch 79/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 170.8182 - val_loss: 161.4221\n",
      "Epoch 80/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 175.7751 - val_loss: 157.8793\n",
      "Epoch 81/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.5102 - val_loss: 197.4972\n",
      "Epoch 82/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.7735 - val_loss: 215.8596\n",
      "Epoch 83/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.4930 - val_loss: 161.8403\n",
      "Epoch 84/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 161.3282 - val_loss: 161.6910\n",
      "Epoch 85/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 176.7908 - val_loss: 162.8488\n",
      "Epoch 86/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 161.0900 - val_loss: 216.8416\n",
      "Epoch 87/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 158.3729 - val_loss: 159.4641\n",
      "Epoch 88/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 214.9070 - val_loss: 459.1817\n",
      "Epoch 89/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 271.5588 - val_loss: 189.4802\n",
      "Epoch 90/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 175.7038 - val_loss: 167.1733\n",
      "Epoch 91/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 167.8238 - val_loss: 160.1082\n",
      "Epoch 92/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 158.9479 - val_loss: 173.6599\n",
      "Epoch 93/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.3153 - val_loss: 186.5360\n",
      "Epoch 94/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.6799 - val_loss: 178.3916\n",
      "Epoch 95/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.3085 - val_loss: 173.9478\n",
      "Epoch 96/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.8413 - val_loss: 163.7744\n",
      "Epoch 97/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 152.5101 - val_loss: 178.0941\n",
      "Epoch 98/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.0274 - val_loss: 194.5865\n",
      "Epoch 99/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 154.2072 - val_loss: 196.4799\n",
      "Epoch 100/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 153.7571 - val_loss: 160.6333\n",
      "Epoch 101/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 158.8678 - val_loss: 211.0648\n",
      "Epoch 102/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.5713 - val_loss: 161.8617\n",
      "Epoch 103/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 155.2504 - val_loss: 171.5679\n",
      "Epoch 104/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.3180 - val_loss: 173.7706\n",
      "Epoch 105/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.4440 - val_loss: 164.8022\n",
      "Epoch 106/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.0543 - val_loss: 170.0724\n",
      "Epoch 107/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 150.6315 - val_loss: 194.9208\n",
      "Epoch 108/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.1307 - val_loss: 184.4988\n",
      "Epoch 109/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 154.9814 - val_loss: 162.7626\n",
      "Epoch 110/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 151.0622 - val_loss: 205.8173\n",
      "Epoch 111/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.3529 - val_loss: 270.2652\n",
      "Epoch 112/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.7138 - val_loss: 185.2700\n",
      "Epoch 113/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.0376 - val_loss: 177.6308\n",
      "Epoch 114/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.2909 - val_loss: 184.8597\n",
      "Epoch 115/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 145.7102 - val_loss: 215.4946\n",
      "Epoch 116/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 152.9403 - val_loss: 264.3490\n",
      "Epoch 117/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.2337 - val_loss: 210.0395\n",
      "Epoch 118/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 144.6300 - val_loss: 198.9496\n",
      "Epoch 119/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 151.6240 - val_loss: 193.2379\n",
      "Epoch 120/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.5490 - val_loss: 176.5355\n",
      "Epoch 121/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.0830 - val_loss: 167.3624\n",
      "Epoch 122/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.3942 - val_loss: 164.3991\n",
      "Epoch 123/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 140.5201 - val_loss: 181.8840\n",
      "Epoch 124/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.3650 - val_loss: 193.0988\n",
      "Epoch 125/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.7530 - val_loss: 183.4695\n",
      "Epoch 126/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.8459 - val_loss: 177.1052\n",
      "Epoch 127/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 145.5631 - val_loss: 192.0503\n",
      "Epoch 128/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.4449 - val_loss: 166.5275\n",
      "Epoch 129/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.8733 - val_loss: 179.3467\n",
      "Epoch 130/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.2242 - val_loss: 202.1258\n",
      "Epoch 131/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.4412 - val_loss: 159.2920\n",
      "Epoch 132/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 147.2372 - val_loss: 161.2755\n",
      "Epoch 133/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.9684 - val_loss: 179.8659\n",
      "Epoch 134/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.8963 - val_loss: 175.5465\n",
      "Epoch 135/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 139.5226 - val_loss: 238.6303\n",
      "Epoch 136/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.7683 - val_loss: 175.6145\n",
      "Epoch 137/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 137.6842 - val_loss: 169.4880\n",
      "Epoch 138/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.2503 - val_loss: 179.9208\n",
      "Epoch 139/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.3226 - val_loss: 170.4071\n",
      "Epoch 140/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 137.5772 - val_loss: 207.4164\n",
      "Epoch 141/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.0106 - val_loss: 180.0695\n",
      "Epoch 142/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 145.0861 - val_loss: 159.3787\n",
      "Epoch 143/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 140.3519 - val_loss: 169.1168\n",
      "Epoch 144/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.0022 - val_loss: 175.6567\n",
      "Epoch 145/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.1715 - val_loss: 182.5421\n",
      "Epoch 146/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 134.5158 - val_loss: 165.8373\n",
      "Epoch 147/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.6921 - val_loss: 259.9591\n",
      "Epoch 148/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.9208 - val_loss: 163.4086\n",
      "Epoch 149/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.7327 - val_loss: 187.9035\n",
      "Epoch 150/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.2649 - val_loss: 168.8831\n",
      "Epoch 151/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.9867 - val_loss: 159.7267\n",
      "Epoch 152/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 136.1017 - val_loss: 160.7465\n",
      "Epoch 153/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 130.5357 - val_loss: 175.1899\n",
      "Epoch 154/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 129.7412 - val_loss: 171.6227\n",
      "Epoch 155/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 147.3796 - val_loss: 179.9103\n",
      "Epoch 156/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 140.4855 - val_loss: 160.3140\n",
      "Epoch 157/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 132.0489 - val_loss: 229.1415\n",
      "Epoch 158/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.3999 - val_loss: 164.3804\n",
      "Epoch 159/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.0285 - val_loss: 199.6764\n",
      "Epoch 160/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 129.3851 - val_loss: 197.1805\n",
      "Epoch 161/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.2308 - val_loss: 163.2229\n",
      "Epoch 162/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.9462 - val_loss: 187.1183\n",
      "Epoch 163/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.2660 - val_loss: 202.9729\n",
      "Epoch 164/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 126.8108 - val_loss: 176.9245\n",
      "Epoch 165/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 129.5600 - val_loss: 244.2565\n",
      "Epoch 166/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.8939 - val_loss: 179.4406\n",
      "Epoch 167/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 126.8943 - val_loss: 172.9912\n",
      "Epoch 168/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 125.1996 - val_loss: 179.0470\n",
      "Epoch 169/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 131.4461 - val_loss: 219.9851\n",
      "Epoch 170/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.8586 - val_loss: 165.1535\n",
      "Epoch 171/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.4134 - val_loss: 173.5131\n",
      "Epoch 172/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 127.9984 - val_loss: 219.4653\n",
      "Epoch 173/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 128.9343 - val_loss: 196.1577\n",
      "Epoch 174/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 131.4219 - val_loss: 172.8679\n",
      "Epoch 175/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 124.0085 - val_loss: 168.3919\n",
      "Epoch 176/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 124.9738 - val_loss: 194.1241\n",
      "Epoch 177/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 123.1624 - val_loss: 176.1333\n",
      "Epoch 178/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.3284 - val_loss: 168.0725\n",
      "Epoch 179/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 127.2658 - val_loss: 166.5820\n",
      "Epoch 180/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 123.2425 - val_loss: 168.2344\n",
      "Epoch 181/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 123.3481 - val_loss: 171.6373\n",
      "Epoch 182/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 125.6596 - val_loss: 175.3699\n",
      "Epoch 183/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 123.2881 - val_loss: 171.8478\n",
      "Epoch 184/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 124.6681 - val_loss: 179.6847\n",
      "Epoch 185/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 125.2302 - val_loss: 166.1038\n",
      "Epoch 186/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 124.8565 - val_loss: 164.1115\n",
      "Epoch 187/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.0290 - val_loss: 163.5811\n",
      "Epoch 188/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 123.1318 - val_loss: 163.7523\n",
      "Epoch 189/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 122.6699 - val_loss: 169.8297\n",
      "Epoch 190/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 124.1730 - val_loss: 197.9035\n",
      "Epoch 191/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 126.4262 - val_loss: 230.2008\n",
      "Epoch 192/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 122.8829 - val_loss: 209.6979\n",
      "Epoch 193/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.8578 - val_loss: 183.6761\n",
      "Epoch 194/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 280.8534 - val_loss: 308.3488\n",
      "Epoch 195/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 225.1551 - val_loss: 405.4654\n",
      "Epoch 196/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 177.0391 - val_loss: 233.8486\n",
      "Epoch 197/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.3497 - val_loss: 189.4942\n",
      "Epoch 198/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.1671 - val_loss: 197.2684\n",
      "Epoch 199/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 128.9923 - val_loss: 158.1483\n",
      "Epoch 200/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.2196 - val_loss: 165.5860\n",
      "Epoch 201/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 122.3655 - val_loss: 176.7631\n",
      "Epoch 202/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.5630 - val_loss: 179.0820\n",
      "Epoch 203/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.4443 - val_loss: 177.4205\n",
      "Epoch 204/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.0593 - val_loss: 181.6035\n",
      "Epoch 205/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.4548 - val_loss: 177.8966\n",
      "Epoch 206/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.5410 - val_loss: 183.0504\n",
      "Epoch 207/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.1380 - val_loss: 177.6941\n",
      "Epoch 208/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 117.3419 - val_loss: 177.0888\n",
      "Epoch 209/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.6187 - val_loss: 177.2844\n",
      "Epoch 210/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.6662 - val_loss: 172.5277\n",
      "Epoch 211/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.0986 - val_loss: 178.6764\n",
      "Epoch 212/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.4422 - val_loss: 175.4576\n",
      "Epoch 213/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.1018 - val_loss: 178.6969\n",
      "Epoch 214/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.2551 - val_loss: 183.3944\n",
      "Epoch 215/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.7528 - val_loss: 180.0023\n",
      "Epoch 216/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.7192 - val_loss: 174.1487\n",
      "Epoch 217/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 117.3468 - val_loss: 185.1045\n",
      "Epoch 218/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 117.2470 - val_loss: 174.2532\n",
      "Epoch 219/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.0284 - val_loss: 174.6288\n",
      "Epoch 220/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.0474 - val_loss: 176.6487\n",
      "Epoch 221/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.8628 - val_loss: 183.2626\n",
      "Epoch 222/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.1314 - val_loss: 188.4823\n",
      "Epoch 223/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.4579 - val_loss: 176.6675\n",
      "Epoch 224/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.3495 - val_loss: 179.1382\n",
      "Epoch 225/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 115.7039 - val_loss: 172.0908\n",
      "Epoch 226/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.7163 - val_loss: 181.9718\n",
      "Epoch 227/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.7217 - val_loss: 181.0440\n",
      "Epoch 228/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.2869 - val_loss: 181.9102\n",
      "Epoch 229/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 114.9650 - val_loss: 176.8041\n",
      "Epoch 230/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.3522 - val_loss: 180.8594\n",
      "Epoch 231/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.1082 - val_loss: 180.2296\n",
      "Epoch 232/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 115.6561 - val_loss: 179.2737\n",
      "Epoch 233/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.3166 - val_loss: 180.3728\n",
      "Epoch 234/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 115.2830 - val_loss: 173.6376\n",
      "Epoch 235/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.5508 - val_loss: 180.1040\n",
      "Epoch 236/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.0453 - val_loss: 170.3973\n",
      "Epoch 237/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.7903 - val_loss: 171.6812\n",
      "Epoch 238/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.0897 - val_loss: 176.4780\n",
      "Epoch 239/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.1170 - val_loss: 184.3373\n",
      "Epoch 240/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.2036 - val_loss: 169.7641\n",
      "Epoch 241/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 114.9718 - val_loss: 176.7923\n",
      "Epoch 242/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 115.4581 - val_loss: 177.4392\n",
      "Epoch 243/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 114.5875 - val_loss: 180.5259\n",
      "Epoch 244/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.1571 - val_loss: 179.4079\n",
      "Epoch 245/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.4314 - val_loss: 181.8397\n",
      "Epoch 246/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 114.7556 - val_loss: 182.1651\n",
      "Epoch 247/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 114.8741 - val_loss: 186.0041\n",
      "Epoch 248/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.2624 - val_loss: 181.0919\n",
      "Epoch 249/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.3397 - val_loss: 177.0071\n",
      "Epoch 250/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 114.3289 - val_loss: 173.2690\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=0, beta_1=0.5)\n",
    "#DCNN = Model([input_layer], [y])\n",
    "DCNN = RULCNNModel(TW, FeatureN)\n",
    "#DCNN.compile(loss=get_score,optimizer=opt)\n",
    "DCNN.compile(loss='mean_squared_error',optimizer=opt)\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "\n",
    "\n",
    "startTime = time.clock()\n",
    "history = DCNN.fit(samples, targets,nb_epoch=nb_epoch, batch_size=batch_size,verbose=1, \n",
    "                   validation_data=(samplet, labelt), callbacks=[lrate])\n",
    "endTime = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 211us/step\n",
      "Root Square Mean Error score: 13.1631671269127\n",
      "Health score: [296.13544]\n",
      "Elapsed time: 97.5915960797374\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "score = DCNN.evaluate(samplet, labelt)\n",
    "y_pred = DCNN.predict(samplet)\n",
    "healtScore = CMAPSAuxFunctions.compute_health_score(labelt, y_pred)\n",
    "\n",
    "print(\"Root Square Mean Error score: {}\".format(np.sqrt(score)))\n",
    "print(\"Health score: {}\".format(healtScore))\n",
    "print(\"Elapsed time: {}\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "engineUnit = 21\n",
    "\n",
    "X_test2, _ = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_test, selected_features, time_window, 'train', unit_Number=engineUnit)\n",
    "X_test2 = min_max_scaler.fit_transform(X_test2)\n",
    "\n",
    "samplet2 = np.reshape(X_test2, newshape=(X_test2.shape[0], int(X_test2.shape[1]/nFeatures), nFeatures))\n",
    "\n",
    "nnPred = modelRUL.predict(X_test2)\n",
    "cnnPred = DCNN.predict(samplet2)\n",
    "\n",
    "maxCycle = X_test2.shape[0]\n",
    "faultCycle = y_test[engineUnit-1]\n",
    "cycles = np.arange(maxCycle)\n",
    "rulArray = np.arange(faultCycle, maxCycle+faultCycle)\n",
    "rulArray[rulArray > constRUL] = constRUL\n",
    "rulArray = np.flipud(rulArray)\n",
    "\n",
    "#print(cycles)\n",
    "#print(rulArray)\n",
    "\n",
    "'''print(\"Testing data\")\n",
    "print(X_test2.shape)\n",
    "print(X_test2[-5:,:])\n",
    "print(nnPred)\n",
    "print(cnnPred)'''\n",
    "\n",
    "plotRUL(cycles, rulArray, nnPred, cnnPred, engineUnit)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 24,
=======
   "execution_count": 21,
>>>>>>> Stashed changes
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotRUL(cycles, rulArray, nnPred, cnnPred, engineUnit):\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(cycles, rulArray, 'bo-', label='RUL')\n",
    "    plt.plot(cycles, nnPred, 'go-', label='NN Pred')\n",
    "    plt.plot(cycles, cnnPred, 'ro-', label='CNN Pred')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time (Cycle)\")\n",
    "    plt.ylabel(\"RUL\")\n",
    "    plt.title(\"Test Engine Unit #{}\".format(engineUnit))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 30, 14)\n",
      "(17731,)\n",
      "(100, 30, 14)\n",
      "(100,)\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.6.3"
=======
   "version": "3.6.1"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
