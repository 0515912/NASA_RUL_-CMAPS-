{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Approach using MLP. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import CMAPSAuxFunctions\n",
    "#import plottingTools\n",
    "#from datetime import datetime\n",
    "#from sklearn.covariance import EllipticEnvelope\n",
    "#from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.model_selection import train_test_split, cross_validate\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#from dataManagement import DataManagerDamadics\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib notebook\n",
    "\n",
    "global constRUL\n",
    "\n",
    "constRUL = 125\n",
    "time_window = 30\n",
    "rul_vector = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and Reshape data\n",
    "\n",
    "Get the data from the text files, store it in a Pandas Dataframe and reshape it as appropiately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file_train = '../CMAPSSData/train_FD001.txt'\n",
    "data_file_test = '../CMAPSSData/test_FD001.txt'\n",
    "\n",
    "#min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "standardScaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "#Selected as per CNN paper\n",
    "selected_features = ['T24', 'T30', 'T50', 'P30', 'Nf', 'Nc', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'htBleed', 'W31', 'W32']\n",
    "\n",
    "nFeatures = len(selected_features)\n",
    "\n",
    "#Get the X and y matrices with the specified time window\n",
    "X_train, y_train = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_train, selected_features, time_window, 'train')\n",
    "X_test, _ = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_test, selected_features, time_window, 'test')\n",
    "y_test = np.loadtxt(\"../CMAPSSData/RUL_FD001.txt\")\n",
    "y_test = np.array([x if x < constRUL else constRUL for x in y_test])\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "\n",
    "X_train, y_train, X_test, y_test = CMAPSAuxFunctions.get_X_y_from_Dataset('1', '../CMAPSSData/', constRUL, time_window)\n",
    "X_train = np.reshape(X_train, newshape=(X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
    "X_test = np.reshape(X_test, newshape=(X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
    "y_train = np.reshape(y_train, newshape=(y_train.shape[0], 1))\n",
    "y_test = np.reshape(y_test, newshape=(y_test.shape[0], 1))\n",
    "\n",
    "\n",
    "#Standardize the data\n",
    "#X_train = min_max_scaler.fit_transform(X_train)\n",
    "#X_test = min_max_scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "(17731, 420)\n",
      "(17731, 1)\n",
      "Testing data\n",
      "(100, 420)\n",
      "(100, 1)\n",
      "Training data\n",
      "[[ 0.42168675  0.12579028  0.17049291 ...  0.5        -0.45736434\n",
      "  -0.78099972]\n",
      " [ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.75193798\n",
      "  -0.26760563]\n",
      " [-0.01204819 -0.02419882  0.47467927 ...  0.66666667 -0.53488372\n",
      "  -0.89201878]\n",
      " [-0.09638554 -0.02419882  0.53376097 ...  0.16666667 -0.76744186\n",
      "  -0.53106877]\n",
      " [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333 -0.64341085\n",
      "  -0.56365645]]\n",
      "[[4]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [0]]\n",
      "Testing data\n",
      "[[-0.55421687 -0.61107478 -0.55232951 ... -0.5         0.27131783\n",
      "   0.56420878]\n",
      " [-0.02409639 -0.30978853 -0.32343011 ...  0.16666667 -0.27131783\n",
      "   0.10770505]\n",
      " [-0.15662651 -0.05777196 -0.29642134 ...  0.         -0.03875969\n",
      "   0.28859431]\n",
      " [-0.40963855 -0.19032047 -0.56617151 ... -0.5         0.25581395\n",
      "   0.28500414]\n",
      " [-0.30722892 -0.11881404 -0.1144497  ...  0.33333333 -0.13178295\n",
      "  -0.1955261 ]]\n",
      "[[125]\n",
      " [ 82]\n",
      " [ 59]\n",
      " [117]\n",
      " [ 20]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"Testing data\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(\"Training data\")\n",
    "print(X_train[-5:,:])\n",
    "print(y_train[-5:,:])\n",
    "print(\"Testing data\")\n",
    "print(X_test[-5:,:])\n",
    "print(y_test[-5:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model\n",
    "\n",
    "We will use a very simple ANN for this example. The model is Dense(ReLU, 100)->Dense(ReLu, 100)->Dense(Linear, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RULmodel(input_shape):\n",
    "    \n",
    "    print(input_shape)\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(500, input_dim=input_shape, activation='tanh', kernel_initializer='glorot_normal', name='fc1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='tanh', kernel_initializer='glorot_normal', name='fc2'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(100, activation='tanh', name='fc3'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(10, activation='tanh', name='fc4'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    #create a placeholder for the input\n",
    "    #X_input = Input(shape=(input_shape))\n",
    "    \n",
    "    #Create the layers\n",
    "    #X = Dense(100, activation='relu', name='fc1')(X_input)\n",
    "    #X = Dense(100, activation='relu', name='fc2')(X)\n",
    "    #X = Dense(1, activation='linear', name='out')(X)\n",
    "    \n",
    "    # Create model. This creates the Keras model instance, you'll use this instance to train/test the model.\n",
    "    #model = Sequential(inputs = X_input, outputs = X, name='RUL')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the keras model\n",
    "Fit the Keras model to the data and determine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n",
      "Epoch 1/250\n",
      "17731/17731 [==============================] - 0s 13us/step - loss: 6335.1290 - mean_squared_error: 6335.1290\n",
      "Epoch 2/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 5675.1038 - mean_squared_error: 5675.1038\n",
      "Epoch 3/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 5244.6205 - mean_squared_error: 5244.6205\n",
      "Epoch 4/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 4851.1704 - mean_squared_error: 4851.1704\n",
      "Epoch 5/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 4490.4882 - mean_squared_error: 4490.4882\n",
      "Epoch 6/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 4151.0204 - mean_squared_error: 4151.0204\n",
      "Epoch 7/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3839.1931 - mean_squared_error: 3839.1931\n",
      "Epoch 8/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3545.9781 - mean_squared_error: 3545.9781\n",
      "Epoch 9/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3280.1871 - mean_squared_error: 3280.1871\n",
      "Epoch 10/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3036.7135 - mean_squared_error: 3036.7135\n",
      "Epoch 11/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2801.5128 - mean_squared_error: 2801.5128\n",
      "Epoch 12/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2594.8191 - mean_squared_error: 2594.8191\n",
      "Epoch 13/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2395.7986 - mean_squared_error: 2395.7986\n",
      "Epoch 14/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2220.1299 - mean_squared_error: 2220.1299: 0s - loss: 2271.2580 - mean_squared_error: 2271.\n",
      "Epoch 15/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2041.0942 - mean_squared_error: 2041.0942\n",
      "Epoch 16/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1896.2381 - mean_squared_error: 1896.2381\n",
      "Epoch 17/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1756.7298 - mean_squared_error: 1756.7298\n",
      "Epoch 18/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 1624.0311 - mean_squared_error: 1624.0311\n",
      "Epoch 19/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 1506.4642 - mean_squared_error: 1506.4642\n",
      "Epoch 20/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1404.9985 - mean_squared_error: 1404.9985\n",
      "Epoch 21/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1299.8259 - mean_squared_error: 1299.8259\n",
      "Epoch 22/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 1206.4207 - mean_squared_error: 1206.4207\n",
      "Epoch 23/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1126.8711 - mean_squared_error: 1126.8711\n",
      "Epoch 24/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1052.5638 - mean_squared_error: 1052.5638\n",
      "Epoch 25/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 979.6480 - mean_squared_error: 979.6480\n",
      "Epoch 26/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 910.8254 - mean_squared_error: 910.8254\n",
      "Epoch 27/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 855.3054 - mean_squared_error: 855.3054\n",
      "Epoch 28/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 802.6121 - mean_squared_error: 802.6121\n",
      "Epoch 29/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 757.5916 - mean_squared_error: 757.5916\n",
      "Epoch 30/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 714.7993 - mean_squared_error: 714.7993\n",
      "Epoch 31/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 674.1612 - mean_squared_error: 674.1612\n",
      "Epoch 32/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 639.6175 - mean_squared_error: 639.6175\n",
      "Epoch 33/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 601.6746 - mean_squared_error: 601.6746\n",
      "Epoch 34/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 572.4981 - mean_squared_error: 572.4981\n",
      "Epoch 35/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 543.0838 - mean_squared_error: 543.0838\n",
      "Epoch 36/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 524.8199 - mean_squared_error: 524.8199\n",
      "Epoch 37/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 500.9139 - mean_squared_error: 500.9139\n",
      "Epoch 38/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 478.4121 - mean_squared_error: 478.4121\n",
      "Epoch 39/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 461.5738 - mean_squared_error: 461.5738\n",
      "Epoch 40/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 444.9050 - mean_squared_error: 444.9050\n",
      "Epoch 41/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 434.4851 - mean_squared_error: 434.4851\n",
      "Epoch 42/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 416.3182 - mean_squared_error: 416.3182\n",
      "Epoch 43/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 406.2215 - mean_squared_error: 406.2215\n",
      "Epoch 44/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 388.8638 - mean_squared_error: 388.8638\n",
      "Epoch 45/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 387.7303 - mean_squared_error: 387.7303\n",
      "Epoch 46/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 372.7424 - mean_squared_error: 372.7424\n",
      "Epoch 47/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 366.0062 - mean_squared_error: 366.0062\n",
      "Epoch 48/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 352.9673 - mean_squared_error: 352.9673\n",
      "Epoch 49/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 357.5855 - mean_squared_error: 357.5855\n",
      "Epoch 50/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 346.0164 - mean_squared_error: 346.0164\n",
      "Epoch 51/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 345.8930 - mean_squared_error: 345.8930\n",
      "Epoch 52/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 341.9430 - mean_squared_error: 341.9430\n",
      "Epoch 53/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 333.5820 - mean_squared_error: 333.5820\n",
      "Epoch 54/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 327.1545 - mean_squared_error: 327.1545\n",
      "Epoch 55/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 328.3561 - mean_squared_error: 328.3561\n",
      "Epoch 56/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 320.2059 - mean_squared_error: 320.2059\n",
      "Epoch 57/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 314.3924 - mean_squared_error: 314.3924\n",
      "Epoch 58/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 318.3994 - mean_squared_error: 318.3994\n",
      "Epoch 59/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 309.3444 - mean_squared_error: 309.3444\n",
      "Epoch 60/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 307.1630 - mean_squared_error: 307.1630\n",
      "Epoch 61/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 306.4841 - mean_squared_error: 306.4841\n",
      "Epoch 62/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 304.4009 - mean_squared_error: 304.4009\n",
      "Epoch 63/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 303.4078 - mean_squared_error: 303.4078\n",
      "Epoch 64/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 298.8081 - mean_squared_error: 298.8081\n",
      "Epoch 65/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 297.4879 - mean_squared_error: 297.4879\n",
      "Epoch 66/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 295.6664 - mean_squared_error: 295.6664\n",
      "Epoch 67/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 301.2894 - mean_squared_error: 301.2894\n",
      "Epoch 68/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 294.0188 - mean_squared_error: 294.0188\n",
      "Epoch 69/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 292.2741 - mean_squared_error: 292.2741\n",
      "Epoch 70/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 290.1731 - mean_squared_error: 290.1731\n",
      "Epoch 71/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 291.1203 - mean_squared_error: 291.1203\n",
      "Epoch 72/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 294.1866 - mean_squared_error: 294.1866\n",
      "Epoch 73/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 288.9920 - mean_squared_error: 288.9920\n",
      "Epoch 74/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 283.0023 - mean_squared_error: 283.0023\n",
      "Epoch 75/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 294.6650 - mean_squared_error: 294.6650\n",
      "Epoch 76/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 283.6709 - mean_squared_error: 283.6709\n",
      "Epoch 77/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 286.3872 - mean_squared_error: 286.3872\n",
      "Epoch 78/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 286.2361 - mean_squared_error: 286.2361\n",
      "Epoch 79/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 280.6757 - mean_squared_error: 280.6757\n",
      "Epoch 80/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 278.5512 - mean_squared_error: 278.5512\n",
      "Epoch 81/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 287.5994 - mean_squared_error: 287.5994\n",
      "Epoch 82/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 277.4456 - mean_squared_error: 277.4456\n",
      "Epoch 83/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 274.6253 - mean_squared_error: 274.6253\n",
      "Epoch 84/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 275.1243 - mean_squared_error: 275.1243\n",
      "Epoch 85/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 274.6014 - mean_squared_error: 274.6014\n",
      "Epoch 86/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 276.0267 - mean_squared_error: 276.0267\n",
      "Epoch 87/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 276.1763 - mean_squared_error: 276.1763\n",
      "Epoch 88/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 277.5976 - mean_squared_error: 277.5976\n",
      "Epoch 89/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 275.0446 - mean_squared_error: 275.0446\n",
      "Epoch 90/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 273.5616 - mean_squared_error: 273.5616\n",
      "Epoch 91/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 274.9801 - mean_squared_error: 274.9801\n",
      "Epoch 92/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 271.3998 - mean_squared_error: 271.3998\n",
      "Epoch 93/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 273.2769 - mean_squared_error: 273.2769\n",
      "Epoch 94/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 270.1009 - mean_squared_error: 270.1009\n",
      "Epoch 95/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 268.1360 - mean_squared_error: 268.1360\n",
      "Epoch 96/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 273.3139 - mean_squared_error: 273.3139\n",
      "Epoch 97/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 269.4274 - mean_squared_error: 269.4274\n",
      "Epoch 98/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 275.7265 - mean_squared_error: 275.7265\n",
      "Epoch 99/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 263.1063 - mean_squared_error: 263.1063\n",
      "Epoch 100/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 266.3256 - mean_squared_error: 266.3256\n",
      "Epoch 101/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 268.9591 - mean_squared_error: 268.9591\n",
      "Epoch 102/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 273.2610 - mean_squared_error: 273.2610\n",
      "Epoch 103/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 271.0682 - mean_squared_error: 271.0682\n",
      "Epoch 104/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.3452 - mean_squared_error: 262.3452\n",
      "Epoch 105/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 264.1392 - mean_squared_error: 264.1392\n",
      "Epoch 106/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 265.6442 - mean_squared_error: 265.6442\n",
      "Epoch 107/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 268.2224 - mean_squared_error: 268.2224\n",
      "Epoch 108/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 264.8042 - mean_squared_error: 264.8042\n",
      "Epoch 109/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.8926 - mean_squared_error: 257.8926\n",
      "Epoch 110/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 266.6090 - mean_squared_error: 266.6090\n",
      "Epoch 111/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 263.5458 - mean_squared_error: 263.5458\n",
      "Epoch 112/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 260.8859 - mean_squared_error: 260.8859\n",
      "Epoch 113/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.1112 - mean_squared_error: 262.1112\n",
      "Epoch 114/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 265.3777 - mean_squared_error: 265.3777\n",
      "Epoch 115/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 260.1105 - mean_squared_error: 260.1105\n",
      "Epoch 116/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.4421 - mean_squared_error: 261.4421\n",
      "Epoch 117/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 263.1461 - mean_squared_error: 263.1461\n",
      "Epoch 118/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.7865 - mean_squared_error: 257.7865\n",
      "Epoch 119/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 264.0863 - mean_squared_error: 264.0863\n",
      "Epoch 120/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.7560 - mean_squared_error: 262.7560\n",
      "Epoch 121/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.5801 - mean_squared_error: 261.5801\n",
      "Epoch 122/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.1730 - mean_squared_error: 261.1730\n",
      "Epoch 123/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 260.5187 - mean_squared_error: 260.5187\n",
      "Epoch 124/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 263.8414 - mean_squared_error: 263.8414\n",
      "Epoch 125/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.8023 - mean_squared_error: 256.8023\n",
      "Epoch 126/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 258.1848 - mean_squared_error: 258.1848\n",
      "Epoch 127/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 259.6251 - mean_squared_error: 259.6251\n",
      "Epoch 128/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.3110 - mean_squared_error: 255.3110\n",
      "Epoch 129/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 264.2196 - mean_squared_error: 264.2196\n",
      "Epoch 130/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.5362 - mean_squared_error: 256.5362\n",
      "Epoch 131/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 253.8734 - mean_squared_error: 253.8734\n",
      "Epoch 132/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.4916 - mean_squared_error: 252.4916\n",
      "Epoch 133/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 259.5486 - mean_squared_error: 259.5486\n",
      "Epoch 134/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 260.8266 - mean_squared_error: 260.8266\n",
      "Epoch 135/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.6456 - mean_squared_error: 257.6456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 256.2809 - mean_squared_error: 256.2809\n",
      "Epoch 137/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 257.7277 - mean_squared_error: 257.7277\n",
      "Epoch 138/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 251.2366 - mean_squared_error: 251.2366\n",
      "Epoch 139/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 256.5770 - mean_squared_error: 256.5770\n",
      "Epoch 140/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.8790 - mean_squared_error: 256.8790\n",
      "Epoch 141/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.9468 - mean_squared_error: 256.9468\n",
      "Epoch 142/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.4369 - mean_squared_error: 254.4369\n",
      "Epoch 143/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.4954 - mean_squared_error: 256.4954\n",
      "Epoch 144/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.4923 - mean_squared_error: 250.4923\n",
      "Epoch 145/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.2821 - mean_squared_error: 255.2821\n",
      "Epoch 146/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 259.5073 - mean_squared_error: 259.5073\n",
      "Epoch 147/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.1356 - mean_squared_error: 255.1356\n",
      "Epoch 148/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.9505 - mean_squared_error: 256.9505\n",
      "Epoch 149/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.0774 - mean_squared_error: 254.0774\n",
      "Epoch 150/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.6844 - mean_squared_error: 252.6844\n",
      "Epoch 151/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.3602 - mean_squared_error: 256.3602\n",
      "Epoch 152/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 253.5643 - mean_squared_error: 253.5643\n",
      "Epoch 153/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.9158 - mean_squared_error: 250.9158\n",
      "Epoch 154/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.7309 - mean_squared_error: 249.7309\n",
      "Epoch 155/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.2037 - mean_squared_error: 252.2037\n",
      "Epoch 156/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 253.1075 - mean_squared_error: 253.1075\n",
      "Epoch 157/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.3383 - mean_squared_error: 252.3383\n",
      "Epoch 158/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.1468 - mean_squared_error: 252.1468\n",
      "Epoch 159/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 247.5826 - mean_squared_error: 247.5826\n",
      "Epoch 160/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.9678 - mean_squared_error: 252.9678\n",
      "Epoch 161/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 245.6312 - mean_squared_error: 245.6312\n",
      "Epoch 162/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 248.1394 - mean_squared_error: 248.1394\n",
      "Epoch 163/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.0075 - mean_squared_error: 246.0075\n",
      "Epoch 164/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.7370 - mean_squared_error: 248.7370\n",
      "Epoch 165/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.1111 - mean_squared_error: 250.1111\n",
      "Epoch 166/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 253.8635 - mean_squared_error: 253.8635\n",
      "Epoch 167/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 245.5495 - mean_squared_error: 245.5495\n",
      "Epoch 168/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 245.3121 - mean_squared_error: 245.3121\n",
      "Epoch 169/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.4422 - mean_squared_error: 247.4422\n",
      "Epoch 170/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.5593 - mean_squared_error: 248.5593\n",
      "Epoch 171/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.8951 - mean_squared_error: 249.8951\n",
      "Epoch 172/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.3323 - mean_squared_error: 249.3323\n",
      "Epoch 173/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.9251 - mean_squared_error: 248.9251\n",
      "Epoch 174/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.2380 - mean_squared_error: 248.2380\n",
      "Epoch 175/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 242.7631 - mean_squared_error: 242.7631\n",
      "Epoch 176/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.8137 - mean_squared_error: 249.8137\n",
      "Epoch 177/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.1867 - mean_squared_error: 250.1867\n",
      "Epoch 178/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 246.7431 - mean_squared_error: 246.7431\n",
      "Epoch 179/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 251.5101 - mean_squared_error: 251.5101\n",
      "Epoch 180/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.3788 - mean_squared_error: 247.3788\n",
      "Epoch 181/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.1532 - mean_squared_error: 241.1532\n",
      "Epoch 182/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.5299 - mean_squared_error: 241.5299\n",
      "Epoch 183/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.1041 - mean_squared_error: 246.1041\n",
      "Epoch 184/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 244.6009 - mean_squared_error: 244.6009\n",
      "Epoch 185/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 245.6425 - mean_squared_error: 245.6425\n",
      "Epoch 186/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 242.3501 - mean_squared_error: 242.3501\n",
      "Epoch 187/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.0723 - mean_squared_error: 246.0723\n",
      "Epoch 188/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 242.1040 - mean_squared_error: 242.1040\n",
      "Epoch 189/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 239.1061 - mean_squared_error: 239.1061\n",
      "Epoch 190/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 243.6883 - mean_squared_error: 243.6883\n",
      "Epoch 191/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 243.7246 - mean_squared_error: 243.7246\n",
      "Epoch 192/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 239.3924 - mean_squared_error: 239.3924\n",
      "Epoch 193/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 242.2814 - mean_squared_error: 242.2814\n",
      "Epoch 194/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.6635 - mean_squared_error: 241.6635\n",
      "Epoch 195/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 240.9501 - mean_squared_error: 240.9501\n",
      "Epoch 196/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.2376 - mean_squared_error: 241.2376\n",
      "Epoch 197/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.3312 - mean_squared_error: 247.3312\n",
      "Epoch 198/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 243.8621 - mean_squared_error: 243.8621\n",
      "Epoch 199/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 244.0094 - mean_squared_error: 244.0094\n",
      "Epoch 200/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.5723 - mean_squared_error: 241.5723\n",
      "Epoch 201/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.3645 - mean_squared_error: 241.3645\n",
      "Epoch 202/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.5667 - mean_squared_error: 236.5667\n",
      "Epoch 203/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.8416 - mean_squared_error: 232.8416\n",
      "Epoch 204/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.8266 - mean_squared_error: 236.8266\n",
      "Epoch 205/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.6247 - mean_squared_error: 236.6247\n",
      "Epoch 206/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.1539 - mean_squared_error: 233.1539\n",
      "Epoch 207/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 233.7690 - mean_squared_error: 233.7690\n",
      "Epoch 208/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.0701 - mean_squared_error: 234.0701\n",
      "Epoch 209/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.5106 - mean_squared_error: 229.5106\n",
      "Epoch 210/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 236.8967 - mean_squared_error: 236.8967\n",
      "Epoch 211/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 226.8716 - mean_squared_error: 226.8716\n",
      "Epoch 212/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 232.8274 - mean_squared_error: 232.8274\n",
      "Epoch 213/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 235.2260 - mean_squared_error: 235.2260\n",
      "Epoch 214/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.0826 - mean_squared_error: 236.0826\n",
      "Epoch 215/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 235.2770 - mean_squared_error: 235.2770\n",
      "Epoch 216/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.0707 - mean_squared_error: 231.0707\n",
      "Epoch 217/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.0595 - mean_squared_error: 232.0595\n",
      "Epoch 218/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.5635 - mean_squared_error: 228.5635\n",
      "Epoch 219/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 231.4713 - mean_squared_error: 231.4713\n",
      "Epoch 220/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.2077 - mean_squared_error: 228.2077\n",
      "Epoch 221/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.3001 - mean_squared_error: 229.3001\n",
      "Epoch 222/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 231.1774 - mean_squared_error: 231.1774\n",
      "Epoch 223/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.4091 - mean_squared_error: 233.4091\n",
      "Epoch 224/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.7774 - mean_squared_error: 231.7774\n",
      "Epoch 225/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.8717 - mean_squared_error: 230.8717\n",
      "Epoch 226/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.4150 - mean_squared_error: 228.4150\n",
      "Epoch 227/250\n",
      "17731/17731 [==============================] - 0s 11us/step - loss: 232.8015 - mean_squared_error: 232.8015\n",
      "Epoch 228/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.5124 - mean_squared_error: 232.5124\n",
      "Epoch 229/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.9387 - mean_squared_error: 231.9387\n",
      "Epoch 230/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.1123 - mean_squared_error: 233.1123\n",
      "Epoch 231/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.9448 - mean_squared_error: 230.9448\n",
      "Epoch 232/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.5435 - mean_squared_error: 228.5435\n",
      "Epoch 233/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 231.8919 - mean_squared_error: 231.8919\n",
      "Epoch 234/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 230.2916 - mean_squared_error: 230.2916\n",
      "Epoch 235/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.0990 - mean_squared_error: 233.0990\n",
      "Epoch 236/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.8104 - mean_squared_error: 229.8104\n",
      "Epoch 237/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.0172 - mean_squared_error: 233.0172\n",
      "Epoch 238/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.9544 - mean_squared_error: 230.9544\n",
      "Epoch 239/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 231.5669 - mean_squared_error: 231.5669\n",
      "Epoch 240/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.2616 - mean_squared_error: 233.2616\n",
      "Epoch 241/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.0275 - mean_squared_error: 229.0275\n",
      "Epoch 242/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 227.3901 - mean_squared_error: 227.3901\n",
      "Epoch 243/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 228.9475 - mean_squared_error: 228.9475\n",
      "Epoch 244/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.0250 - mean_squared_error: 231.0250\n",
      "Epoch 245/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.0964 - mean_squared_error: 229.0964\n",
      "Epoch 246/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.8410 - mean_squared_error: 229.8410\n",
      "Epoch 247/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 230.4452 - mean_squared_error: 230.4452\n",
      "Epoch 248/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.1163 - mean_squared_error: 234.1163\n",
      "Epoch 249/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.9446 - mean_squared_error: 231.9446\n",
      "Epoch 250/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.4635 - mean_squared_error: 236.4635\n"
     ]
    }
   ],
   "source": [
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "opt = Adam(lr=0, beta_1=0.5)\n",
    "\n",
    "#Create the model\n",
    "modelRUL = RULmodel(X_train.shape[1])\n",
    "\n",
    "#Compile the model.\n",
    "modelRUL.compile(optimizer = opt, loss = \"mean_squared_error\", metrics = [\"mse\"])\n",
    "\n",
    "startTime = time.clock()\n",
    "#Train the model.\n",
    "modelRUL.fit(x = X_train, y = y_train, epochs = 250, batch_size = 512, callbacks=[lrate])  \n",
    "endTime = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 290us/step\n",
      "Root Square Mean Error score: 14.195613320521813\n",
      "Health score: [507.31751693]\n",
      "Elapsed time: 41.29436907207547\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "score = modelRUL.evaluate(x = X_test, y = y_test)\n",
    "y_pred = modelRUL.predict(X_test)\n",
    "healtScore = CMAPSAuxFunctions.compute_health_score(y_test, y_pred)\n",
    "\n",
    "print(\"Root Square Mean Error score: {}\".format(np.sqrt(score[0])))\n",
    "print(\"Health score: {}\".format(healtScore))\n",
    "print(\"Elapsed time: {}\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model\n",
    "Fit the Keras model to the data using a CNN and determine its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, math, random, pickle, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler,EarlyStopping\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.pooling import AveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Activation, Input, merge, Conv2D, Reshape, Flatten, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import CMAPSAuxFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FeatureN = 14\n",
    "nb_epoch = 250\n",
    "batch_size = 512\n",
    "FilterN = 10\n",
    "FilterL = 10\n",
    "rmse,sco,tm = [], [], []\n",
    "\n",
    "ConstRUL = 125\n",
    "TW = 30\n",
    "Dataset = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reshape data to fit a convNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "(17731, 30, 14)\n",
      "(17731,)\n",
      "Testing data\n",
      "(100, 30, 14)\n",
      "(100,)\n",
      "Training data\n",
      "[[[ 0.42168675  0.12579028  0.17049291 ...  0.33333333 -0.27131783\n",
      "   -0.07953604]\n",
      "  [ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.27131783\n",
      "    0.01712234]\n",
      "  [-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  ...\n",
      "  [ 0.51204819  0.14453891  0.52464551 ...  0.         -0.62790698\n",
      "   -0.34217067]\n",
      "  [ 0.3253012   0.26444299  0.67623228 ...  0.         -1.\n",
      "   -0.17674676]\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]]\n",
      "\n",
      " [[ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.27131783\n",
      "    0.01712234]\n",
      "  [-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  [-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  ...\n",
      "  [ 0.3253012   0.26444299  0.67623228 ...  0.         -1.\n",
      "   -0.17674676]\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]]\n",
      "\n",
      " [[-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  [-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  ...\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]]\n",
      "\n",
      " [[-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  [ 0.06024096  0.36516242  0.4409183  ...  0.         -0.45736434\n",
      "   -0.17702292]\n",
      "  ...\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]\n",
      "  [ 0.21686747  0.49204273  0.49493585 ...  0.16666667 -0.76744186\n",
      "   -0.53106877]]\n",
      "\n",
      " [[ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  [ 0.06024096  0.36516242  0.4409183  ...  0.         -0.45736434\n",
      "   -0.17702292]\n",
      "  [ 0.31927711  0.06780031  0.22822417 ... -0.16666667 -0.56589147\n",
      "   -0.21264844]\n",
      "  ...\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]\n",
      "  [ 0.21686747  0.49204273  0.49493585 ...  0.16666667 -0.76744186\n",
      "   -0.53106877]\n",
      "  [ 0.59036145  0.2792675   0.68433491 ...  0.33333333 -0.64341085\n",
      "   -0.56365645]]]\n",
      "[4 3 2 1 0]\n",
      "Testing data\n",
      "[[[-0.55421687 -0.61107478 -0.55232951 ... -0.33333333  0.39534884\n",
      "    0.55426678]\n",
      "  [-0.65060241 -0.17680401 -0.75151924 ... -0.33333333  0.28682171\n",
      "    0.14526374]\n",
      "  [-0.55421687 -0.58098975 -0.41458474 ... -0.33333333  0.41085271\n",
      "    0.341066  ]\n",
      "  ...\n",
      "  [-0.46987952  0.10660562 -0.28257934 ... -0.16666667  0.34883721\n",
      "    0.38663353]\n",
      "  [-0.24698795 -0.3381295  -0.24004051 ... -0.16666667  0.51937984\n",
      "    0.11875173]\n",
      "  [-0.34337349 -0.13494659 -0.47029034 ... -0.5         0.27131783\n",
      "    0.56420878]]\n",
      "\n",
      " [[-0.02409639 -0.30978853 -0.32343011 ... -0.33333333  0.37984496\n",
      "    0.25158796]\n",
      "  [-0.1626506  -0.29016787 -0.23869007 ... -0.16666667  0.27131783\n",
      "    0.31593482]\n",
      "  [-0.39156627 -0.29627207 -0.36664416 ... -0.16666667  0.25581395\n",
      "    0.3985087 ]\n",
      "  ...\n",
      "  [-0.40361446 -0.21342926 -0.32039163 ... -0.16666667  0.37984496\n",
      "    0.31759183]\n",
      "  [-0.39156627 -0.39481142 -0.26772451 ... -0.33333333  0.06976744\n",
      "    0.50483292]\n",
      "  [-0.1686747  -0.48027033 -0.03207292 ...  0.16666667 -0.27131783\n",
      "    0.10770505]]\n",
      "\n",
      " [[-0.15662651 -0.05777196 -0.29642134 ... -0.16666667  0.17829457\n",
      "    0.11543772]\n",
      "  [-0.24698795 -0.23348594 -0.11006077 ... -0.16666667  0.1627907\n",
      "    0.1248274 ]\n",
      "  [-0.18072289 -0.24177022 -0.04051317 ... -0.33333333  0.2248062\n",
      "    0.34962717]\n",
      "  ...\n",
      "  [ 0.12048193 -0.27883148  0.05773126 ...  0.16666667 -0.10077519\n",
      "    0.02734051]\n",
      "  [ 0.01204819 -0.03727927 -0.28899392 ...  0.         -0.02325581\n",
      "    0.00303783]\n",
      "  [-0.11445783  0.24133421  0.1215395  ...  0.         -0.03875969\n",
      "    0.28859431]]\n",
      "\n",
      " [[-0.40963855 -0.19032047 -0.56617151 ... -0.66666667  0.42635659\n",
      "    0.32836233]\n",
      "  [-0.62048193 -0.57096141 -0.67555706 ... -0.66666667  0.51937984\n",
      "    0.4084507 ]\n",
      "  [-0.42771084 -0.36908655 -0.486158   ... -0.5         0.95348837\n",
      "    0.48715824]\n",
      "  ...\n",
      "  [-0.04216867 -0.5382603  -0.49729912 ... -0.5         0.44186047\n",
      "    0.08091687]\n",
      "  [-0.3253012  -0.46152169 -0.28865631 ... -0.66666667  0.28682171\n",
      "    0.54045844]\n",
      "  [-0.52409639 -0.39001526 -0.46893991 ... -0.5         0.25581395\n",
      "    0.28500414]]\n",
      "\n",
      " [[-0.30722892 -0.11881404 -0.1144497  ... -0.33333333  0.06976744\n",
      "    0.1996686 ]\n",
      "  [-0.10240964 -0.17680401 -0.13301823 ... -0.16666667  0.14728682\n",
      "    0.11516156]\n",
      "  [-0.02409639 -0.31981687 -0.02295746 ... -0.33333333  0.08527132\n",
      "   -0.12703673]\n",
      "  ...\n",
      "  [ 0.34337349 -0.03597122 -0.17049291 ...  0.16666667 -0.25581395\n",
      "   -0.1413974 ]\n",
      "  [ 0.23493976  0.0442555   0.25286968 ...  0.16666667 -0.19379845\n",
      "    0.03755869]\n",
      "  [ 0.04819277  0.33333333  0.44294396 ...  0.33333333 -0.13178295\n",
      "   -0.1955261 ]]]\n",
      "[125  82  59 117  20]\n"
     ]
    }
   ],
   "source": [
    "'''samples = np.reshape(X_train, newshape=(X_train.shape[0], int(X_train.shape[1]/nFeatures), nFeatures))\n",
    "samplet = np.reshape(X_test, newshape=(X_test.shape[0], int(X_test.shape[1]/nFeatures), nFeatures))\n",
    "targets = y_train\n",
    "labelt = y_test'''\n",
    "\n",
    "samples, targets, samplet, labelt = CMAPSAuxFunctions.get_X_y_from_Dataset('1', '../CMAPSSData/', constRUL, time_window)\n",
    "\n",
    "print(\"Training data\")\n",
    "print(samples.shape)\n",
    "print(targets.shape)\n",
    "print(\"Testing data\")\n",
    "print(samplet.shape)\n",
    "print(labelt.shape)\n",
    "print(\"Training data\")\n",
    "print(samples[-5:,:])\n",
    "print(targets[-5:])\n",
    "print(\"Testing data\")\n",
    "print(samplet[-5:,:])\n",
    "print(labelt[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model\n",
    "\n",
    "CNN model. The model is Dense(ReLU, 100)->Dense(ReLu, 100)->Dense(Linear, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RULCNNModel(TW, FeatureN):\n",
    "    \n",
    "    input_layer = Input(shape=(TW, FeatureN))\n",
    "    y = Reshape((TW, FeatureN, 1), input_shape=(TW, FeatureN, ),name = 'Reshape')(input_layer)\n",
    "\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C1')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C2')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C3')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C4')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C5')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C6')(y)\n",
    "    \n",
    "    y = Conv2D(1, 3, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='Clast')(y)  \n",
    "    \n",
    "    y = Reshape((TW,14))(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    \n",
    "    #y = Dense(100, activation='tanh', init='glorot_normal', activity_regularizer=keras.regularizers.l2(0.01),)(y)\n",
    "    y = Dense(100,activation='tanh', kernel_initializer='glorot_normal', name='fc')(y)\n",
    "    y = Dense(1)(y)\n",
    "    \n",
    "    model = Model(inputs = input_layer, outputs = y, name='RUL_CNN_Model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the keras model\n",
    "\n",
    "Fit the Keras model to the data and determine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), padding=\"same\", name=\"C1\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  \n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), padding=\"same\", name=\"C2\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  import sys\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), padding=\"same\", name=\"C3\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  \n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), padding=\"same\", name=\"C4\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (3, 1), padding=\"same\", name=\"Clast\", kernel_initializer=\"glorot_normal\", activation=\"tanh\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17731 samples, validate on 100 samples\n",
      "Epoch 1/250\n",
      "17731/17731 [==============================] - 1s 70us/step - loss: 6498.8944 - val_loss: 4993.4146\n",
      "Epoch 2/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 5660.3737 - val_loss: 4571.3345\n",
      "Epoch 3/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 5215.3743 - val_loss: 4199.5576\n",
      "Epoch 4/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 4815.2465 - val_loss: 3860.5178\n",
      "Epoch 5/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4448.4811 - val_loss: 3549.6030\n",
      "Epoch 6/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 4110.1514 - val_loss: 3260.2922\n",
      "Epoch 7/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3797.6195 - val_loss: 2995.1016\n",
      "Epoch 8/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 3507.4581 - val_loss: 2745.6318\n",
      "Epoch 9/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3238.1227 - val_loss: 2515.3210\n",
      "Epoch 10/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2989.0717 - val_loss: 2304.9712\n",
      "Epoch 11/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2759.2463 - val_loss: 2100.0779\n",
      "Epoch 12/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 2548.1877 - val_loss: 1918.5565\n",
      "Epoch 13/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 2353.5785 - val_loss: 1809.1090\n",
      "Epoch 14/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 2176.3849 - val_loss: 1608.5559\n",
      "Epoch 15/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 2006.6118 - val_loss: 1464.3606\n",
      "Epoch 16/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1852.0604 - val_loss: 1347.0189\n",
      "Epoch 17/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1711.9574 - val_loss: 1224.3226\n",
      "Epoch 18/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1586.4496 - val_loss: 1119.9308\n",
      "Epoch 19/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 1465.3301 - val_loss: 1031.3207\n",
      "Epoch 20/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 1351.6995 - val_loss: 934.2416\n",
      "Epoch 21/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1249.3415 - val_loss: 852.5265\n",
      "Epoch 22/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1168.2534 - val_loss: 783.8724\n",
      "Epoch 23/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 1069.7237 - val_loss: 727.8953\n",
      "Epoch 24/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 998.0033 - val_loss: 665.2661\n",
      "Epoch 25/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 931.7317 - val_loss: 604.8176\n",
      "Epoch 26/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 864.6463 - val_loss: 553.7521\n",
      "Epoch 27/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 819.7415 - val_loss: 565.8857\n",
      "Epoch 28/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 752.5281 - val_loss: 485.7041\n",
      "Epoch 29/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 697.0973 - val_loss: 474.7287\n",
      "Epoch 30/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 655.6706 - val_loss: 473.7102\n",
      "Epoch 31/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 617.3182 - val_loss: 377.6540\n",
      "Epoch 32/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 572.2137 - val_loss: 358.3694\n",
      "Epoch 33/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 548.1615 - val_loss: 352.5532\n",
      "Epoch 34/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 511.5056 - val_loss: 309.7088\n",
      "Epoch 35/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 486.6145 - val_loss: 287.5079\n",
      "Epoch 36/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 478.5970 - val_loss: 290.5173\n",
      "Epoch 37/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 441.3492 - val_loss: 274.7759\n",
      "Epoch 38/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 405.7026 - val_loss: 246.4678\n",
      "Epoch 39/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 393.5840 - val_loss: 256.2430\n",
      "Epoch 40/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 376.6621 - val_loss: 234.2642\n",
      "Epoch 41/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 352.4065 - val_loss: 230.1924\n",
      "Epoch 42/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 353.0114 - val_loss: 205.8289\n",
      "Epoch 43/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 333.5447 - val_loss: 215.3065\n",
      "Epoch 44/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 319.8214 - val_loss: 201.9352\n",
      "Epoch 45/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 312.2370 - val_loss: 194.3019\n",
      "Epoch 46/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 296.9722 - val_loss: 209.2463\n",
      "Epoch 47/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 288.1498 - val_loss: 201.7367\n",
      "Epoch 48/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 290.5985 - val_loss: 193.9563\n",
      "Epoch 49/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 265.4651 - val_loss: 196.5190\n",
      "Epoch 50/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 281.9886 - val_loss: 191.4252\n",
      "Epoch 51/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 258.0009 - val_loss: 161.0669\n",
      "Epoch 52/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 259.7428 - val_loss: 248.4145\n",
      "Epoch 53/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 253.1349 - val_loss: 161.5856\n",
      "Epoch 54/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 234.4647 - val_loss: 162.3658\n",
      "Epoch 55/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 249.1887 - val_loss: 160.4021\n",
      "Epoch 56/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 227.6829 - val_loss: 159.6138\n",
      "Epoch 57/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 225.7310 - val_loss: 160.7817\n",
      "Epoch 58/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 221.3810 - val_loss: 171.8839\n",
      "Epoch 59/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 231.1298 - val_loss: 163.8362\n",
      "Epoch 60/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 218.6234 - val_loss: 161.1915\n",
      "Epoch 61/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 204.7979 - val_loss: 155.0924\n",
      "Epoch 62/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 206.9786 - val_loss: 241.4003\n",
      "Epoch 63/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 207.6643 - val_loss: 289.7419\n",
      "Epoch 64/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 198.8481 - val_loss: 163.0110\n",
      "Epoch 65/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 214.9866 - val_loss: 163.7525\n",
      "Epoch 66/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 194.3202 - val_loss: 162.7689\n",
      "Epoch 67/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 214.3599 - val_loss: 173.1570\n",
      "Epoch 68/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 192.3204 - val_loss: 156.9183\n",
      "Epoch 69/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 191.4514 - val_loss: 190.1748\n",
      "Epoch 70/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 180.9660 - val_loss: 155.3956\n",
      "Epoch 71/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 191.5446 - val_loss: 252.8201\n",
      "Epoch 72/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 202.8824 - val_loss: 159.7746\n",
      "Epoch 73/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 178.1730 - val_loss: 164.9516\n",
      "Epoch 74/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 21us/step - loss: 176.4628 - val_loss: 177.9437\n",
      "Epoch 75/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 177.5512 - val_loss: 163.5049\n",
      "Epoch 76/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 175.3709 - val_loss: 155.9617\n",
      "Epoch 77/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 176.9591 - val_loss: 187.3227\n",
      "Epoch 78/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 181.9026 - val_loss: 161.3189\n",
      "Epoch 79/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 170.8182 - val_loss: 161.4221\n",
      "Epoch 80/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 175.7751 - val_loss: 157.8793\n",
      "Epoch 81/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.5102 - val_loss: 197.4972\n",
      "Epoch 82/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.7735 - val_loss: 215.8596\n",
      "Epoch 83/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.4930 - val_loss: 161.8403\n",
      "Epoch 84/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 161.3282 - val_loss: 161.6910\n",
      "Epoch 85/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 176.7908 - val_loss: 162.8488\n",
      "Epoch 86/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 161.0900 - val_loss: 216.8416\n",
      "Epoch 87/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 158.3729 - val_loss: 159.4641\n",
      "Epoch 88/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 214.9070 - val_loss: 459.1817\n",
      "Epoch 89/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 271.5588 - val_loss: 189.4802\n",
      "Epoch 90/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 175.7038 - val_loss: 167.1733\n",
      "Epoch 91/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 167.8238 - val_loss: 160.1082\n",
      "Epoch 92/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 158.9479 - val_loss: 173.6599\n",
      "Epoch 93/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.3153 - val_loss: 186.5360\n",
      "Epoch 94/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.6799 - val_loss: 178.3916\n",
      "Epoch 95/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.3085 - val_loss: 173.9478\n",
      "Epoch 96/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.8413 - val_loss: 163.7744\n",
      "Epoch 97/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 152.5101 - val_loss: 178.0941\n",
      "Epoch 98/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.0274 - val_loss: 194.5865\n",
      "Epoch 99/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 154.2072 - val_loss: 196.4799\n",
      "Epoch 100/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 153.7571 - val_loss: 160.6333\n",
      "Epoch 101/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 158.8678 - val_loss: 211.0648\n",
      "Epoch 102/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.5713 - val_loss: 161.8617\n",
      "Epoch 103/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 155.2504 - val_loss: 171.5679\n",
      "Epoch 104/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.3180 - val_loss: 173.7706\n",
      "Epoch 105/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.4440 - val_loss: 164.8022\n",
      "Epoch 106/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.0543 - val_loss: 170.0724\n",
      "Epoch 107/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 150.6315 - val_loss: 194.9208\n",
      "Epoch 108/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.1307 - val_loss: 184.4988\n",
      "Epoch 109/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 154.9814 - val_loss: 162.7626\n",
      "Epoch 110/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 151.0622 - val_loss: 205.8173\n",
      "Epoch 111/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.3529 - val_loss: 270.2652\n",
      "Epoch 112/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.7138 - val_loss: 185.2700\n",
      "Epoch 113/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.0376 - val_loss: 177.6308\n",
      "Epoch 114/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.2909 - val_loss: 184.8597\n",
      "Epoch 115/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 145.7102 - val_loss: 215.4946\n",
      "Epoch 116/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 152.9403 - val_loss: 264.3490\n",
      "Epoch 117/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.2337 - val_loss: 210.0395\n",
      "Epoch 118/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 144.6300 - val_loss: 198.9496\n",
      "Epoch 119/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 151.6240 - val_loss: 193.2379\n",
      "Epoch 120/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.5490 - val_loss: 176.5355\n",
      "Epoch 121/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.0830 - val_loss: 167.3624\n",
      "Epoch 122/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.3942 - val_loss: 164.3991\n",
      "Epoch 123/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 140.5201 - val_loss: 181.8840\n",
      "Epoch 124/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.3650 - val_loss: 193.0988\n",
      "Epoch 125/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.7530 - val_loss: 183.4695\n",
      "Epoch 126/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.8459 - val_loss: 177.1052\n",
      "Epoch 127/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 145.5631 - val_loss: 192.0503\n",
      "Epoch 128/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.4449 - val_loss: 166.5275\n",
      "Epoch 129/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.8733 - val_loss: 179.3467\n",
      "Epoch 130/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.2242 - val_loss: 202.1258\n",
      "Epoch 131/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.4412 - val_loss: 159.2920\n",
      "Epoch 132/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 147.2372 - val_loss: 161.2755\n",
      "Epoch 133/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.9684 - val_loss: 179.8659\n",
      "Epoch 134/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.8963 - val_loss: 175.5465\n",
      "Epoch 135/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 139.5226 - val_loss: 238.6303\n",
      "Epoch 136/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.7683 - val_loss: 175.6145\n",
      "Epoch 137/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 137.6842 - val_loss: 169.4880\n",
      "Epoch 138/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.2503 - val_loss: 179.9208\n",
      "Epoch 139/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.3226 - val_loss: 170.4071\n",
      "Epoch 140/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 137.5772 - val_loss: 207.4164\n",
      "Epoch 141/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.0106 - val_loss: 180.0695\n",
      "Epoch 142/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 145.0861 - val_loss: 159.3787\n",
      "Epoch 143/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 140.3519 - val_loss: 169.1168\n",
      "Epoch 144/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.0022 - val_loss: 175.6567\n",
      "Epoch 145/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.1715 - val_loss: 182.5421\n",
      "Epoch 146/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 134.5158 - val_loss: 165.8373\n",
      "Epoch 147/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.6921 - val_loss: 259.9591\n",
      "Epoch 148/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.9208 - val_loss: 163.4086\n",
      "Epoch 149/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.7327 - val_loss: 187.9035\n",
      "Epoch 150/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.2649 - val_loss: 168.8831\n",
      "Epoch 151/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.9867 - val_loss: 159.7267\n",
      "Epoch 152/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 136.1017 - val_loss: 160.7465\n",
      "Epoch 153/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 130.5357 - val_loss: 175.1899\n",
      "Epoch 154/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 129.7412 - val_loss: 171.6227\n",
      "Epoch 155/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 147.3796 - val_loss: 179.9103\n",
      "Epoch 156/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 140.4855 - val_loss: 160.3140\n",
      "Epoch 157/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 132.0489 - val_loss: 229.1415\n",
      "Epoch 158/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.3999 - val_loss: 164.3804\n",
      "Epoch 159/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.0285 - val_loss: 199.6764\n",
      "Epoch 160/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 129.3851 - val_loss: 197.1805\n",
      "Epoch 161/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.2308 - val_loss: 163.2229\n",
      "Epoch 162/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.9462 - val_loss: 187.1183\n",
      "Epoch 163/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.2660 - val_loss: 202.9729\n",
      "Epoch 164/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 126.8108 - val_loss: 176.9245\n",
      "Epoch 165/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 129.5600 - val_loss: 244.2565\n",
      "Epoch 166/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.8939 - val_loss: 179.4406\n",
      "Epoch 167/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 126.8943 - val_loss: 172.9912\n",
      "Epoch 168/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 125.1996 - val_loss: 179.0470\n",
      "Epoch 169/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 131.4461 - val_loss: 219.9851\n",
      "Epoch 170/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.8586 - val_loss: 165.1535\n",
      "Epoch 171/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.4134 - val_loss: 173.5131\n",
      "Epoch 172/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 127.9984 - val_loss: 219.4653\n",
      "Epoch 173/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 128.9343 - val_loss: 196.1577\n",
      "Epoch 174/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 131.4219 - val_loss: 172.8679\n",
      "Epoch 175/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 124.0085 - val_loss: 168.3919\n",
      "Epoch 176/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 124.9738 - val_loss: 194.1241\n",
      "Epoch 177/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 123.1624 - val_loss: 176.1333\n",
      "Epoch 178/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.3284 - val_loss: 168.0725\n",
      "Epoch 179/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 127.2658 - val_loss: 166.5820\n",
      "Epoch 180/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 123.2425 - val_loss: 168.2344\n",
      "Epoch 181/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 123.3481 - val_loss: 171.6373\n",
      "Epoch 182/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 125.6596 - val_loss: 175.3699\n",
      "Epoch 183/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 123.2881 - val_loss: 171.8478\n",
      "Epoch 184/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 124.6681 - val_loss: 179.6847\n",
      "Epoch 185/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 125.2302 - val_loss: 166.1038\n",
      "Epoch 186/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 124.8565 - val_loss: 164.1115\n",
      "Epoch 187/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.0290 - val_loss: 163.5811\n",
      "Epoch 188/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 123.1318 - val_loss: 163.7523\n",
      "Epoch 189/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 122.6699 - val_loss: 169.8297\n",
      "Epoch 190/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 124.1730 - val_loss: 197.9035\n",
      "Epoch 191/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 126.4262 - val_loss: 230.2008\n",
      "Epoch 192/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 122.8829 - val_loss: 209.6979\n",
      "Epoch 193/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.8578 - val_loss: 183.6761\n",
      "Epoch 194/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 280.8534 - val_loss: 308.3488\n",
      "Epoch 195/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 225.1551 - val_loss: 405.4654\n",
      "Epoch 196/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 177.0391 - val_loss: 233.8486\n",
      "Epoch 197/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.3497 - val_loss: 189.4942\n",
      "Epoch 198/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.1671 - val_loss: 197.2684\n",
      "Epoch 199/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 128.9923 - val_loss: 158.1483\n",
      "Epoch 200/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.2196 - val_loss: 165.5860\n",
      "Epoch 201/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 122.3655 - val_loss: 176.7631\n",
      "Epoch 202/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.5630 - val_loss: 179.0820\n",
      "Epoch 203/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.4443 - val_loss: 177.4205\n",
      "Epoch 204/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.0593 - val_loss: 181.6035\n",
      "Epoch 205/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.4548 - val_loss: 177.8966\n",
      "Epoch 206/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.5410 - val_loss: 183.0504\n",
      "Epoch 207/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.1380 - val_loss: 177.6941\n",
      "Epoch 208/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 117.3419 - val_loss: 177.0888\n",
      "Epoch 209/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.6187 - val_loss: 177.2844\n",
      "Epoch 210/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.6662 - val_loss: 172.5277\n",
      "Epoch 211/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.0986 - val_loss: 178.6764\n",
      "Epoch 212/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.4422 - val_loss: 175.4576\n",
      "Epoch 213/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.1018 - val_loss: 178.6969\n",
      "Epoch 214/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.2551 - val_loss: 183.3944\n",
      "Epoch 215/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.7528 - val_loss: 180.0023\n",
      "Epoch 216/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.7192 - val_loss: 174.1487\n",
      "Epoch 217/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 117.3468 - val_loss: 185.1045\n",
      "Epoch 218/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 117.2470 - val_loss: 174.2532\n",
      "Epoch 219/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.0284 - val_loss: 174.6288\n",
      "Epoch 220/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.0474 - val_loss: 176.6487\n",
      "Epoch 221/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.8628 - val_loss: 183.2626\n",
      "Epoch 222/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.1314 - val_loss: 188.4823\n",
      "Epoch 223/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.4579 - val_loss: 176.6675\n",
      "Epoch 224/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.3495 - val_loss: 179.1382\n",
      "Epoch 225/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 115.7039 - val_loss: 172.0908\n",
      "Epoch 226/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.7163 - val_loss: 181.9718\n",
      "Epoch 227/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.7217 - val_loss: 181.0440\n",
      "Epoch 228/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.2869 - val_loss: 181.9102\n",
      "Epoch 229/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 114.9650 - val_loss: 176.8041\n",
      "Epoch 230/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.3522 - val_loss: 180.8594\n",
      "Epoch 231/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.1082 - val_loss: 180.2296\n",
      "Epoch 232/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 115.6561 - val_loss: 179.2737\n",
      "Epoch 233/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.3166 - val_loss: 180.3728\n",
      "Epoch 234/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 115.2830 - val_loss: 173.6376\n",
      "Epoch 235/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.5508 - val_loss: 180.1040\n",
      "Epoch 236/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.0453 - val_loss: 170.3973\n",
      "Epoch 237/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 116.7903 - val_loss: 171.6812\n",
      "Epoch 238/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.0897 - val_loss: 176.4780\n",
      "Epoch 239/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 116.1170 - val_loss: 184.3373\n",
      "Epoch 240/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.2036 - val_loss: 169.7641\n",
      "Epoch 241/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 114.9718 - val_loss: 176.7923\n",
      "Epoch 242/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 115.4581 - val_loss: 177.4392\n",
      "Epoch 243/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 114.5875 - val_loss: 180.5259\n",
      "Epoch 244/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.1571 - val_loss: 179.4079\n",
      "Epoch 245/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.4314 - val_loss: 181.8397\n",
      "Epoch 246/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 114.7556 - val_loss: 182.1651\n",
      "Epoch 247/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 114.8741 - val_loss: 186.0041\n",
      "Epoch 248/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.2624 - val_loss: 181.0919\n",
      "Epoch 249/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 115.3397 - val_loss: 177.0071\n",
      "Epoch 250/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 114.3289 - val_loss: 173.2690\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=0, beta_1=0.5)\n",
    "#DCNN = Model([input_layer], [y])\n",
    "DCNN = RULCNNModel(TW, FeatureN)\n",
    "#DCNN.compile(loss=get_score,optimizer=opt)\n",
    "DCNN.compile(loss='mean_squared_error',optimizer=opt)\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "\n",
    "\n",
    "startTime = time.clock()\n",
    "history = DCNN.fit(samples, targets,nb_epoch=nb_epoch, batch_size=batch_size,verbose=1, \n",
    "                   validation_data=(samplet, labelt), callbacks=[lrate])\n",
    "endTime = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 211us/step\n",
      "Root Square Mean Error score: 13.1631671269127\n",
      "Health score: [296.13544]\n",
      "Elapsed time: 97.5915960797374\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "score = DCNN.evaluate(samplet, labelt)\n",
    "y_pred = DCNN.predict(samplet)\n",
    "healtScore = CMAPSAuxFunctions.compute_health_score(labelt, y_pred)\n",
    "\n",
    "print(\"Root Square Mean Error score: {}\".format(np.sqrt(score)))\n",
    "print(\"Health score: {}\".format(healtScore))\n",
    "print(\"Elapsed time: {}\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engineUnit = 21\n",
    "\n",
    "X_test2, _ = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_test, selected_features, time_window, 'train', unit_Number=engineUnit)\n",
    "X_test2 = min_max_scaler.fit_transform(X_test2)\n",
    "\n",
    "samplet2 = np.reshape(X_test2, newshape=(X_test2.shape[0], int(X_test2.shape[1]/nFeatures), nFeatures))\n",
    "\n",
    "nnPred = modelRUL.predict(X_test2)\n",
    "cnnPred = DCNN.predict(samplet2)\n",
    "\n",
    "maxCycle = X_test2.shape[0]\n",
    "faultCycle = y_test[engineUnit-1]\n",
    "cycles = np.arange(maxCycle)\n",
    "rulArray = np.arange(faultCycle, maxCycle+faultCycle)\n",
    "rulArray[rulArray > constRUL] = constRUL\n",
    "rulArray = np.flipud(rulArray)\n",
    "\n",
    "#print(cycles)\n",
    "#print(rulArray)\n",
    "\n",
    "'''print(\"Testing data\")\n",
    "print(X_test2.shape)\n",
    "print(X_test2[-5:,:])\n",
    "print(nnPred)\n",
    "print(cnnPred)'''\n",
    "\n",
    "plotRUL(cycles, rulArray, nnPred, cnnPred, engineUnit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotRUL(cycles, rulArray, nnPred, cnnPred, engineUnit):\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(cycles, rulArray, 'bo-', label='RUL')\n",
    "    plt.plot(cycles, nnPred, 'go-', label='NN Pred')\n",
    "    plt.plot(cycles, cnnPred, 'ro-', label='CNN Pred')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time (Cycle)\")\n",
    "    plt.ylabel(\"RUL\")\n",
    "    plt.title(\"Test Engine Unit #{}\".format(engineUnit))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
