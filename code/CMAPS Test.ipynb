{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Approach using MLP. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import CMAPSAuxFunctions\n",
    "#import plottingTools\n",
    "#from datetime import datetime\n",
    "#from sklearn.covariance import EllipticEnvelope\n",
    "#from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.model_selection import train_test_split, cross_validate\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#from dataManagement import DataManagerDamadics\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib notebook\n",
    "\n",
    "global constRUL\n",
    "\n",
    "constRUL = 125\n",
    "time_window = 30\n",
    "rul_vector = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and Reshape data\n",
    "\n",
    "Get the data from the text files, store it in a Pandas Dataframe and reshape it as appropiately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_train = '../CMAPSSData/train_FD001.txt'\n",
    "data_file_test = '../CMAPSSData/test_FD001.txt'\n",
    "\n",
    "#min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "standardScaler = StandardScaler()\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "#Selected as per CNN paper\n",
    "selected_features = ['T24', 'T30', 'T50', 'P30', 'Nf', 'Nc', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'htBleed', 'W31', 'W32']\n",
    "\n",
    "nFeatures = len(selected_features)\n",
    "\n",
    "#Get the X and y matrices with the specified time window\n",
    "#X_train, y_train = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_train, selected_features, time_window, 'train')\n",
    "#X_test, _ = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_test, selected_features, time_window, 'test')\n",
    "\n",
    "#Standardize the data\n",
    "#X_train = min_max_scaler.fit_transform(X_train)\n",
    "#X_test = min_max_scaler.fit_transform(X_test)\n",
    "\n",
    "#Retrieve the data as Xiang\n",
    "\n",
    "X_train, y_train, min_max_scaler = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_train, selected_features, \n",
    "                                                               time_window, 'train', scaler=min_max_scaler)\n",
    "\n",
    "X_test, _ , _ = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_test, selected_features, \n",
    "                                                        time_window, 'test', scaler=min_max_scaler, fit_transform=False)\n",
    "\n",
    "y_test = np.loadtxt(\"../CMAPSSData/RUL_FD001.txt\")\n",
    "y_test = np.array([x if x < constRUL else constRUL for x in y_test])\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "\n",
    "#Create samples and labels matrices to be used with the CNN\n",
    "samples = np.reshape(X_train, newshape=(X_train.shape[0], time_window, int(X_train.shape[1]/time_window)))\n",
    "samplet = np.reshape(X_test, newshape=(X_test.shape[0], time_window, int(X_test.shape[1]/time_window)))\n",
    "targets = np.reshape(y_train, newshape=(y_train.shape[0], -1))\n",
    "labelt = np.reshape(y_test, newshape=(y_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data (X, y)\n",
      "(17731, 420)\n",
      "(17731, 1)\n",
      "Testing data (X, y)\n",
      "(100, 420)\n",
      "(100, 1)\n",
      "Training data (samples, targets)\n",
      "(17731, 30, 14)\n",
      "(17731, 1)\n",
      "Testing data (samples, targets)\n",
      "(100, 30, 14)\n",
      "(100, 1)\n",
      "Training data (X, y)\n",
      "[[ 0.42168675  0.12579028  0.17049291 ...  0.5        -0.45736434\n",
      "  -0.78099972]\n",
      " [ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.75193798\n",
      "  -0.26760563]\n",
      " [-0.01204819 -0.02419882  0.47467927 ...  0.66666667 -0.53488372\n",
      "  -0.89201878]\n",
      " [-0.09638554 -0.02419882  0.53376097 ...  0.16666667 -0.76744186\n",
      "  -0.53106877]\n",
      " [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333 -0.64341085\n",
      "  -0.56365645]]\n",
      "[[4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Testing data (X, y)\n",
      "[[-0.55421687 -0.61107478 -0.55232951 ... -0.5         0.27131783\n",
      "   0.56420878]\n",
      " [-0.02409639 -0.30978853 -0.32343011 ...  0.16666667 -0.27131783\n",
      "   0.10770505]\n",
      " [-0.15662651 -0.05777196 -0.29642134 ...  0.         -0.03875969\n",
      "   0.28859431]\n",
      " [-0.40963855 -0.19032047 -0.56617151 ... -0.5         0.25581395\n",
      "   0.28500414]\n",
      " [-0.30722892 -0.11881404 -0.1144497  ...  0.33333333 -0.13178295\n",
      "  -0.1955261 ]]\n",
      "[[125.]\n",
      " [ 82.]\n",
      " [ 59.]\n",
      " [117.]\n",
      " [ 20.]]\n",
      "Training data (samples, targets)\n",
      "[[[ 0.42168675  0.12579028  0.17049291 ...  0.33333333 -0.27131783\n",
      "   -0.07953604]\n",
      "  [ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.27131783\n",
      "    0.01712234]\n",
      "  [-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  ...\n",
      "  [ 0.51204819  0.14453891  0.52464551 ...  0.         -0.62790698\n",
      "   -0.34217067]\n",
      "  [ 0.3253012   0.26444299  0.67623228 ...  0.         -1.\n",
      "   -0.17674676]\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]]\n",
      "\n",
      " [[ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.27131783\n",
      "    0.01712234]\n",
      "  [-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  [-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  ...\n",
      "  [ 0.3253012   0.26444299  0.67623228 ...  0.         -1.\n",
      "   -0.17674676]\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]]\n",
      "\n",
      " [[-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  [-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  ...\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]]\n",
      "\n",
      " [[-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  [ 0.06024096  0.36516242  0.4409183  ...  0.         -0.45736434\n",
      "   -0.17702292]\n",
      "  ...\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]\n",
      "  [ 0.21686747  0.49204273  0.49493585 ...  0.16666667 -0.76744186\n",
      "   -0.53106877]]\n",
      "\n",
      " [[ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  [ 0.06024096  0.36516242  0.4409183  ...  0.         -0.45736434\n",
      "   -0.17702292]\n",
      "  [ 0.31927711  0.06780031  0.22822417 ... -0.16666667 -0.56589147\n",
      "   -0.21264844]\n",
      "  ...\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]\n",
      "  [ 0.21686747  0.49204273  0.49493585 ...  0.16666667 -0.76744186\n",
      "   -0.53106877]\n",
      "  [ 0.59036145  0.2792675   0.68433491 ...  0.33333333 -0.64341085\n",
      "   -0.56365645]]]\n",
      "[[4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Testing data (samples, targets)\n",
      "[[[-0.55421687 -0.61107478 -0.55232951 ... -0.33333333  0.39534884\n",
      "    0.55426678]\n",
      "  [-0.65060241 -0.17680401 -0.75151924 ... -0.33333333  0.28682171\n",
      "    0.14526374]\n",
      "  [-0.55421687 -0.58098975 -0.41458474 ... -0.33333333  0.41085271\n",
      "    0.341066  ]\n",
      "  ...\n",
      "  [-0.46987952  0.10660562 -0.28257934 ... -0.16666667  0.34883721\n",
      "    0.38663353]\n",
      "  [-0.24698795 -0.3381295  -0.24004051 ... -0.16666667  0.51937984\n",
      "    0.11875173]\n",
      "  [-0.34337349 -0.13494659 -0.47029034 ... -0.5         0.27131783\n",
      "    0.56420878]]\n",
      "\n",
      " [[-0.02409639 -0.30978853 -0.32343011 ... -0.33333333  0.37984496\n",
      "    0.25158796]\n",
      "  [-0.1626506  -0.29016787 -0.23869007 ... -0.16666667  0.27131783\n",
      "    0.31593482]\n",
      "  [-0.39156627 -0.29627207 -0.36664416 ... -0.16666667  0.25581395\n",
      "    0.3985087 ]\n",
      "  ...\n",
      "  [-0.40361446 -0.21342926 -0.32039163 ... -0.16666667  0.37984496\n",
      "    0.31759183]\n",
      "  [-0.39156627 -0.39481142 -0.26772451 ... -0.33333333  0.06976744\n",
      "    0.50483292]\n",
      "  [-0.1686747  -0.48027033 -0.03207292 ...  0.16666667 -0.27131783\n",
      "    0.10770505]]\n",
      "\n",
      " [[-0.15662651 -0.05777196 -0.29642134 ... -0.16666667  0.17829457\n",
      "    0.11543772]\n",
      "  [-0.24698795 -0.23348594 -0.11006077 ... -0.16666667  0.1627907\n",
      "    0.1248274 ]\n",
      "  [-0.18072289 -0.24177022 -0.04051317 ... -0.33333333  0.2248062\n",
      "    0.34962717]\n",
      "  ...\n",
      "  [ 0.12048193 -0.27883148  0.05773126 ...  0.16666667 -0.10077519\n",
      "    0.02734051]\n",
      "  [ 0.01204819 -0.03727927 -0.28899392 ...  0.         -0.02325581\n",
      "    0.00303783]\n",
      "  [-0.11445783  0.24133421  0.1215395  ...  0.         -0.03875969\n",
      "    0.28859431]]\n",
      "\n",
      " [[-0.40963855 -0.19032047 -0.56617151 ... -0.66666667  0.42635659\n",
      "    0.32836233]\n",
      "  [-0.62048193 -0.57096141 -0.67555706 ... -0.66666667  0.51937984\n",
      "    0.4084507 ]\n",
      "  [-0.42771084 -0.36908655 -0.486158   ... -0.5         0.95348837\n",
      "    0.48715824]\n",
      "  ...\n",
      "  [-0.04216867 -0.5382603  -0.49729912 ... -0.5         0.44186047\n",
      "    0.08091687]\n",
      "  [-0.3253012  -0.46152169 -0.28865631 ... -0.66666667  0.28682171\n",
      "    0.54045844]\n",
      "  [-0.52409639 -0.39001526 -0.46893991 ... -0.5         0.25581395\n",
      "    0.28500414]]\n",
      "\n",
      " [[-0.30722892 -0.11881404 -0.1144497  ... -0.33333333  0.06976744\n",
      "    0.1996686 ]\n",
      "  [-0.10240964 -0.17680401 -0.13301823 ... -0.16666667  0.14728682\n",
      "    0.11516156]\n",
      "  [-0.02409639 -0.31981687 -0.02295746 ... -0.33333333  0.08527132\n",
      "   -0.12703673]\n",
      "  ...\n",
      "  [ 0.34337349 -0.03597122 -0.17049291 ...  0.16666667 -0.25581395\n",
      "   -0.1413974 ]\n",
      "  [ 0.23493976  0.0442555   0.25286968 ...  0.16666667 -0.19379845\n",
      "    0.03755869]\n",
      "  [ 0.04819277  0.33333333  0.44294396 ...  0.33333333 -0.13178295\n",
      "   -0.1955261 ]]]\n",
      "[[125.]\n",
      " [ 82.]\n",
      " [ 59.]\n",
      " [117.]\n",
      " [ 20.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data (X, y)\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"Testing data (X, y)\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(\"Training data (samples, targets)\")\n",
    "print(samples.shape)\n",
    "print(targets.shape)\n",
    "print(\"Testing data (samples, targets)\")\n",
    "print(samplet.shape)\n",
    "print(labelt.shape)\n",
    "\n",
    "'''\n",
    "print(\"Training data (X, y)\")\n",
    "print(X_train[:5,:])\n",
    "print(y_train[:5,:])\n",
    "print(\"Testing data (X, y)\")\n",
    "print(X_test[:5,:])\n",
    "print(y_test[:5,:])\n",
    "\n",
    "print(\"Training data (samples, targets)\")\n",
    "print(samples[:5,:])\n",
    "print(targets[:5])\n",
    "print(\"Testing data (samples, targets)\")\n",
    "print(samplet[:5,:])\n",
    "print(labelt[:5])\n",
    "'''\n",
    "\n",
    "print(\"Training data (X, y)\")\n",
    "print(X_train[-5:,:])\n",
    "print(y_train[-5:,:])\n",
    "print(\"Testing data (X, y)\")\n",
    "print(X_test[-5:,:])\n",
    "print(y_test[-5:,:])\n",
    "\n",
    "print(\"Training data (samples, targets)\")\n",
    "print(samples[-5:,:])\n",
    "print(targets[-5:])\n",
    "print(\"Testing data (samples, targets)\")\n",
    "print(samplet[-5:,:])\n",
    "print(labelt[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model\n",
    "\n",
    "We will use a very simple ANN for this example. The model is Dense(ReLU, 100)->Dense(ReLu, 100)->Dense(Linear, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RULmodel(input_shape):\n",
    "    \n",
    "    print(input_shape)\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(500, input_dim=input_shape, activation='tanh', kernel_initializer='glorot_normal', name='fc1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='tanh', kernel_initializer='glorot_normal', name='fc2'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(100, activation='tanh', name='fc3'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(10, activation='tanh', name='fc4'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    #create a placeholder for the input\n",
    "    #X_input = Input(shape=(input_shape))\n",
    "    \n",
    "    #Create the layers\n",
    "    #X = Dense(100, activation='relu', name='fc1')(X_input)\n",
    "    #X = Dense(100, activation='relu', name='fc2')(X)\n",
    "    #X = Dense(1, activation='linear', name='out')(X)\n",
    "    \n",
    "    # Create model. This creates the Keras model instance, you'll use this instance to train/test the model.\n",
    "    #model = Sequential(inputs = X_input, outputs = X, name='RUL')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the keras model\n",
    "Fit the Keras model to the data and determine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n",
      "WARNING:tensorflow:From C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 1/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 6334.0640 - mean_squared_error: 6334.0640\n",
      "Epoch 2/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 5670.7711 - mean_squared_error: 5670.7711\n",
      "Epoch 3/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 5236.3471 - mean_squared_error: 5236.3471\n",
      "Epoch 4/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 4841.6110 - mean_squared_error: 4841.6110\n",
      "Epoch 5/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 4478.2607 - mean_squared_error: 4478.2607\n",
      "Epoch 6/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 4151.5401 - mean_squared_error: 4151.5401\n",
      "Epoch 7/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3832.8784 - mean_squared_error: 3832.8784\n",
      "Epoch 8/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3540.6372 - mean_squared_error: 3540.6372\n",
      "Epoch 9/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 3274.2537 - mean_squared_error: 3274.2537\n",
      "Epoch 10/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 3030.0490 - mean_squared_error: 3030.0490\n",
      "Epoch 11/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 2802.4098 - mean_squared_error: 2802.4098\n",
      "Epoch 12/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2592.0042 - mean_squared_error: 2592.0042\n",
      "Epoch 13/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2398.9823 - mean_squared_error: 2398.9823\n",
      "Epoch 14/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2217.2077 - mean_squared_error: 2217.2077\n",
      "Epoch 15/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 2053.8207 - mean_squared_error: 2053.8207\n",
      "Epoch 16/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1901.8907 - mean_squared_error: 1901.8907\n",
      "Epoch 17/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 1755.5734 - mean_squared_error: 1755.5734\n",
      "Epoch 18/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 1626.5026 - mean_squared_error: 1626.5026\n",
      "Epoch 19/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1510.2679 - mean_squared_error: 1510.2679\n",
      "Epoch 20/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 1398.9417 - mean_squared_error: 1398.9417\n",
      "Epoch 21/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1296.9295 - mean_squared_error: 1296.9295\n",
      "Epoch 22/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 1207.6916 - mean_squared_error: 1207.6916\n",
      "Epoch 23/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1127.2412 - mean_squared_error: 1127.2412\n",
      "Epoch 24/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 1046.2195 - mean_squared_error: 1046.2195\n",
      "Epoch 25/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 977.6496 - mean_squared_error: 977.6496\n",
      "Epoch 26/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 917.7395 - mean_squared_error: 917.7395\n",
      "Epoch 27/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 861.6415 - mean_squared_error: 861.6415\n",
      "Epoch 28/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 803.2751 - mean_squared_error: 803.2751\n",
      "Epoch 29/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 759.0830 - mean_squared_error: 759.0830\n",
      "Epoch 30/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 705.7304 - mean_squared_error: 705.7304\n",
      "Epoch 31/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 671.4121 - mean_squared_error: 671.4121\n",
      "Epoch 32/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 635.9439 - mean_squared_error: 635.9439\n",
      "Epoch 33/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 598.5339 - mean_squared_error: 598.5339\n",
      "Epoch 34/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 575.8251 - mean_squared_error: 575.8251\n",
      "Epoch 35/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 545.3641 - mean_squared_error: 545.3641\n",
      "Epoch 36/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 523.0878 - mean_squared_error: 523.0878\n",
      "Epoch 37/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 496.3503 - mean_squared_error: 496.3503\n",
      "Epoch 38/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 477.6440 - mean_squared_error: 477.6440\n",
      "Epoch 39/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 465.3056 - mean_squared_error: 465.3056\n",
      "Epoch 40/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 449.1564 - mean_squared_error: 449.1564\n",
      "Epoch 41/250\n",
      "17731/17731 [==============================] - 0s 11us/step - loss: 428.6815 - mean_squared_error: 428.6815\n",
      "Epoch 42/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 413.8213 - mean_squared_error: 413.8213\n",
      "Epoch 43/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 407.8704 - mean_squared_error: 407.8704\n",
      "Epoch 44/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 392.6241 - mean_squared_error: 392.6241\n",
      "Epoch 45/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 387.9133 - mean_squared_error: 387.9133\n",
      "Epoch 46/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 377.3124 - mean_squared_error: 377.3124\n",
      "Epoch 47/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 374.7638 - mean_squared_error: 374.7638\n",
      "Epoch 48/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 360.7809 - mean_squared_error: 360.7809\n",
      "Epoch 49/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 357.0071 - mean_squared_error: 357.0071\n",
      "Epoch 50/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 350.6209 - mean_squared_error: 350.6209\n",
      "Epoch 51/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 345.8820 - mean_squared_error: 345.8820\n",
      "Epoch 52/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 337.4909 - mean_squared_error: 337.4909\n",
      "Epoch 53/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 334.5636 - mean_squared_error: 334.5636\n",
      "Epoch 54/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 329.3331 - mean_squared_error: 329.3331\n",
      "Epoch 55/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 327.4309 - mean_squared_error: 327.4309\n",
      "Epoch 56/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 315.1075 - mean_squared_error: 315.1075\n",
      "Epoch 57/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 316.3855 - mean_squared_error: 316.3855\n",
      "Epoch 58/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 318.6843 - mean_squared_error: 318.6843\n",
      "Epoch 59/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 313.2407 - mean_squared_error: 313.2407\n",
      "Epoch 60/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 312.1142 - mean_squared_error: 312.1142\n",
      "Epoch 61/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 314.6155 - mean_squared_error: 314.6155\n",
      "Epoch 62/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 302.8743 - mean_squared_error: 302.8743\n",
      "Epoch 63/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 308.0871 - mean_squared_error: 308.0871\n",
      "Epoch 64/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 298.5128 - mean_squared_error: 298.5128\n",
      "Epoch 65/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 301.5376 - mean_squared_error: 301.5376\n",
      "Epoch 66/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 8us/step - loss: 295.8387 - mean_squared_error: 295.8387\n",
      "Epoch 67/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 297.0449 - mean_squared_error: 297.0449\n",
      "Epoch 68/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 297.2977 - mean_squared_error: 297.2977\n",
      "Epoch 69/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 295.8412 - mean_squared_error: 295.8412\n",
      "Epoch 70/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 295.9284 - mean_squared_error: 295.9284\n",
      "Epoch 71/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 283.3271 - mean_squared_error: 283.3271\n",
      "Epoch 72/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 294.2778 - mean_squared_error: 294.2778\n",
      "Epoch 73/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 283.0989 - mean_squared_error: 283.0989\n",
      "Epoch 74/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 287.7737 - mean_squared_error: 287.7737\n",
      "Epoch 75/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 287.1995 - mean_squared_error: 287.1995\n",
      "Epoch 76/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 284.8943 - mean_squared_error: 284.8943\n",
      "Epoch 77/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 283.2427 - mean_squared_error: 283.2427\n",
      "Epoch 78/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 285.0213 - mean_squared_error: 285.0213\n",
      "Epoch 79/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 286.5594 - mean_squared_error: 286.5594\n",
      "Epoch 80/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 278.4149 - mean_squared_error: 278.4149\n",
      "Epoch 81/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 281.5953 - mean_squared_error: 281.5953\n",
      "Epoch 82/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 283.7758 - mean_squared_error: 283.7758\n",
      "Epoch 83/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 280.9306 - mean_squared_error: 280.9306\n",
      "Epoch 84/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 277.4587 - mean_squared_error: 277.4587\n",
      "Epoch 85/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 276.3245 - mean_squared_error: 276.3245\n",
      "Epoch 86/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 277.9981 - mean_squared_error: 277.9981\n",
      "Epoch 87/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 281.9221 - mean_squared_error: 281.9221\n",
      "Epoch 88/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 279.4701 - mean_squared_error: 279.4701\n",
      "Epoch 89/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 270.1613 - mean_squared_error: 270.1613\n",
      "Epoch 90/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 275.6174 - mean_squared_error: 275.6174\n",
      "Epoch 91/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 275.4138 - mean_squared_error: 275.4138\n",
      "Epoch 92/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 278.1454 - mean_squared_error: 278.1454\n",
      "Epoch 93/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 272.6046 - mean_squared_error: 272.6046\n",
      "Epoch 94/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 271.7803 - mean_squared_error: 271.7803\n",
      "Epoch 95/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 269.6378 - mean_squared_error: 269.6378\n",
      "Epoch 96/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 279.2295 - mean_squared_error: 279.2295\n",
      "Epoch 97/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 276.1728 - mean_squared_error: 276.1728\n",
      "Epoch 98/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 271.0630 - mean_squared_error: 271.0630\n",
      "Epoch 99/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 270.5137 - mean_squared_error: 270.5137\n",
      "Epoch 100/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 271.8776 - mean_squared_error: 271.8776\n",
      "Epoch 101/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 272.8675 - mean_squared_error: 272.8675\n",
      "Epoch 102/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 265.7868 - mean_squared_error: 265.7868\n",
      "Epoch 103/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 266.1143 - mean_squared_error: 266.1143\n",
      "Epoch 104/250\n",
      "17731/17731 [==============================] - 0s 15us/step - loss: 262.0913 - mean_squared_error: 262.0913\n",
      "Epoch 105/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 267.8856 - mean_squared_error: 267.8856\n",
      "Epoch 106/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 265.2609 - mean_squared_error: 265.2609\n",
      "Epoch 107/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 268.2470 - mean_squared_error: 268.2470\n",
      "Epoch 108/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 268.7772 - mean_squared_error: 268.7772\n",
      "Epoch 109/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 265.6093 - mean_squared_error: 265.6093\n",
      "Epoch 110/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 265.9805 - mean_squared_error: 265.9805\n",
      "Epoch 111/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 268.8173 - mean_squared_error: 268.8173\n",
      "Epoch 112/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 267.8083 - mean_squared_error: 267.8083\n",
      "Epoch 113/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 268.1767 - mean_squared_error: 268.1767\n",
      "Epoch 114/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 266.1518 - mean_squared_error: 266.1518\n",
      "Epoch 115/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 267.4022 - mean_squared_error: 267.4022\n",
      "Epoch 116/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.8487 - mean_squared_error: 262.8487\n",
      "Epoch 117/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 269.0813 - mean_squared_error: 269.0813\n",
      "Epoch 118/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.6079 - mean_squared_error: 261.6079\n",
      "Epoch 119/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 264.4240 - mean_squared_error: 264.4240\n",
      "Epoch 120/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 260.7880 - mean_squared_error: 260.7880\n",
      "Epoch 121/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.8735 - mean_squared_error: 262.8735\n",
      "Epoch 122/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 260.4873 - mean_squared_error: 260.4873\n",
      "Epoch 123/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 260.4163 - mean_squared_error: 260.4163\n",
      "Epoch 124/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 263.2871 - mean_squared_error: 263.2871\n",
      "Epoch 125/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.4996 - mean_squared_error: 262.4996\n",
      "Epoch 126/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.7669 - mean_squared_error: 262.7669\n",
      "Epoch 127/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.7390 - mean_squared_error: 261.7390\n",
      "Epoch 128/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 258.6598 - mean_squared_error: 258.6598\n",
      "Epoch 129/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 250.4720 - mean_squared_error: 250.4720\n",
      "Epoch 130/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.8305 - mean_squared_error: 257.8305\n",
      "Epoch 131/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.2519 - mean_squared_error: 257.2519\n",
      "Epoch 132/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.8519 - mean_squared_error: 256.8519\n",
      "Epoch 133/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.4372 - mean_squared_error: 254.4372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 256.6491 - mean_squared_error: 256.6491\n",
      "Epoch 135/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.9208 - mean_squared_error: 252.9208\n",
      "Epoch 136/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 259.2470 - mean_squared_error: 259.2470\n",
      "Epoch 137/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.0208 - mean_squared_error: 261.0208\n",
      "Epoch 138/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 261.6916 - mean_squared_error: 261.6916\n",
      "Epoch 139/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 262.8073 - mean_squared_error: 262.8073\n",
      "Epoch 140/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.6444 - mean_squared_error: 256.6444\n",
      "Epoch 141/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.4697 - mean_squared_error: 254.4697\n",
      "Epoch 142/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.4234 - mean_squared_error: 257.4234\n",
      "Epoch 143/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.9711 - mean_squared_error: 255.9711\n",
      "Epoch 144/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 253.1695 - mean_squared_error: 253.1695\n",
      "Epoch 145/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.7775 - mean_squared_error: 254.7775\n",
      "Epoch 146/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 258.5410 - mean_squared_error: 258.5410\n",
      "Epoch 147/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.4634 - mean_squared_error: 254.4634\n",
      "Epoch 148/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 253.8896 - mean_squared_error: 253.8896\n",
      "Epoch 149/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.3872 - mean_squared_error: 252.3872\n",
      "Epoch 150/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.1363 - mean_squared_error: 256.1363\n",
      "Epoch 151/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 252.0532 - mean_squared_error: 252.0532\n",
      "Epoch 152/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 251.7453 - mean_squared_error: 251.7453\n",
      "Epoch 153/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.0861 - mean_squared_error: 254.0861\n",
      "Epoch 154/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 251.4533 - mean_squared_error: 251.4533\n",
      "Epoch 155/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 257.2469 - mean_squared_error: 257.2469\n",
      "Epoch 156/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 251.9584 - mean_squared_error: 251.9584\n",
      "Epoch 157/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.9971 - mean_squared_error: 255.9971\n",
      "Epoch 158/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 256.8432 - mean_squared_error: 256.8432\n",
      "Epoch 159/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 251.8706 - mean_squared_error: 251.8706\n",
      "Epoch 160/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.1502 - mean_squared_error: 255.1502\n",
      "Epoch 161/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.6217 - mean_squared_error: 250.6217\n",
      "Epoch 162/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 255.5800 - mean_squared_error: 255.5800\n",
      "Epoch 163/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 250.0919 - mean_squared_error: 250.0919\n",
      "Epoch 164/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.4217 - mean_squared_error: 246.4217\n",
      "Epoch 165/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 254.0795 - mean_squared_error: 254.0795\n",
      "Epoch 166/250\n",
      "17731/17731 [==============================] - 0s 12us/step - loss: 254.7810 - mean_squared_error: 254.7810\n",
      "Epoch 167/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 248.8084 - mean_squared_error: 248.8084\n",
      "Epoch 168/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 247.8109 - mean_squared_error: 247.8109\n",
      "Epoch 169/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 252.4690 - mean_squared_error: 252.4690\n",
      "Epoch 170/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.0998 - mean_squared_error: 247.0998\n",
      "Epoch 171/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 247.1730 - mean_squared_error: 247.1730\n",
      "Epoch 172/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.0792 - mean_squared_error: 248.0792\n",
      "Epoch 173/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.4695 - mean_squared_error: 246.4695\n",
      "Epoch 174/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.1318 - mean_squared_error: 247.1318\n",
      "Epoch 175/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.7054 - mean_squared_error: 248.7054\n",
      "Epoch 176/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.9106 - mean_squared_error: 246.9106\n",
      "Epoch 177/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.5719 - mean_squared_error: 246.5719\n",
      "Epoch 178/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 243.4094 - mean_squared_error: 243.4094\n",
      "Epoch 179/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.1674 - mean_squared_error: 249.1674\n",
      "Epoch 180/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.0059 - mean_squared_error: 248.0059\n",
      "Epoch 181/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 249.0965 - mean_squared_error: 249.0965\n",
      "Epoch 182/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.3192 - mean_squared_error: 248.3192\n",
      "Epoch 183/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.7783 - mean_squared_error: 248.7783\n",
      "Epoch 184/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.1113 - mean_squared_error: 241.1113\n",
      "Epoch 185/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 244.5393 - mean_squared_error: 244.5393\n",
      "Epoch 186/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.4782 - mean_squared_error: 247.4782\n",
      "Epoch 187/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 247.6599 - mean_squared_error: 247.6599\n",
      "Epoch 188/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 248.5294 - mean_squared_error: 248.5294\n",
      "Epoch 189/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 242.0180 - mean_squared_error: 242.0180\n",
      "Epoch 190/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.3237 - mean_squared_error: 241.3237\n",
      "Epoch 191/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 248.5075 - mean_squared_error: 248.5075\n",
      "Epoch 192/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 245.3338 - mean_squared_error: 245.3338\n",
      "Epoch 193/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 242.3892 - mean_squared_error: 242.3892\n",
      "Epoch 194/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 247.6715 - mean_squared_error: 247.6715\n",
      "Epoch 195/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 241.3933 - mean_squared_error: 241.3933\n",
      "Epoch 196/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 246.9953 - mean_squared_error: 246.9953\n",
      "Epoch 197/250\n",
      "17731/17731 [==============================] - 0s 11us/step - loss: 244.9956 - mean_squared_error: 244.9956\n",
      "Epoch 198/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 247.9767 - mean_squared_error: 247.9767\n",
      "Epoch 199/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 241.8372 - mean_squared_error: 241.8372\n",
      "Epoch 200/250\n",
      "17731/17731 [==============================] - 0s 11us/step - loss: 243.0200 - mean_squared_error: 243.0200\n",
      "Epoch 201/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 11us/step - loss: 244.2429 - mean_squared_error: 244.2429\n",
      "Epoch 202/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.9632 - mean_squared_error: 234.9632\n",
      "Epoch 203/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 235.9394 - mean_squared_error: 235.9394\n",
      "Epoch 204/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 234.0824 - mean_squared_error: 234.0824\n",
      "Epoch 205/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 240.2013 - mean_squared_error: 240.2013\n",
      "Epoch 206/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 235.9896 - mean_squared_error: 235.9896\n",
      "Epoch 207/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 236.8984 - mean_squared_error: 236.8984\n",
      "Epoch 208/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 237.0966 - mean_squared_error: 237.0966\n",
      "Epoch 209/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.5928 - mean_squared_error: 234.5928\n",
      "Epoch 210/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 238.6925 - mean_squared_error: 238.6925\n",
      "Epoch 211/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 231.3004 - mean_squared_error: 231.3004\n",
      "Epoch 212/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.7604 - mean_squared_error: 232.7604\n",
      "Epoch 213/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.7085 - mean_squared_error: 234.7085\n",
      "Epoch 214/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 237.4074 - mean_squared_error: 237.4074\n",
      "Epoch 215/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.6108 - mean_squared_error: 232.6108\n",
      "Epoch 216/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 235.6703 - mean_squared_error: 235.6703\n",
      "Epoch 217/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.8178 - mean_squared_error: 232.8178\n",
      "Epoch 218/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 238.9832 - mean_squared_error: 238.9832\n",
      "Epoch 219/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.0724 - mean_squared_error: 236.0724\n",
      "Epoch 220/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.3683 - mean_squared_error: 232.3683\n",
      "Epoch 221/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 235.8754 - mean_squared_error: 235.8754\n",
      "Epoch 222/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 236.0428 - mean_squared_error: 236.0428\n",
      "Epoch 223/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 235.6899 - mean_squared_error: 235.6899\n",
      "Epoch 224/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.1626 - mean_squared_error: 233.1626\n",
      "Epoch 225/250\n",
      "17731/17731 [==============================] - 0s 8us/step - loss: 233.5551 - mean_squared_error: 233.5551\n",
      "Epoch 226/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 235.4580 - mean_squared_error: 235.4580\n",
      "Epoch 227/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.5304 - mean_squared_error: 233.5304\n",
      "Epoch 228/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.6718 - mean_squared_error: 228.6718\n",
      "Epoch 229/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.4574 - mean_squared_error: 233.4574\n",
      "Epoch 230/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.0047 - mean_squared_error: 231.0047\n",
      "Epoch 231/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.2922 - mean_squared_error: 232.2922\n",
      "Epoch 232/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.3267 - mean_squared_error: 233.3267\n",
      "Epoch 233/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 229.9749 - mean_squared_error: 229.9749\n",
      "Epoch 234/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.4138 - mean_squared_error: 233.4138\n",
      "Epoch 235/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.9212 - mean_squared_error: 232.9212\n",
      "Epoch 236/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.1788 - mean_squared_error: 234.1788\n",
      "Epoch 237/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.3541 - mean_squared_error: 231.3541\n",
      "Epoch 238/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 235.1735 - mean_squared_error: 235.1735\n",
      "Epoch 239/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.8765 - mean_squared_error: 230.8765\n",
      "Epoch 240/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.5343 - mean_squared_error: 230.5343\n",
      "Epoch 241/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.7843 - mean_squared_error: 230.7843\n",
      "Epoch 242/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 233.1997 - mean_squared_error: 233.1997\n",
      "Epoch 243/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 231.1593 - mean_squared_error: 231.1593\n",
      "Epoch 244/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 226.9490 - mean_squared_error: 226.9490\n",
      "Epoch 245/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 232.4395 - mean_squared_error: 232.4395\n",
      "Epoch 246/250\n",
      "17731/17731 [==============================] - 0s 10us/step - loss: 233.2487 - mean_squared_error: 233.2487\n",
      "Epoch 247/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 230.4315 - mean_squared_error: 230.4315\n",
      "Epoch 248/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 228.9533 - mean_squared_error: 228.9533\n",
      "Epoch 249/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.0293 - mean_squared_error: 234.0293\n",
      "Epoch 250/250\n",
      "17731/17731 [==============================] - 0s 9us/step - loss: 234.2428 - mean_squared_error: 234.2428\n"
     ]
    }
   ],
   "source": [
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "opt = Adam(lr=0, beta_1=0.5)\n",
    "\n",
    "#Create the model\n",
    "modelRUL = RULmodel(X_train.shape[1])\n",
    "\n",
    "#Compile the model.\n",
    "modelRUL.compile(optimizer = opt, loss = \"mean_squared_error\", metrics = [\"mse\"])\n",
    "\n",
    "startTime = time.clock()\n",
    "#Train the model.\n",
    "modelRUL.fit(x = X_train, y = y_train, epochs = 250, batch_size = 512, callbacks=[lrate])  \n",
    "endTime = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 280us/step\n",
      "Root Square Mean Error score: 14.270422636856171\n",
      "Health score: [509.56402979]\n",
      "Elapsed time: 41.50375710705195\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "score = modelRUL.evaluate(x = X_test, y = y_test)\n",
    "y_pred = modelRUL.predict(X_test)\n",
    "healtScore = CMAPSAuxFunctions.compute_health_score(y_test, y_pred)\n",
    "\n",
    "print(\"Root Square Mean Error score: {}\".format(np.sqrt(score[0])))\n",
    "print(\"Health score: {}\".format(healtScore))\n",
    "print(\"Elapsed time: {}\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model\n",
    "Fit the Keras model to the data using a CNN and determine its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, pickle, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler,EarlyStopping\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.pooling import AveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Activation, Input, merge, Conv2D, Reshape, Flatten, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import CMAPSAuxFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureN = 14\n",
    "nb_epoch = 250\n",
    "batch_size = 512\n",
    "FilterN = 10\n",
    "FilterL = 10\n",
    "rmse,sco,tm = [], [], []\n",
    "\n",
    "ConstRUL = 125\n",
    "TW = 30\n",
    "Dataset = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reshape data to fit a convNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model\n",
    "\n",
    "CNN model. The model is Dense(ReLU, 100)->Dense(ReLu, 100)->Dense(Linear, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RULCNNModel(TW, FeatureN):\n",
    "    \n",
    "    input_layer = Input(shape=(TW, FeatureN))\n",
    "    y = Reshape((TW, FeatureN, 1), input_shape=(TW, FeatureN, ),name = 'Reshape')(input_layer)\n",
    "\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C1')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C2')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C3')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C4')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C5')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C6')(y)\n",
    "    \n",
    "    y = Conv2D(1, 3, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='Clast')(y)  \n",
    "    \n",
    "    y = Reshape((TW,14))(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    \n",
    "    #y = Dense(100, activation='tanh', init='glorot_normal', activity_regularizer=keras.regularizers.l2(0.01),)(y)\n",
    "    y = Dense(100,activation='tanh', kernel_initializer='glorot_normal', name='fc')(y)\n",
    "    y = Dense(1)(y)\n",
    "    \n",
    "    model = Model(inputs = input_layer, outputs = y, name='RUL_CNN_Model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the keras model\n",
    "\n",
    "Fit the Keras model to the data and determine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), kernel_initializer=\"glorot_normal\", activation=\"tanh\", padding=\"same\", name=\"C1\")`\n",
      "  \n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), kernel_initializer=\"glorot_normal\", activation=\"tanh\", padding=\"same\", name=\"C2\")`\n",
      "  import sys\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), kernel_initializer=\"glorot_normal\", activation=\"tanh\", padding=\"same\", name=\"C3\")`\n",
      "  \n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), kernel_initializer=\"glorot_normal\", activation=\"tanh\", padding=\"same\", name=\"C4\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (3, 1), kernel_initializer=\"glorot_normal\", activation=\"tanh\", padding=\"same\", name=\"Clast\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17731 samples, validate on 100 samples\n",
      "Epoch 1/250\n",
      "17731/17731 [==============================] - 1s 71us/step - loss: 6717.0844 - val_loss: 5202.4780\n",
      "Epoch 2/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 5885.9272 - val_loss: 4766.7607\n",
      "Epoch 3/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 5428.4363 - val_loss: 4383.7920\n",
      "Epoch 4/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 5017.0690 - val_loss: 4034.3181\n",
      "Epoch 5/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4639.7179 - val_loss: 3712.2393\n",
      "Epoch 6/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4291.3311 - val_loss: 3414.4634\n",
      "Epoch 7/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3968.8188 - val_loss: 3141.4143\n",
      "Epoch 8/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3669.4003 - val_loss: 2883.8618\n",
      "Epoch 9/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3391.5316 - val_loss: 2647.2244\n",
      "Epoch 10/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3134.4578 - val_loss: 2438.1125\n",
      "Epoch 11/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2897.8419 - val_loss: 2216.9890\n",
      "Epoch 12/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2680.1780 - val_loss: 2084.2217\n",
      "Epoch 13/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2479.7414 - val_loss: 1896.5107\n",
      "Epoch 14/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2290.4982 - val_loss: 1707.3383\n",
      "Epoch 15/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2114.1520 - val_loss: 1608.3838\n",
      "Epoch 16/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1956.2316 - val_loss: 1528.8303\n",
      "Epoch 17/250\n",
      "17731/17731 [==============================] - ETA: 0s - loss: 1838.16 - 0s 22us/step - loss: 1824.2054 - val_loss: 1304.5498\n",
      "Epoch 18/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1670.7764 - val_loss: 1234.4337\n",
      "Epoch 19/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1548.6368 - val_loss: 1088.4121\n",
      "Epoch 20/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1429.0804 - val_loss: 993.3940\n",
      "Epoch 21/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1328.5565 - val_loss: 978.5246\n",
      "Epoch 22/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 1233.3700 - val_loss: 828.9808\n",
      "Epoch 23/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 1143.0322 - val_loss: 786.4475\n",
      "Epoch 24/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1057.5515 - val_loss: 703.1851\n",
      "Epoch 25/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 990.4144 - val_loss: 648.8448\n",
      "Epoch 26/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 922.2097 - val_loss: 596.8425\n",
      "Epoch 27/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 867.9219 - val_loss: 553.0841\n",
      "Epoch 28/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 793.6512 - val_loss: 517.7558\n",
      "Epoch 29/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 763.4099 - val_loss: 470.8757\n",
      "Epoch 30/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 708.5478 - val_loss: 432.6356\n",
      "Epoch 31/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 682.9626 - val_loss: 414.7556\n",
      "Epoch 32/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 634.0556 - val_loss: 387.0670\n",
      "Epoch 33/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 581.0242 - val_loss: 440.3317\n",
      "Epoch 34/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 597.3726 - val_loss: 330.2125\n",
      "Epoch 35/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 518.8871 - val_loss: 319.4900\n",
      "Epoch 36/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 488.5670 - val_loss: 299.1492\n",
      "Epoch 37/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 477.6842 - val_loss: 288.7579\n",
      "Epoch 38/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 437.9267 - val_loss: 263.9186\n",
      "Epoch 39/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 418.7270 - val_loss: 254.8358\n",
      "Epoch 40/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 403.7944 - val_loss: 240.5108\n",
      "Epoch 41/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 397.4596 - val_loss: 359.6085\n",
      "Epoch 42/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 391.9052 - val_loss: 231.5670\n",
      "Epoch 43/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 356.9631 - val_loss: 211.1503\n",
      "Epoch 44/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 328.6685 - val_loss: 270.8151\n",
      "Epoch 45/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 338.0257 - val_loss: 241.2383\n",
      "Epoch 46/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 308.6385 - val_loss: 195.7849\n",
      "Epoch 47/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 334.3025 - val_loss: 183.6249\n",
      "Epoch 48/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 293.8186 - val_loss: 447.1456\n",
      "Epoch 49/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 341.3259 - val_loss: 176.1097\n",
      "Epoch 50/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 289.4987 - val_loss: 176.7939\n",
      "Epoch 51/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 286.4236 - val_loss: 174.5203\n",
      "Epoch 52/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 338.3916 - val_loss: 191.8680\n",
      "Epoch 53/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 261.5420 - val_loss: 177.7534\n",
      "Epoch 54/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 262.5224 - val_loss: 185.1436\n",
      "Epoch 55/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 252.4440 - val_loss: 168.7502\n",
      "Epoch 56/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 247.1767 - val_loss: 160.0110\n",
      "Epoch 57/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 255.9732 - val_loss: 166.7032\n",
      "Epoch 58/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 236.8740 - val_loss: 184.3301\n",
      "Epoch 59/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 255.6263 - val_loss: 165.1505\n",
      "Epoch 60/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 223.5827 - val_loss: 165.6066\n",
      "Epoch 61/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 231.8134 - val_loss: 224.6234\n",
      "Epoch 62/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 236.1763 - val_loss: 242.8710\n",
      "Epoch 63/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 217.6558 - val_loss: 188.9767\n",
      "Epoch 64/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 230.0744 - val_loss: 181.2053\n",
      "Epoch 65/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 214.3940 - val_loss: 162.5898\n",
      "Epoch 66/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 217.0326 - val_loss: 174.7546\n",
      "Epoch 67/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 211.0434 - val_loss: 177.3413\n",
      "Epoch 68/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 205.8616 - val_loss: 160.5580\n",
      "Epoch 69/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 203.0905 - val_loss: 352.8773\n",
      "Epoch 70/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 266.7907 - val_loss: 162.0346\n",
      "Epoch 71/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 192.3978 - val_loss: 173.6724\n",
      "Epoch 72/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 216.3827 - val_loss: 175.6579\n",
      "Epoch 73/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 194.6314 - val_loss: 162.7056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 268.2331 - val_loss: 398.9021\n",
      "Epoch 75/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 356.5885 - val_loss: 273.1588\n",
      "Epoch 76/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 292.5782 - val_loss: 176.1036\n",
      "Epoch 77/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 209.3674 - val_loss: 164.1171\n",
      "Epoch 78/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 205.5475 - val_loss: 191.0395\n",
      "Epoch 79/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 186.7979 - val_loss: 174.4910\n",
      "Epoch 80/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 191.3580 - val_loss: 178.2907\n",
      "Epoch 81/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 190.6494 - val_loss: 215.4575\n",
      "Epoch 82/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 195.5091 - val_loss: 295.1719\n",
      "Epoch 83/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 185.2912 - val_loss: 181.0740\n",
      "Epoch 84/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 185.0092 - val_loss: 161.5883\n",
      "Epoch 85/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 179.6831 - val_loss: 164.1343\n",
      "Epoch 86/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 191.0356 - val_loss: 171.6705\n",
      "Epoch 87/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 183.8177 - val_loss: 176.0221\n",
      "Epoch 88/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 172.0649 - val_loss: 168.9700\n",
      "Epoch 89/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 195.7454 - val_loss: 182.9714\n",
      "Epoch 90/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.7242 - val_loss: 165.9888\n",
      "Epoch 91/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 169.9570 - val_loss: 232.0300\n",
      "Epoch 92/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 186.7745 - val_loss: 173.0968\n",
      "Epoch 93/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 172.2482 - val_loss: 210.8730\n",
      "Epoch 94/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 167.5983 - val_loss: 188.6756\n",
      "Epoch 95/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 177.1207 - val_loss: 179.2328\n",
      "Epoch 96/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 169.7242 - val_loss: 201.0546\n",
      "Epoch 97/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 169.0676 - val_loss: 183.0233\n",
      "Epoch 98/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.8669 - val_loss: 168.2429\n",
      "Epoch 99/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 163.7357 - val_loss: 187.5771\n",
      "Epoch 100/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 174.8921 - val_loss: 171.9954\n",
      "Epoch 101/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 162.5929 - val_loss: 218.3586\n",
      "Epoch 102/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 193.3314 - val_loss: 158.4412\n",
      "Epoch 103/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.6289 - val_loss: 215.0157\n",
      "Epoch 104/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 185.3248 - val_loss: 165.6599\n",
      "Epoch 105/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.5796 - val_loss: 173.1058\n",
      "Epoch 106/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 159.1095 - val_loss: 177.5350\n",
      "Epoch 107/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 155.5067 - val_loss: 161.5090\n",
      "Epoch 108/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.6880 - val_loss: 167.4581\n",
      "Epoch 109/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 177.1924 - val_loss: 167.7102\n",
      "Epoch 110/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 161.8701 - val_loss: 185.6432\n",
      "Epoch 111/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 159.4096 - val_loss: 204.4326\n",
      "Epoch 112/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 161.8473 - val_loss: 159.6273\n",
      "Epoch 113/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 166.7192 - val_loss: 184.6589\n",
      "Epoch 114/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 154.6278 - val_loss: 162.1325\n",
      "Epoch 115/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.7219 - val_loss: 166.5735\n",
      "Epoch 116/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 156.7006 - val_loss: 199.2463\n",
      "Epoch 117/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 172.5634 - val_loss: 168.6509\n",
      "Epoch 118/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 161.3709 - val_loss: 245.2647\n",
      "Epoch 119/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.1379 - val_loss: 159.3116\n",
      "Epoch 120/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.4731 - val_loss: 160.7382\n",
      "Epoch 121/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 161.7291 - val_loss: 248.6070\n",
      "Epoch 122/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 171.8973 - val_loss: 176.9509\n",
      "Epoch 123/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 148.2799 - val_loss: 166.7314\n",
      "Epoch 124/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 152.6604 - val_loss: 221.0665\n",
      "Epoch 125/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.8617 - val_loss: 188.6632\n",
      "Epoch 126/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 154.8118 - val_loss: 172.4988\n",
      "Epoch 127/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.2736 - val_loss: 189.7403\n",
      "Epoch 128/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 151.7785 - val_loss: 167.2887\n",
      "Epoch 129/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 167.0729 - val_loss: 192.8239\n",
      "Epoch 130/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.3708 - val_loss: 154.4549\n",
      "Epoch 131/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.8099 - val_loss: 160.7607\n",
      "Epoch 132/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 158.4738 - val_loss: 214.3844\n",
      "Epoch 133/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 150.7716 - val_loss: 198.2939\n",
      "Epoch 134/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 161.1988 - val_loss: 180.4115\n",
      "Epoch 135/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 145.6721 - val_loss: 159.2941\n",
      "Epoch 136/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 158.6801 - val_loss: 178.6970\n",
      "Epoch 137/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.9195 - val_loss: 189.1044\n",
      "Epoch 138/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.4985 - val_loss: 166.5542\n",
      "Epoch 139/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.4638 - val_loss: 185.5665\n",
      "Epoch 140/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.0430 - val_loss: 151.3931\n",
      "Epoch 141/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.2056 - val_loss: 197.3493\n",
      "Epoch 142/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.8148 - val_loss: 204.2579\n",
      "Epoch 143/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 146.8896 - val_loss: 160.7227\n",
      "Epoch 144/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 153.4734 - val_loss: 176.7271\n",
      "Epoch 145/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.6930 - val_loss: 221.3169\n",
      "Epoch 146/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 152.7536 - val_loss: 166.4960\n",
      "Epoch 147/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 22us/step - loss: 146.3231 - val_loss: 178.1789\n",
      "Epoch 148/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 140.2366 - val_loss: 161.0207\n",
      "Epoch 149/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 152.0912 - val_loss: 183.3679\n",
      "Epoch 150/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.3227 - val_loss: 161.7312\n",
      "Epoch 151/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.4402 - val_loss: 197.5541\n",
      "Epoch 152/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 179.8768 - val_loss: 182.8827\n",
      "Epoch 153/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 141.8543 - val_loss: 184.7430\n",
      "Epoch 154/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.8107 - val_loss: 229.1389\n",
      "Epoch 155/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.5570 - val_loss: 168.4115\n",
      "Epoch 156/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.6260 - val_loss: 254.6594\n",
      "Epoch 157/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.4576 - val_loss: 178.8683\n",
      "Epoch 158/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.7812 - val_loss: 195.1029\n",
      "Epoch 159/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 138.6511 - val_loss: 150.4910\n",
      "Epoch 160/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 138.6494 - val_loss: 155.6011\n",
      "Epoch 161/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 140.5552 - val_loss: 157.5823\n",
      "Epoch 162/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.4690 - val_loss: 193.5208\n",
      "Epoch 163/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.2506 - val_loss: 151.9448\n",
      "Epoch 164/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.4224 - val_loss: 234.1296\n",
      "Epoch 165/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.9214 - val_loss: 313.6514\n",
      "Epoch 166/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 151.2941 - val_loss: 152.3987\n",
      "Epoch 167/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.5754 - val_loss: 201.2885\n",
      "Epoch 168/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.6997 - val_loss: 166.8586\n",
      "Epoch 169/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.2570 - val_loss: 170.0504\n",
      "Epoch 170/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 136.9548 - val_loss: 155.1581\n",
      "Epoch 171/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.9281 - val_loss: 174.6216\n",
      "Epoch 172/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 136.8804 - val_loss: 163.1173\n",
      "Epoch 173/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.0781 - val_loss: 195.6638\n",
      "Epoch 174/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 146.9409 - val_loss: 160.5911\n",
      "Epoch 175/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.3692 - val_loss: 152.4678\n",
      "Epoch 176/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.0632 - val_loss: 207.9397\n",
      "Epoch 177/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.5447 - val_loss: 159.5449\n",
      "Epoch 178/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.7763 - val_loss: 170.2104\n",
      "Epoch 179/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.3060 - val_loss: 164.4823\n",
      "Epoch 180/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 136.1905 - val_loss: 159.7462\n",
      "Epoch 181/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.5429 - val_loss: 158.2107\n",
      "Epoch 182/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 153.6642 - val_loss: 167.4019\n",
      "Epoch 183/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.2385 - val_loss: 159.3595\n",
      "Epoch 184/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.5682 - val_loss: 201.9389\n",
      "Epoch 185/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.0451 - val_loss: 150.9944\n",
      "Epoch 186/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.7367 - val_loss: 182.8449\n",
      "Epoch 187/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.1824 - val_loss: 161.6750\n",
      "Epoch 188/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 136.3821 - val_loss: 181.4559\n",
      "Epoch 189/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.7556 - val_loss: 194.4548\n",
      "Epoch 190/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.3856 - val_loss: 165.4886\n",
      "Epoch 191/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.3460 - val_loss: 159.4669\n",
      "Epoch 192/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.5916 - val_loss: 178.0387\n",
      "Epoch 193/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.2893 - val_loss: 167.0701\n",
      "Epoch 194/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.5493 - val_loss: 181.5428\n",
      "Epoch 195/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.1213 - val_loss: 161.6584\n",
      "Epoch 196/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.1183 - val_loss: 159.3793\n",
      "Epoch 197/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.4045 - val_loss: 161.9319\n",
      "Epoch 198/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 128.1609 - val_loss: 150.9726\n",
      "Epoch 199/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.5257 - val_loss: 153.0826\n",
      "Epoch 200/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 128.7596 - val_loss: 155.2664\n",
      "Epoch 201/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 123.9277 - val_loss: 163.5389\n",
      "Epoch 202/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 123.0515 - val_loss: 169.5929\n",
      "Epoch 203/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.0626 - val_loss: 167.4259\n",
      "Epoch 204/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.3425 - val_loss: 170.9557\n",
      "Epoch 205/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 122.1351 - val_loss: 165.9400\n",
      "Epoch 206/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 122.0543 - val_loss: 167.3127\n",
      "Epoch 207/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.7429 - val_loss: 169.4197\n",
      "Epoch 208/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.4508 - val_loss: 170.0690\n",
      "Epoch 209/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 122.4386 - val_loss: 164.7825\n",
      "Epoch 210/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.5530 - val_loss: 168.5117\n",
      "Epoch 211/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.6442 - val_loss: 169.2673\n",
      "Epoch 212/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.2123 - val_loss: 171.6690\n",
      "Epoch 213/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.4702 - val_loss: 166.8801\n",
      "Epoch 214/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.5040 - val_loss: 170.3275\n",
      "Epoch 215/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.9370 - val_loss: 165.6335\n",
      "Epoch 216/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.3029 - val_loss: 173.0072\n",
      "Epoch 217/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.7794 - val_loss: 177.3356\n",
      "Epoch 218/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.9210 - val_loss: 169.6023\n",
      "Epoch 219/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.9359 - val_loss: 171.9852\n",
      "Epoch 220/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.4823 - val_loss: 171.7762\n",
      "Epoch 221/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.7681 - val_loss: 167.9075\n",
      "Epoch 222/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.9002 - val_loss: 172.2673\n",
      "Epoch 223/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.6148 - val_loss: 170.4365\n",
      "Epoch 224/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.5818 - val_loss: 178.1594\n",
      "Epoch 225/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.1327 - val_loss: 172.5107\n",
      "Epoch 226/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 122.1733 - val_loss: 166.8613\n",
      "Epoch 227/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.9887 - val_loss: 177.2325\n",
      "Epoch 228/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 120.6088 - val_loss: 174.1392\n",
      "Epoch 229/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.4989 - val_loss: 169.1211\n",
      "Epoch 230/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 121.9074 - val_loss: 170.2417\n",
      "Epoch 231/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 121.1303 - val_loss: 170.4030\n",
      "Epoch 232/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.9931 - val_loss: 167.4886\n",
      "Epoch 233/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.9682 - val_loss: 163.2080\n",
      "Epoch 234/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.0635 - val_loss: 167.3311\n",
      "Epoch 235/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 121.2193 - val_loss: 172.6089\n",
      "Epoch 236/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.5560 - val_loss: 171.4980\n",
      "Epoch 237/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.7016 - val_loss: 167.6497\n",
      "Epoch 238/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.8841 - val_loss: 163.6445\n",
      "Epoch 239/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 120.0455 - val_loss: 175.7829\n",
      "Epoch 240/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.6429 - val_loss: 164.2762\n",
      "Epoch 241/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 121.6006 - val_loss: 170.6509\n",
      "Epoch 242/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.0109 - val_loss: 175.6955\n",
      "Epoch 243/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.2198 - val_loss: 169.6263\n",
      "Epoch 244/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.3417 - val_loss: 176.1007\n",
      "Epoch 245/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.3233 - val_loss: 173.7549\n",
      "Epoch 246/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.0435 - val_loss: 170.0846\n",
      "Epoch 247/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.4787 - val_loss: 168.5685\n",
      "Epoch 248/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 119.7494 - val_loss: 169.8900\n",
      "Epoch 249/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.6318 - val_loss: 162.5405\n",
      "Epoch 250/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.3886 - val_loss: 171.2642\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=0, beta_1=0.5)\n",
    "#DCNN = Model([input_layer], [y])\n",
    "DCNN = RULCNNModel(TW, FeatureN)\n",
    "#DCNN.compile(loss=get_score,optimizer=opt)\n",
    "DCNN.compile(loss='mean_squared_error',optimizer=opt)\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "\n",
    "\n",
    "startTime = time.clock()\n",
    "history = DCNN.fit(samples, targets,nb_epoch=nb_epoch, batch_size=batch_size,verbose=1, \n",
    "                   validation_data=(samplet, labelt), callbacks=[lrate])\n",
    "endTime = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 135us/step\n",
      "Root Square Mean Error score: 13.086794689742208\n",
      "Health score: [294.03698705]\n",
      "Elapsed time: 99.57991812871201\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "score = DCNN.evaluate(samplet, labelt)\n",
    "y_pred = DCNN.predict(samplet)\n",
    "healtScore = CMAPSAuxFunctions.compute_health_score(labelt, y_pred)\n",
    "\n",
    "print(\"Root Square Mean Error score: {}\".format(np.sqrt(score)))\n",
    "print(\"Health score: {}\".format(healtScore))\n",
    "print(\"Elapsed time: {}\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRUL(cycles, rulArray, nnPred, cnnPred, engineUnit):\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(cycles, rulArray, 'bo-', label='RUL')\n",
    "    plt.plot(cycles, nnPred, 'go-', label='NN Pred')\n",
    "    plt.plot(cycles, cnnPred, 'ro-', label='CNN Pred')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time (Cycle)\")\n",
    "    plt.ylabel(\"RUL\")\n",
    "    plt.title(\"Test Engine Unit #{}\".format(engineUnit))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "engineUnit = 21\n",
    "\n",
    "X_test2, _ , _ = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_test, selected_features, time_window, \n",
    "                                                             'train', scaler=min_max_scaler, fit_transform=False, unit_number=engineUnit)\n",
    "#X_test2, _ = CMAPSAuxFunctions.retrieve_and_reshape_data(data_file_test, selected_features, time_window, 'train', unit_Number=engineUnit)\n",
    "#X_test2 = min_max_scaler.fit_transform(X_test2)\n",
    "\n",
    "samplet2 = np.reshape(X_test2, newshape=(X_test2.shape[0], int(X_test2.shape[1]/nFeatures), nFeatures))\n",
    "\n",
    "nnPred = modelRUL.predict(X_test2)\n",
    "cnnPred = DCNN.predict(samplet2)\n",
    "\n",
    "maxCycle = X_test2.shape[0]\n",
    "faultCycle = y_test[engineUnit-1]\n",
    "cycles = np.arange(maxCycle)\n",
    "rulArray = np.arange(faultCycle, maxCycle+faultCycle)\n",
    "rulArray[rulArray > constRUL] = constRUL\n",
    "rulArray = np.flipud(rulArray)\n",
    "\n",
    "#print(cycles)\n",
    "#print(rulArray)\n",
    "\n",
    "'''print(\"Testing data\")\n",
    "print(X_test2.shape)\n",
    "print(X_test2[-5:,:])\n",
    "print(nnPred)\n",
    "print(cnnPred)'''\n",
    "\n",
    "plotRUL(cycles, rulArray, nnPred, cnnPred, engineUnit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
