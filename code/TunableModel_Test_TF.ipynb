{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the DataHandlers. Test the CMAPSS DataHandler\n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\controlslab\\Anaconda3\\envs\\gpu_tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "from data_handler_CMAPS import CMAPSDataHandler\n",
    "from tunable_model import SequenceTunableModelRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Reshape, Conv2D, Flatten, MaxPooling2D, LSTM\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "\n",
    "import sklearn\n",
    "import CMAPSAuxFunctions\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define different types of Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_lambda_regularization = 0\n",
    "#l1_lambda_regularization = 0.2\n",
    "\n",
    "def RULmodel_LSTM(input_shape):\n",
    "    \"\"\"Define the RNN model\"\"\"\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    #model.add(Masking(mask_value=0, imput))\n",
    "    #model.add(LSTM(input_shape=input_shape, units=100, return_sequences=True, name='lstm1')))\n",
    "    model.add(LSTM(input_shape=input_shape, units=20, return_sequences=False, name='lstm2'))\n",
    "    model.add(Dense(10, input_dim=input_shape, activation='relu', kernel_initializer='glorot_normal', \n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda_regularization), name='fc1'))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(input_shape, output_shape):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None,input_shape), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=None, name=\"y\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def create_placeholders_rnn(num_features, sequence_length, output_shape):\n",
    "    X = tf.placeholder(tf.float32, shape=(None,sequence_length,num_features), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=None, name=\"y\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "#regularizers.l2(l2_lambda_regularization)\n",
    "def tf_model(X):\n",
    "    \n",
    "    l2_lambda_regularization = 0.20\n",
    "    l1_lambda_regularization = 0.10\n",
    "    \n",
    "    A1 = tf.layers.dense(X, 20, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False), \n",
    "                         kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1_lambda_regularization,l2_lambda_regularization), name=\"fc1\")\n",
    "    A2 = tf.layers.dense(A1, 20, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1_lambda_regularization,l2_lambda_regularization), name=\"fc2\")\n",
    "    y = tf.layers.dense(A2, 1, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1_lambda_regularization,l2_lambda_regularization), name=\"out\")\n",
    "    \n",
    "    return y\n",
    "\n",
    "def tf_lstm_model(X):\n",
    "    \n",
    "    l2_lambda_regularization = 0.20\n",
    "    l1_lambda_regularization = 0.10\n",
    "    \n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(20)\n",
    "    outputs,states = tf.contrib.rnn.static_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "    \n",
    "    A2 = tf.layers.dense(outputs[-1], 10, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                         kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1_lambda_regularization,l2_lambda_regularization), name=\"fc2\")\n",
    "    y = tf.layers.dense(A2, 1, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                        kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(l1_lambda_regularization,l2_lambda_regularization), name=\"out\")\n",
    "    return y \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_weights(model):\n",
    "\n",
    "    for layer in model.layers:\n",
    "        weights = layer.get_weights() # list of numpy arrays\n",
    "        \n",
    "        for weight in weights:\n",
    "        \n",
    "            print(weight.shape)\n",
    "            print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Data Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selected as per CNN paper\n",
    "features = ['T2', 'T24', 'T30', 'T50', 'P2', 'P15', 'P30', 'Nf', 'Nc', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', \n",
    "                     'BPR', 'farB', 'htBleed', 'Nf_dmd', 'PCNfR_dmd', 'W31', 'W32']\n",
    "selected_indices = np.array([2, 3, 4, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21])\n",
    "selected_features = list(features[i] for i in selected_indices-1)\n",
    "data_folder = '../CMAPSSData'\n",
    "\n",
    "window_size = 30\n",
    "window_stride = 1\n",
    "max_rul = 125\n",
    "\n",
    "dHandler_cmaps = CMAPSDataHandler(data_folder, 1, selected_features, max_rul, window_size, window_stride)\n",
    "#dHandler_cmaps.load_data(verbose=1, cross_validation_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tunable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#min_max_scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_compiled_model(num_features, sequence_length, output_shape):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    X, y = create_placeholders_rnn(num_features, sequence_length, output_shape)\n",
    "    \n",
    "    X = tf.unstack(X, axis=1)\n",
    "    \n",
    "    y_pred = tf_lstm_model(X)\n",
    "    cost = tf.losses.mean_squared_error(y, y_pred)\n",
    "    reg_cost = tf.losses.get_regularization_loss()\n",
    "    total_cost = cost + reg_cost\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.5).minimize(total_cost)\n",
    "    \n",
    "    return {'X_placeholder':X, 'y_placeholder':y, 'y_pred':y_pred, 'cost':cost, 'total_cost':total_cost, 'optimizer':optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tModel = SequenceTunableModelRegression('ModelRUL_LSTM_1', None, lib_type='tensorflow', data_handler=dHandler_cmaps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tModel.data_handler.sequence_length = 24\n",
    "#tModel.data_handler.sequence_length = maxWindowSize[datasetNumber]\n",
    "tModel.data_handler.sequence_stride = 1\n",
    "tModel.data_handler.max_rul = 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load Non sequenced data (unroll sequence into a single feature vector)\n",
    "\n",
    "# tModel.data_handler.data_scaler = None\n",
    "# tModel.data_scaler = min_max_scaler\n",
    "\n",
    "# tModel.load_data(unroll=True, verbose=1, cross_validation_ratio=0)\n",
    "# #tModel.data_handler.print_data()\n",
    "# tModel.print_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for dataset 1 with window_size of 24, stride of 1 and maxRUL of 129. Cros-Validation ratio 0\n",
      "Loading data from file and computing dataframes\n",
      "(512, 24, 14)\n",
      "24\n",
      "(512, 14)\n",
      "y_temp\n",
      "36\n",
      "(512, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#x_temp, y_temp = sklearn.utils.shuffle(tModel.X_train,tModel.y_train)\\nprint(\"x_temp\")\\nprint(len(X_batches))\\nprint(X_batches[0].shape)\\nprint(\"y_temp\")\\nprint(len(y_batches))\\nprint(y_batches[0].shape)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load sequenced data (do not unroll sequence into a single feature vector)\n",
    "\n",
    "tModel.data_handler.data_scaler = min_max_scaler\n",
    "tModel.data_scaler = None\n",
    "\n",
    "tModel.load_data(unroll=False, verbose=1, cross_validation_ratio=0)\n",
    "#tModel.data_handler.print_data()\n",
    "\n",
    "\n",
    "#tModel.print_data()\n",
    "\n",
    "X_batches, y_batches, total_batch = CMAPSAuxFunctions.get_minibatches(tModel.X_train,tModel.y_train, 512)\n",
    "print(X_batches[0].shape)\n",
    "X_input = tf.unstack(X_batches[0], axis=1)\n",
    "print(len(X_input))\n",
    "print(X_input[0].shape)\n",
    "\n",
    "print(\"y_temp\")\n",
    "print(len(y_batches))\n",
    "print(y_batches[0].shape)\n",
    "\n",
    "\"\"\"\n",
    "#x_temp, y_temp = sklearn.utils.shuffle(tModel.X_train,tModel.y_train)\n",
    "print(\"x_temp\")\n",
    "print(len(X_batches))\n",
    "print(X_batches[0].shape)\n",
    "print(\"y_temp\")\n",
    "print(len(y_batches))\n",
    "print(y_batches[0].shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model and test some Tunable Model functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-af2aafc5a101>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mtModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_session\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Projects\\NASA_RUL_-CMAPS-\\code\\tunable_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, verbose, learningRate_scheduler, tf_session)\u001b[0m\n\u001b[0;32m     80\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"A valid tensorflow session is needed to perform the training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_tf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Projects\\NASA_RUL_-CMAPS-\\code\\tunable_model.py\u001b[0m in \u001b[0;36mtrain_tf\u001b[1;34m(self, tf_session, verbose)\u001b[0m\n\u001b[0;32m    157\u001b[0m                                 \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m                                 \u001b[0mcost_tot\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                                 \u001b[0mcost_reg_tot\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc_reg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "nFeatures = len(selected_features)\n",
    "\n",
    "#lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "\n",
    "# shape = len(selected_features)*tModel.data_handler.sequence_length\n",
    "# print(shape)\n",
    "\n",
    "shape = (nFeatures,tModel.data_handler.sequence_length)\n",
    "\n",
    "#modelRULSN = RULmodel_SN_5(shape)\n",
    "#modelRULSN.compile(optimizer = optimizer, loss = lossFunction, metrics = metrics)\n",
    "\n",
    "#model = get_compiled_model(shape, model_type='ann')\n",
    "#tModel.change_model('ModelRUL_SN_1', model, 'keras')\n",
    "\n",
    "model = tf_compiled_model(nFeatures,tModel.data_handler.sequence_length, 1)\n",
    "tModel.change_model('ModelRUL_LSTM_1', model, 'tensorflow')\n",
    "\n",
    "#tModel.print_data()\n",
    "\n",
    "#shape = (window_size, num_features)\n",
    "#model = get_compiled_model(shape, model_type='lstm')\n",
    "#tModel.change_model('ModelRUL_RNN_1', model, 'keras')\n",
    "\n",
    "#tModel.data_handler.data_scaler = min_max_scaler\n",
    "#tModel.data_scaler = min_max_scaler\n",
    "\n",
    "#print(\"Printing model weights\")\n",
    "#print_model_weights(tModel.model)\n",
    "\n",
    "tModel.epochs = 200\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "tModel.train_model(tf_session=sess, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tModel.evaluate_model(['rhs', 'rmse'], round=2, tf_session=sess)\n",
    "print(\"scores\")\n",
    "\n",
    "cScores = tModel.scores\n",
    "#rmse = math.sqrt(cScores['score_1'])\n",
    "rmse2 = cScores['rmse']\n",
    "rhs = cScores['rhs']\n",
    "time = tModel.train_time\n",
    "\n",
    "i = range(1,len(tModel.y_test)+1)\n",
    "\n",
    "\n",
    "for i, rul_prediction, rul_prediction_rounded, true_rul in zip(i, np.ravel(tModel.y_predicted), tModel.y_predicted_rounded, np.ravel(tModel.y_test)):\n",
    "    print('Engine {}, Predicted RUL {}, Rounded RUL {}, Real RUL {}'.format(i, rul_prediction, \n",
    "                                                                    rul_prediction_rounded, \n",
    "                                                                    true_rul))\n",
    "\n",
    "print(cScores)\n",
    "#print(\"RMSE: {}\".format(rmse))\n",
    "print(\"RMSE2: {}\".format(rmse2))\n",
    "print(\"RHS: {}\".format(rhs))\n",
    "print(\"Time : {} seconds\".format(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
